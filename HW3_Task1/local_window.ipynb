{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"local_window.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"sCXRy7hMe_eN","colab_type":"text"},"source":["## 1. Read Me \n","**Run first cell :** install dependencies \n","\n","**Run second cell :** install files and local environment on the VM, choose which task you want to run by setting task_1 or task_2 value to true\n","\n","**Run third cell :** try diferent init of the agent. When one is found -> the agent is trained  "]},{"cell_type":"code","metadata":{"id":"8Rbvg4vCI9nE","colab_type":"code","outputId":"b71f92d0-136d-446d-ba3e-0547f5a5132b","executionInfo":{"status":"ok","timestamp":1587499793491,"user_tz":-480,"elapsed":11631,"user":{"displayName":"N","photoUrl":"","userId":"08619141603155082384"}},"colab":{"base_uri":"https://localhost:8080/","height":352}},"source":["# Dependencies\n","\n","!pip install torch numpy git+https://github.com/cs4246/gym-grid-driving.git\n","!pip install -U -q PyDrive"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting git+https://github.com/cs4246/gym-grid-driving.git\n","  Cloning https://github.com/cs4246/gym-grid-driving.git to /tmp/pip-req-build-82_ci8ih\n","  Running command git clone -q https://github.com/cs4246/gym-grid-driving.git /tmp/pip-req-build-82_ci8ih\n","Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.18.2)\n","Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (from gym-grid-driving==0.0.1) (0.17.1)\n","Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym->gym-grid-driving==0.0.1) (1.3.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym->gym-grid-driving==0.0.1) (1.4.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym->gym-grid-driving==0.0.1) (1.12.0)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym->gym-grid-driving==0.0.1) (1.5.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym->gym-grid-driving==0.0.1) (0.16.0)\n","Building wheels for collected packages: gym-grid-driving\n","  Building wheel for gym-grid-driving (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gym-grid-driving: filename=gym_grid_driving-0.0.1-cp36-none-any.whl size=8623 sha256=aa377145059dc764cd69deaef6859ca58d41cc7f5d1f8e7038fb1d330abfdae6\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-29nxqdjs/wheels/e1/30/f2/157c0938ab9bfe9c10c29c9fcab8392f587c9d141f215b67ca\n","Successfully built gym-grid-driving\n","Installing collected packages: gym-grid-driving\n","Successfully installed gym-grid-driving-0.0.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KPeqg5K3Xjjs","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":437},"outputId":"dd5e0b0c-3e12-4c1b-fc80-45c0fe16d174","executionInfo":{"status":"ok","timestamp":1587500067103,"user_tz":-480,"elapsed":5990,"user":{"displayName":"N","photoUrl":"","userId":"08619141603155082384"}}},"source":["!unzip HW3_Task2\\ \\(5\\).zip"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Archive:  HW3_Task2 (5).zip\n","   creating: HW3_Task2/\n"," extracting: HW3_Task2/untitled      \n","   creating: HW3_Task2/saves/\n","  inflating: HW3_Task2/saves/m__cb_15_1.pt  \n","   creating: HW3_Task2/.ipynb_checkpoints/\n","   creating: HW3_Task2/agent/\n","  inflating: HW3_Task2/agent/state_change.py  \n","  inflating: HW3_Task2/agent/__init__.py  \n","  inflating: HW3_Task2/agent/buffer_src.py  \n","  inflating: HW3_Task2/agent/agent.py  \n","   creating: HW3_Task2/agent/__pycache__/\n","  inflating: HW3_Task2/agent/__pycache__/__init__.cpython-36.pyc  \n","  inflating: HW3_Task2/agent/__pycache__/models.cpython-36.pyc  \n","  inflating: HW3_Task2/agent/__pycache__/loss_src.cpython-36.pyc  \n","  inflating: HW3_Task2/agent/__pycache__/buffer_src.cpython-36.pyc  \n","  inflating: HW3_Task2/agent/__pycache__/agent.cpython-36.pyc  \n","  inflating: HW3_Task2/agent/__pycache__/env.cpython-36.pyc  \n","  inflating: HW3_Task2/agent/loss.py  \n","   creating: HW3_Task2/agent/.ipynb_checkpoints/\n","  inflating: HW3_Task2/agent/env.py  \n","  inflating: HW3_Task2/agent/models.py  \n","  inflating: HW3_Task2/agent/loss_src.py  \n","  inflating: HW3_Task2/setup.py      \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Y1Elko7GXhdv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"5edca80b-115f-40eb-bb53-46b744d65363"},"source":["from HW3_Task2.agent import models \n","from HW3_Task2.agent import agent \n","from HW3_Task2.agent import env as env_builder\n","import importlib \n","import collections\n","importlib.reload(models)\n","importlib.reload(agent)\n","importlib.reload(env_builder)\n","try:\n","  importlib.reload(models)\n","  importlib.reload(agent)\n","  importlib.reload(env)\n","except:pass\n","Config = collections.namedtuple('Config', ('x_debut', 'y_debut', 'max_episode', 'max_epsilon', 'epsilon_decay', 'test_interval','save_interval','batch_size','buffer_limit','methode','gamma_nstep','nstep'))\n","config_l=[]\n","last_y = 2\n","for i in range(0,9):\n","  x_debut = 10+i*5\n","  \n","\n","  for j in range(0,last_y):\n","    time = i + j \n","    \n","    y_debut = j\n","    max_episode = 250*(i+1)+500*(j)\n","    max_episode = min(max_episode,8000)\n","    # max_epsilon decrease with time \n","    max_epsilon = max(1.0 / (time+1),0.3)\n","    epsilon_decay = max_episode // 5\n","    test_interval = min(500,max_episode//2)\n","    save_interval = 1000\n","    batch_size = 128\n","    buffer_limit = min(40000,4000*(time+1))\n","    methode = 'Mixed Monte Carlo + Conv DQN'\n","    gamma_nstep  = 0.8\n","    nstep = 4\n","    config_cur = Config(x_debut,y_debut,max_episode,max_epsilon,epsilon_decay,test_interval,save_interval,batch_size,buffer_limit,methode,gamma_nstep,nstep)\n","    \n","    env = env_builder.construct_task2_env_ij(x_debut,y_debut)\n","    \n","    if i == 0 and j == 0 :\n","      model = models.train(agent.ConvDQN, env=env,pretrain=False,model_p= None,savepath='HW3_Task2/saves/m_',config=config_cur)\n","    else:\n","      model = models.train(agent.ConvDQN, env=env,pretrain=True,model_p= model,savepath='HW3_Task2/saves/m_',config=config_cur)\n","\n","\n","  last_y += 1\n","\n","!zip -r HW3_Task2_ConvDQN_g_0.8.zip HW3_Task2\n","from google.colab import files\n","files.download('HW3_Task2_ConvDQN_g_0.8.zip')\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["device is: cuda\n","config : \n"," x debut 10, y_debut 0, max ep 250, max epsilon 1.0 \n"," Epsilon_decay 50, test_interval 125\n","Save_interval 1000, batch_size 128, buffer_limit 4000 \n"," Methode Mixed Monte Carlo + Conv DQN, gamma_nstep 0.8, nstep 4\n","Start learning from : x_debut 10 y_debut 0\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n","  out=out, **kwargs)\n","/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n","  ret = ret.dtype.type(ret / rcount)\n"],"name":"stderr"},{"output_type":"stream","text":["[Episode 50]\t rewards globals : 0.588 \tavg rewards : 0.588,\tavg loss: : nan,\tbuffer size : 268,\tepsilon : 37.4%, \t r <=40 0.0, \t r > 40 5.88235294117647\n","[Episode 100]\t rewards globals : 0.297 \tavg rewards : 0.000,\tavg loss: : nan,\tbuffer size : 490,\tepsilon : 14.4%, \t r <=40 0.0, \t r > 40 0.0\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 0.0\n","[task_2_tmax40] 100 run(s) avg rewards : 0.0\n","Point: 0.0\n","[Episode 150]\t rewards globals : 1.126 \tavg rewards : 2.745,\tavg loss: : 0.738981,\tbuffer size : 771,\tepsilon : 5.9%, \t r <=40 0.0, \t r > 40 27.450980392156865\n","[Episode 200]\t rewards globals : 2.488 \tavg rewards : 6.667,\tavg loss: : 0.268105,\tbuffer size : 1029,\tepsilon : 2.8%, \t r <=40 0.0, \t r > 40 66.66666666666666\n","error while saving final\n","device is: cuda\n","config : \n"," x debut 10, y_debut 1, max ep 750, max epsilon 0.5 \n"," Epsilon_decay 150, test_interval 375\n","Save_interval 1000, batch_size 128, buffer_limit 8000 \n"," Methode Mixed Monte Carlo + Conv DQN, gamma_nstep 0.8, nstep 4\n","Start learning from : x_debut 10 y_debut 1\n","[Episode 50]\t rewards globals : 3.137 \tavg rewards : 3.137,\tavg loss: : nan,\tbuffer size : 228,\tepsilon : 36.1%, \t r <=40 0.0, \t r > 40 31.372549019607842\n","[Episode 100]\t rewards globals : 2.970 \tavg rewards : 2.941,\tavg loss: : nan,\tbuffer size : 468,\tepsilon : 26.2%, \t r <=40 0.0, \t r > 40 29.411764705882355\n","[Episode 150]\t rewards globals : 3.377 \tavg rewards : 4.118,\tavg loss: : 1.615405,\tbuffer size : 700,\tepsilon : 19.0%, \t r <=40 0.0, \t r > 40 41.17647058823529\n","[Episode 200]\t rewards globals : 3.582 \tavg rewards : 4.314,\tavg loss: : 0.392203,\tbuffer size : 972,\tepsilon : 13.9%, \t r <=40 0.0, \t r > 40 43.13725490196079\n","[Episode 250]\t rewards globals : 3.586 \tavg rewards : 3.725,\tavg loss: : 0.255804,\tbuffer size : 1187,\tepsilon : 10.3%, \t r <=40 0.0, \t r > 40 37.254901960784316\n","[Episode 300]\t rewards globals : 3.953 \tavg rewards : 5.686,\tavg loss: : 0.191427,\tbuffer size : 1437,\tepsilon : 7.6%, \t r <=40 0.0, \t r > 40 56.86274509803921\n","[Episode 350]\t rewards globals : 3.932 \tavg rewards : 3.922,\tavg loss: : 0.157585,\tbuffer size : 1659,\tepsilon : 5.8%, \t r <=40 0.0, \t r > 40 39.21568627450981\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 5.5\n","[task_2_tmax40] 100 run(s) avg rewards : 5.0\n","Point: 5.25\n","[Episode 400]\t rewards globals : 4.065 \tavg rewards : 4.902,\tavg loss: : 0.139267,\tbuffer size : 1908,\tepsilon : 4.4%, \t r <=40 0.0, \t r > 40 49.01960784313725\n","[Episode 450]\t rewards globals : 4.124 \tavg rewards : 4.510,\tavg loss: : 0.128011,\tbuffer size : 2107,\tepsilon : 3.4%, \t r <=40 0.0, \t r > 40 45.09803921568628\n","[Episode 500]\t rewards globals : 4.271 \tavg rewards : 5.490,\tavg loss: : 0.116708,\tbuffer size : 2321,\tepsilon : 2.7%, \t r <=40 0.0, \t r > 40 54.90196078431373\n","[Episode 550]\t rewards globals : 4.392 \tavg rewards : 5.490,\tavg loss: : 0.107785,\tbuffer size : 2566,\tepsilon : 2.3%, \t r <=40 0.0, \t r > 40 54.90196078431373\n","[Episode 600]\t rewards globals : 4.542 \tavg rewards : 6.275,\tavg loss: : 0.100477,\tbuffer size : 2821,\tepsilon : 1.9%, \t r <=40 0.0, \t r > 40 62.745098039215684\n","[Episode 650]\t rewards globals : 4.593 \tavg rewards : 5.294,\tavg loss: : 0.098214,\tbuffer size : 3048,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 52.94117647058824\n","[Episode 700]\t rewards globals : 4.765 \tavg rewards : 7.059,\tavg loss: : 0.093199,\tbuffer size : 3311,\tepsilon : 1.5%, \t r <=40 0.0, \t r > 40 70.58823529411765\n","error while saving final\n","device is: cuda\n","config : \n"," x debut 15, y_debut 0, max ep 500, max epsilon 0.5 \n"," Epsilon_decay 100, test_interval 250\n","Save_interval 1000, batch_size 128, buffer_limit 8000 \n"," Methode Mixed Monte Carlo + Conv DQN, gamma_nstep 0.8, nstep 4\n","Start learning from : x_debut 15 y_debut 0\n","[Episode 50]\t rewards globals : 4.118 \tavg rewards : 4.118,\tavg loss: : nan,\tbuffer size : 328,\tepsilon : 30.7%, \t r <=40 0.0, \t r > 40 41.17647058823529\n","[Episode 100]\t rewards globals : 5.248 \tavg rewards : 6.275,\tavg loss: : 1.023043,\tbuffer size : 712,\tepsilon : 19.0%, \t r <=40 0.0, \t r > 40 62.745098039215684\n","[Episode 150]\t rewards globals : 5.563 \tavg rewards : 6.275,\tavg loss: : 0.202895,\tbuffer size : 1096,\tepsilon : 11.9%, \t r <=40 0.0, \t r > 40 62.745098039215684\n","[Episode 200]\t rewards globals : 6.020 \tavg rewards : 7.255,\tavg loss: : 0.132297,\tbuffer size : 1474,\tepsilon : 7.6%, \t r <=40 0.0, \t r > 40 72.54901960784314\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 8.9\n","[task_2_tmax40] 100 run(s) avg rewards : 8.4\n","Point: 8.65\n","Re do test: double check\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 8.4\n","[task_2_tmax40] 100 run(s) avg rewards : 8.9\n","Point: 8.65\n","Training should stop\n","error while saving final\n","device is: cuda\n","config : \n"," x debut 15, y_debut 1, max ep 1000, max epsilon 0.3333333333333333 \n"," Epsilon_decay 200, test_interval 500\n","Save_interval 1000, batch_size 128, buffer_limit 12000 \n"," Methode Mixed Monte Carlo + Conv DQN, gamma_nstep 0.8, nstep 4\n","Start learning from : x_debut 15 y_debut 1\n","[Episode 50]\t rewards globals : 5.098 \tavg rewards : 5.098,\tavg loss: : nan,\tbuffer size : 347,\tepsilon : 26.2%, \t r <=40 0.0, \t r > 40 50.98039215686274\n","[Episode 100]\t rewards globals : 5.347 \tavg rewards : 5.490,\tavg loss: : 1.057288,\tbuffer size : 690,\tepsilon : 20.6%, \t r <=40 0.0, \t r > 40 54.90196078431373\n","[Episode 150]\t rewards globals : 5.629 \tavg rewards : 6.275,\tavg loss: : 0.157734,\tbuffer size : 1059,\tepsilon : 16.3%, \t r <=40 0.0, \t r > 40 62.745098039215684\n","[Episode 200]\t rewards globals : 5.572 \tavg rewards : 5.490,\tavg loss: : 0.100698,\tbuffer size : 1404,\tepsilon : 12.9%, \t r <=40 0.0, \t r > 40 54.90196078431373\n","[Episode 250]\t rewards globals : 5.777 \tavg rewards : 6.667,\tavg loss: : 0.084175,\tbuffer size : 1779,\tepsilon : 10.3%, \t r <=40 0.0, \t r > 40 66.66666666666666\n","[Episode 300]\t rewards globals : 5.914 \tavg rewards : 6.667,\tavg loss: : 0.071164,\tbuffer size : 2139,\tepsilon : 8.2%, \t r <=40 0.0, \t r > 40 66.66666666666666\n","[Episode 350]\t rewards globals : 6.239 \tavg rewards : 8.039,\tavg loss: : 0.063832,\tbuffer size : 2520,\tepsilon : 6.6%, \t r <=40 0.0, \t r > 40 80.3921568627451\n","[Episode 400]\t rewards globals : 6.384 \tavg rewards : 7.451,\tavg loss: : 0.057791,\tbuffer size : 2879,\tepsilon : 5.4%, \t r <=40 0.0, \t r > 40 74.50980392156863\n","[Episode 450]\t rewards globals : 6.475 \tavg rewards : 7.059,\tavg loss: : 0.056184,\tbuffer size : 3195,\tepsilon : 4.4%, \t r <=40 0.0, \t r > 40 70.58823529411765\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 8.0\n","[task_2_tmax40] 100 run(s) avg rewards : 8.4\n","Point: 8.2\n","[Episode 500]\t rewards globals : 6.647 \tavg rewards : 8.235,\tavg loss: : 0.052284,\tbuffer size : 3541,\tepsilon : 3.7%, \t r <=40 0.0, \t r > 40 82.35294117647058\n","[Episode 550]\t rewards globals : 6.751 \tavg rewards : 7.843,\tavg loss: : 0.050164,\tbuffer size : 3871,\tepsilon : 3.1%, \t r <=40 0.0, \t r > 40 78.43137254901961\n","[Episode 600]\t rewards globals : 6.905 \tavg rewards : 8.627,\tavg loss: : 0.047985,\tbuffer size : 4221,\tepsilon : 2.6%, \t r <=40 0.0, \t r > 40 86.27450980392157\n","[Episode 650]\t rewards globals : 6.974 \tavg rewards : 7.843,\tavg loss: : 0.048192,\tbuffer size : 4572,\tepsilon : 2.3%, \t r <=40 0.0, \t r > 40 78.43137254901961\n","[Episode 700]\t rewards globals : 7.104 \tavg rewards : 8.824,\tavg loss: : 0.045820,\tbuffer size : 4928,\tepsilon : 2.0%, \t r <=40 0.0, \t r > 40 88.23529411764706\n","[Episode 750]\t rewards globals : 7.177 \tavg rewards : 8.235,\tavg loss: : 0.044618,\tbuffer size : 5274,\tepsilon : 1.8%, \t r <=40 0.0, \t r > 40 82.35294117647058\n","[Episode 800]\t rewards globals : 7.253 \tavg rewards : 8.431,\tavg loss: : 0.043266,\tbuffer size : 5634,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 84.31372549019608\n","[Episode 850]\t rewards globals : 7.344 \tavg rewards : 8.824,\tavg loss: : 0.042501,\tbuffer size : 5972,\tepsilon : 1.5%, \t r <=40 0.0, \t r > 40 88.23529411764706\n","[Episode 900]\t rewards globals : 7.425 \tavg rewards : 8.824,\tavg loss: : 0.041527,\tbuffer size : 6338,\tepsilon : 1.4%, \t r <=40 0.0, \t r > 40 88.23529411764706\n","[Episode 950]\t rewards globals : 7.497 \tavg rewards : 8.824,\tavg loss: : 0.040662,\tbuffer size : 6705,\tepsilon : 1.3%, \t r <=40 0.0, \t r > 40 88.23529411764706\n","error while saving final\n","device is: cuda\n","config : \n"," x debut 15, y_debut 2, max ep 1500, max epsilon 0.3 \n"," Epsilon_decay 300, test_interval 500\n","Save_interval 1000, batch_size 128, buffer_limit 16000 \n"," Methode Mixed Monte Carlo + Conv DQN, gamma_nstep 0.8, nstep 4\n","Start learning from : x_debut 15 y_debut 2\n","[Episode 50]\t rewards globals : 4.510 \tavg rewards : 4.510,\tavg loss: : nan,\tbuffer size : 383,\tepsilon : 25.5%, \t r <=40 0.0, \t r > 40 45.09803921568628\n","[Episode 100]\t rewards globals : 4.653 \tavg rewards : 4.706,\tavg loss: : 0.444332,\tbuffer size : 753,\tepsilon : 21.8%, \t r <=40 0.0, \t r > 40 47.05882352941176\n","[Episode 150]\t rewards globals : 5.298 \tavg rewards : 6.471,\tavg loss: : 0.122711,\tbuffer size : 1124,\tepsilon : 18.6%, \t r <=40 0.0, \t r > 40 64.70588235294117\n","[Episode 200]\t rewards globals : 5.473 \tavg rewards : 6.078,\tavg loss: : 0.080585,\tbuffer size : 1506,\tepsilon : 15.9%, \t r <=40 0.0, \t r > 40 60.78431372549019\n","[Episode 250]\t rewards globals : 5.857 \tavg rewards : 7.451,\tavg loss: : 0.069405,\tbuffer size : 1889,\tepsilon : 13.6%, \t r <=40 0.0, \t r > 40 74.50980392156863\n","[Episode 300]\t rewards globals : 5.914 \tavg rewards : 6.275,\tavg loss: : 0.059891,\tbuffer size : 2243,\tepsilon : 11.7%, \t r <=40 0.0, \t r > 40 62.745098039215684\n","[Episode 350]\t rewards globals : 6.125 \tavg rewards : 7.451,\tavg loss: : 0.052779,\tbuffer size : 2638,\tepsilon : 10.0%, \t r <=40 0.0, \t r > 40 74.50980392156863\n","[Episode 400]\t rewards globals : 6.284 \tavg rewards : 7.255,\tavg loss: : 0.049507,\tbuffer size : 3011,\tepsilon : 8.6%, \t r <=40 0.0, \t r > 40 72.54901960784314\n","[Episode 450]\t rewards globals : 6.408 \tavg rewards : 7.451,\tavg loss: : 0.047475,\tbuffer size : 3371,\tepsilon : 7.5%, \t r <=40 0.0, \t r > 40 74.50980392156863\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 8.2\n","[task_2_tmax40] 100 run(s) avg rewards : 8.2\n","Point: 8.2\n","[Episode 500]\t rewards globals : 6.527 \tavg rewards : 7.647,\tavg loss: : 0.045318,\tbuffer size : 3741,\tepsilon : 6.5%, \t r <=40 0.0, \t r > 40 76.47058823529412\n","[Episode 550]\t rewards globals : 6.661 \tavg rewards : 8.039,\tavg loss: : 0.042934,\tbuffer size : 4120,\tepsilon : 5.6%, \t r <=40 0.0, \t r > 40 80.3921568627451\n","[Episode 600]\t rewards globals : 6.622 \tavg rewards : 6.275,\tavg loss: : 0.041981,\tbuffer size : 4479,\tepsilon : 4.9%, \t r <=40 0.0, \t r > 40 62.745098039215684\n","[Episode 650]\t rewards globals : 6.713 \tavg rewards : 7.843,\tavg loss: : 0.041856,\tbuffer size : 4851,\tepsilon : 4.3%, \t r <=40 0.0, \t r > 40 78.43137254901961\n","[Episode 700]\t rewards globals : 6.790 \tavg rewards : 7.843,\tavg loss: : 0.040677,\tbuffer size : 5232,\tepsilon : 3.8%, \t r <=40 0.0, \t r > 40 78.43137254901961\n","[Episode 750]\t rewards globals : 6.858 \tavg rewards : 7.647,\tavg loss: : 0.039502,\tbuffer size : 5623,\tepsilon : 3.4%, \t r <=40 0.0, \t r > 40 76.47058823529412\n","[Episode 800]\t rewards globals : 6.904 \tavg rewards : 7.647,\tavg loss: : 0.038968,\tbuffer size : 5995,\tepsilon : 3.0%, \t r <=40 0.0, \t r > 40 76.47058823529412\n","[Episode 850]\t rewards globals : 6.933 \tavg rewards : 7.451,\tavg loss: : 0.038745,\tbuffer size : 6345,\tepsilon : 2.7%, \t r <=40 0.0, \t r > 40 74.50980392156863\n","[Episode 900]\t rewards globals : 7.014 \tavg rewards : 8.431,\tavg loss: : 0.037573,\tbuffer size : 6734,\tepsilon : 2.4%, \t r <=40 0.0, \t r > 40 84.31372549019608\n","[Episode 950]\t rewards globals : 7.066 \tavg rewards : 7.843,\tavg loss: : 0.036774,\tbuffer size : 7110,\tepsilon : 2.2%, \t r <=40 0.0, \t r > 40 78.43137254901961\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 8.3\n","[task_2_tmax40] 100 run(s) avg rewards : 8.1\n","Point: 8.2\n","[Episode 1000]\t rewards globals : 7.163 \tavg rewards : 9.020,\tavg loss: : 0.035948,\tbuffer size : 7495,\tepsilon : 2.0%, \t r <=40 0.0, \t r > 40 90.19607843137256\n","[Episode 1050]\t rewards globals : 7.212 \tavg rewards : 8.235,\tavg loss: : 0.035950,\tbuffer size : 7877,\tepsilon : 1.9%, \t r <=40 0.0, \t r > 40 82.35294117647058\n","[Episode 1100]\t rewards globals : 7.266 \tavg rewards : 8.431,\tavg loss: : 0.035207,\tbuffer size : 8262,\tepsilon : 1.7%, \t r <=40 0.0, \t r > 40 84.31372549019608\n","[Episode 1150]\t rewards globals : 7.333 \tavg rewards : 8.824,\tavg loss: : 0.034529,\tbuffer size : 8668,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 88.23529411764706\n","[Episode 1200]\t rewards globals : 7.410 \tavg rewards : 9.216,\tavg loss: : 0.033905,\tbuffer size : 9045,\tepsilon : 1.5%, \t r <=40 0.0, \t r > 40 92.15686274509804\n","[Episode 1250]\t rewards globals : 7.450 \tavg rewards : 8.431,\tavg loss: : 0.034180,\tbuffer size : 9420,\tepsilon : 1.4%, \t r <=40 0.0, \t r > 40 84.31372549019608\n","[Episode 1300]\t rewards globals : 7.463 \tavg rewards : 7.647,\tavg loss: : 0.033542,\tbuffer size : 9796,\tepsilon : 1.4%, \t r <=40 0.0, \t r > 40 76.47058823529412\n","[Episode 1350]\t rewards globals : 7.513 \tavg rewards : 8.627,\tavg loss: : 0.033014,\tbuffer size : 10174,\tepsilon : 1.3%, \t r <=40 0.0, \t r > 40 86.27450980392157\n","[Episode 1400]\t rewards globals : 7.559 \tavg rewards : 8.627,\tavg loss: : 0.032624,\tbuffer size : 10545,\tepsilon : 1.3%, \t r <=40 0.0, \t r > 40 86.27450980392157\n","[Episode 1450]\t rewards globals : 7.574 \tavg rewards : 8.039,\tavg loss: : 0.032770,\tbuffer size : 10915,\tepsilon : 1.2%, \t r <=40 0.0, \t r > 40 80.3921568627451\n","error while saving final\n","device is: cuda\n","config : \n"," x debut 20, y_debut 0, max ep 750, max epsilon 0.3333333333333333 \n"," Epsilon_decay 150, test_interval 375\n","Save_interval 1000, batch_size 128, buffer_limit 12000 \n"," Methode Mixed Monte Carlo + Conv DQN, gamma_nstep 0.8, nstep 4\n","Start learning from : x_debut 20 y_debut 0\n","[Episode 50]\t rewards globals : 5.294 \tavg rewards : 5.294,\tavg loss: : nan,\tbuffer size : 385,\tepsilon : 24.2%, \t r <=40 0.0, \t r > 40 52.94117647058824\n","[Episode 100]\t rewards globals : 5.347 \tavg rewards : 5.294,\tavg loss: : 0.544039,\tbuffer size : 805,\tepsilon : 17.6%, \t r <=40 0.0, \t r > 40 52.94117647058824\n","[Episode 150]\t rewards globals : 6.093 \tavg rewards : 7.451,\tavg loss: : 0.175551,\tbuffer size : 1302,\tepsilon : 12.9%, \t r <=40 0.0, \t r > 40 74.50980392156863\n","[Episode 200]\t rewards globals : 6.318 \tavg rewards : 7.059,\tavg loss: : 0.114753,\tbuffer size : 1760,\tepsilon : 9.5%, \t r <=40 0.0, \t r > 40 70.58823529411765\n","[Episode 250]\t rewards globals : 6.454 \tavg rewards : 6.863,\tavg loss: : 0.091556,\tbuffer size : 2218,\tepsilon : 7.1%, \t r <=40 0.0, \t r > 40 68.62745098039215\n","[Episode 300]\t rewards globals : 6.811 \tavg rewards : 8.627,\tavg loss: : 0.075765,\tbuffer size : 2713,\tepsilon : 5.4%, \t r <=40 0.0, \t r > 40 86.27450980392157\n","[Episode 350]\t rewards globals : 7.037 \tavg rewards : 8.235,\tavg loss: : 0.065633,\tbuffer size : 3266,\tepsilon : 4.1%, \t r <=40 0.0, \t r > 40 82.35294117647058\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 9.0\n","[task_2_tmax40] 100 run(s) avg rewards : 8.7\n","Point: 8.85\n","Re do test: double check\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 8.8\n","[task_2_tmax40] 100 run(s) avg rewards : 8.6\n","Point: 8.7\n","Training should stop\n","error while saving final\n","device is: cuda\n","config : \n"," x debut 20, y_debut 1, max ep 1250, max epsilon 0.3 \n"," Epsilon_decay 250, test_interval 500\n","Save_interval 1000, batch_size 128, buffer_limit 16000 \n"," Methode Mixed Monte Carlo + Conv DQN, gamma_nstep 0.8, nstep 4\n","Start learning from : x_debut 20 y_debut 1\n","[Episode 50]\t rewards globals : 4.510 \tavg rewards : 4.510,\tavg loss: : nan,\tbuffer size : 405,\tepsilon : 24.7%, \t r <=40 0.0, \t r > 40 45.09803921568628\n","[Episode 100]\t rewards globals : 5.149 \tavg rewards : 5.686,\tavg loss: : 0.332237,\tbuffer size : 849,\tepsilon : 20.4%, \t r <=40 0.0, \t r > 40 56.86274509803921\n","[Episode 150]\t rewards globals : 5.430 \tavg rewards : 5.882,\tavg loss: : 0.122588,\tbuffer size : 1256,\tepsilon : 16.9%, \t r <=40 0.0, \t r > 40 58.82352941176471\n","[Episode 200]\t rewards globals : 5.672 \tavg rewards : 6.471,\tavg loss: : 0.084811,\tbuffer size : 1704,\tepsilon : 14.0%, \t r <=40 0.0, \t r > 40 64.70588235294117\n","[Episode 250]\t rewards globals : 5.936 \tavg rewards : 6.863,\tavg loss: : 0.071847,\tbuffer size : 2188,\tepsilon : 11.7%, \t r <=40 0.0, \t r > 40 68.62745098039215\n","[Episode 300]\t rewards globals : 6.346 \tavg rewards : 8.431,\tavg loss: : 0.062029,\tbuffer size : 2718,\tepsilon : 9.7%, \t r <=40 0.0, \t r > 40 84.31372549019608\n","[Episode 350]\t rewards globals : 6.382 \tavg rewards : 6.667,\tavg loss: : 0.056459,\tbuffer size : 3168,\tepsilon : 8.2%, \t r <=40 0.0, \t r > 40 66.66666666666666\n","[Episode 400]\t rewards globals : 6.584 \tavg rewards : 8.039,\tavg loss: : 0.051322,\tbuffer size : 3626,\tepsilon : 6.9%, \t r <=40 0.0, \t r > 40 80.3921568627451\n","[Episode 450]\t rewards globals : 6.630 \tavg rewards : 7.059,\tavg loss: : 0.049735,\tbuffer size : 4082,\tepsilon : 5.8%, \t r <=40 0.0, \t r > 40 70.58823529411765\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 8.8\n","[task_2_tmax40] 100 run(s) avg rewards : 8.8\n","Point: 8.8\n","Re do test: double check\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 8.7\n","[task_2_tmax40] 100 run(s) avg rewards : 9.4\n","Point: 9.05\n","Training should stop\n","error while saving final\n","device is: cuda\n","config : \n"," x debut 20, y_debut 2, max ep 1750, max epsilon 0.3 \n"," Epsilon_decay 350, test_interval 500\n","Save_interval 1000, batch_size 128, buffer_limit 20000 \n"," Methode Mixed Monte Carlo + Conv DQN, gamma_nstep 0.8, nstep 4\n","Start learning from : x_debut 20 y_debut 2\n","[Episode 50]\t rewards globals : 5.490 \tavg rewards : 5.490,\tavg loss: : nan,\tbuffer size : 432,\tepsilon : 26.1%, \t r <=40 0.0, \t r > 40 54.90196078431373\n","[Episode 100]\t rewards globals : 5.347 \tavg rewards : 5.294,\tavg loss: : 0.213622,\tbuffer size : 886,\tepsilon : 22.8%, \t r <=40 0.0, \t r > 40 52.94117647058824\n","[Episode 150]\t rewards globals : 5.033 \tavg rewards : 4.314,\tavg loss: : 0.095252,\tbuffer size : 1271,\tepsilon : 19.9%, \t r <=40 0.0, \t r > 40 43.13725490196079\n","[Episode 200]\t rewards globals : 5.224 \tavg rewards : 5.882,\tavg loss: : 0.069375,\tbuffer size : 1713,\tepsilon : 17.4%, \t r <=40 0.0, \t r > 40 58.82352941176471\n","[Episode 250]\t rewards globals : 5.219 \tavg rewards : 5.294,\tavg loss: : 0.060768,\tbuffer size : 2141,\tepsilon : 15.2%, \t r <=40 0.0, \t r > 40 52.94117647058824\n","[Episode 300]\t rewards globals : 5.249 \tavg rewards : 5.490,\tavg loss: : 0.053769,\tbuffer size : 2597,\tepsilon : 13.3%, \t r <=40 0.0, \t r > 40 54.90196078431373\n","[Episode 350]\t rewards globals : 5.442 \tavg rewards : 6.471,\tavg loss: : 0.049156,\tbuffer size : 3047,\tepsilon : 11.7%, \t r <=40 0.0, \t r > 40 64.70588235294117\n","[Episode 400]\t rewards globals : 5.686 \tavg rewards : 7.451,\tavg loss: : 0.046904,\tbuffer size : 3561,\tepsilon : 10.2%, \t r <=40 0.0, \t r > 40 74.50980392156863\n","[Episode 450]\t rewards globals : 5.876 \tavg rewards : 7.451,\tavg loss: : 0.046555,\tbuffer size : 4053,\tepsilon : 9.0%, \t r <=40 0.0, \t r > 40 74.50980392156863\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 8.2\n","[task_2_tmax40] 100 run(s) avg rewards : 8.5\n","Point: 8.35\n","[Episode 500]\t rewards globals : 5.968 \tavg rewards : 6.863,\tavg loss: : 0.043392,\tbuffer size : 4500,\tepsilon : 7.9%, \t r <=40 0.0, \t r > 40 68.62745098039215\n","[Episode 550]\t rewards globals : 6.044 \tavg rewards : 6.863,\tavg loss: : 0.041840,\tbuffer size : 4949,\tepsilon : 7.0%, \t r <=40 0.0, \t r > 40 68.62745098039215\n","[Episode 600]\t rewards globals : 6.173 \tavg rewards : 7.451,\tavg loss: : 0.040773,\tbuffer size : 5458,\tepsilon : 6.2%, \t r <=40 0.0, \t r > 40 74.50980392156863\n","[Episode 650]\t rewards globals : 6.221 \tavg rewards : 6.863,\tavg loss: : 0.041009,\tbuffer size : 5918,\tepsilon : 5.5%, \t r <=40 0.0, \t r > 40 68.62745098039215\n","[Episode 700]\t rewards globals : 6.334 \tavg rewards : 7.843,\tavg loss: : 0.039882,\tbuffer size : 6379,\tepsilon : 4.9%, \t r <=40 0.0, \t r > 40 78.43137254901961\n","[Episode 750]\t rewards globals : 6.471 \tavg rewards : 8.235,\tavg loss: : 0.039089,\tbuffer size : 6859,\tepsilon : 4.4%, \t r <=40 0.0, \t r > 40 82.35294117647058\n","[Episode 800]\t rewards globals : 6.542 \tavg rewards : 7.451,\tavg loss: : 0.038182,\tbuffer size : 7323,\tepsilon : 3.9%, \t r <=40 0.0, \t r > 40 74.50980392156863\n","[Episode 850]\t rewards globals : 6.569 \tavg rewards : 7.059,\tavg loss: : 0.038383,\tbuffer size : 7771,\tepsilon : 3.6%, \t r <=40 0.0, \t r > 40 70.58823529411765\n","[Episode 900]\t rewards globals : 6.704 \tavg rewards : 9.020,\tavg loss: : 0.037540,\tbuffer size : 8256,\tepsilon : 3.2%, \t r <=40 0.0, \t r > 40 90.19607843137256\n","[Episode 950]\t rewards globals : 6.740 \tavg rewards : 7.451,\tavg loss: : 0.036949,\tbuffer size : 8729,\tepsilon : 2.9%, \t r <=40 0.0, \t r > 40 74.50980392156863\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 8.0\n","[task_2_tmax40] 100 run(s) avg rewards : 8.1\n","Point: 8.05\n","[Episode 1000]\t rewards globals : 6.833 \tavg rewards : 8.627,\tavg loss: : 0.036561,\tbuffer size : 9219,\tepsilon : 2.7%, \t r <=40 0.0, \t r > 40 86.27450980392157\n","[Episode 1050]\t rewards globals : 6.917 \tavg rewards : 8.627,\tavg loss: : 0.036703,\tbuffer size : 9696,\tepsilon : 2.4%, \t r <=40 0.0, \t r > 40 86.27450980392157\n","[Episode 1100]\t rewards globals : 6.975 \tavg rewards : 8.235,\tavg loss: : 0.036311,\tbuffer size : 10185,\tepsilon : 2.3%, \t r <=40 0.0, \t r > 40 82.35294117647058\n","[Episode 1150]\t rewards globals : 7.011 \tavg rewards : 7.843,\tavg loss: : 0.035867,\tbuffer size : 10652,\tepsilon : 2.1%, \t r <=40 0.0, \t r > 40 78.43137254901961\n","[Episode 1200]\t rewards globals : 7.069 \tavg rewards : 8.431,\tavg loss: : 0.035190,\tbuffer size : 11132,\tepsilon : 1.9%, \t r <=40 0.0, \t r > 40 84.31372549019608\n","[Episode 1250]\t rewards globals : 7.098 \tavg rewards : 7.647,\tavg loss: : 0.035446,\tbuffer size : 11587,\tepsilon : 1.8%, \t r <=40 0.0, \t r > 40 76.47058823529412\n","[Episode 1300]\t rewards globals : 7.179 \tavg rewards : 9.020,\tavg loss: : 0.034916,\tbuffer size : 12095,\tepsilon : 1.7%, \t r <=40 0.0, \t r > 40 90.19607843137256\n","[Episode 1350]\t rewards globals : 7.232 \tavg rewards : 8.627,\tavg loss: : 0.034761,\tbuffer size : 12601,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 86.27450980392157\n","[Episode 1400]\t rewards globals : 7.259 \tavg rewards : 8.039,\tavg loss: : 0.034465,\tbuffer size : 13093,\tepsilon : 1.5%, \t r <=40 0.0, \t r > 40 80.3921568627451\n","[Episode 1450]\t rewards globals : 7.271 \tavg rewards : 7.647,\tavg loss: : 0.034809,\tbuffer size : 13553,\tepsilon : 1.5%, \t r <=40 0.0, \t r > 40 76.47058823529412\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 7.8\n","[task_2_tmax40] 100 run(s) avg rewards : 8.3\n","Point: 8.05\n","[Episode 1500]\t rewards globals : 7.315 \tavg rewards : 8.627,\tavg loss: : 0.034635,\tbuffer size : 14046,\tepsilon : 1.4%, \t r <=40 0.0, \t r > 40 86.27450980392157\n","[Episode 1550]\t rewards globals : 7.363 \tavg rewards : 8.824,\tavg loss: : 0.034298,\tbuffer size : 14517,\tepsilon : 1.3%, \t r <=40 0.0, \t r > 40 88.23529411764706\n","[Episode 1600]\t rewards globals : 7.370 \tavg rewards : 7.647,\tavg loss: : 0.034054,\tbuffer size : 15005,\tepsilon : 1.3%, \t r <=40 0.0, \t r > 40 76.47058823529412\n","[Episode 1650]\t rewards globals : 7.414 \tavg rewards : 8.627,\tavg loss: : 0.034386,\tbuffer size : 15496,\tepsilon : 1.3%, \t r <=40 0.0, \t r > 40 86.27450980392157\n","[Episode 1700]\t rewards globals : 7.454 \tavg rewards : 8.824,\tavg loss: : 0.033976,\tbuffer size : 15975,\tepsilon : 1.2%, \t r <=40 0.0, \t r > 40 88.23529411764706\n","error while saving final\n","device is: cuda\n","config : \n"," x debut 20, y_debut 3, max ep 2250, max epsilon 0.3 \n"," Epsilon_decay 450, test_interval 500\n","Save_interval 1000, batch_size 128, buffer_limit 24000 \n"," Methode Mixed Monte Carlo + Conv DQN, gamma_nstep 0.8, nstep 4\n","Start learning from : x_debut 20 y_debut 3\n","[Episode 50]\t rewards globals : 2.157 \tavg rewards : 2.157,\tavg loss: : nan,\tbuffer size : 373,\tepsilon : 27.0%, \t r <=40 0.0, \t r > 40 21.568627450980394\n","[Episode 100]\t rewards globals : 2.970 \tavg rewards : 3.725,\tavg loss: : 0.473085,\tbuffer size : 811,\tepsilon : 24.2%, \t r <=40 0.0, \t r > 40 37.254901960784316\n","[Episode 150]\t rewards globals : 3.113 \tavg rewards : 3.333,\tavg loss: : 0.164511,\tbuffer size : 1277,\tepsilon : 21.8%, \t r <=40 0.0, \t r > 40 33.33333333333333\n","[Episode 200]\t rewards globals : 3.433 \tavg rewards : 4.314,\tavg loss: : 0.110387,\tbuffer size : 1736,\tepsilon : 19.6%, \t r <=40 0.0, \t r > 40 43.13725490196079\n","[Episode 250]\t rewards globals : 3.785 \tavg rewards : 5.294,\tavg loss: : 0.091496,\tbuffer size : 2229,\tepsilon : 17.6%, \t r <=40 0.0, \t r > 40 52.94117647058824\n","[Episode 300]\t rewards globals : 3.887 \tavg rewards : 4.510,\tavg loss: : 0.077942,\tbuffer size : 2645,\tepsilon : 15.9%, \t r <=40 0.0, \t r > 40 45.09803921568628\n","[Episode 350]\t rewards globals : 3.960 \tavg rewards : 4.314,\tavg loss: : 0.069507,\tbuffer size : 3079,\tepsilon : 14.3%, \t r <=40 0.0, \t r > 40 43.13725490196079\n","[Episode 400]\t rewards globals : 4.165 \tavg rewards : 5.686,\tavg loss: : 0.063387,\tbuffer size : 3582,\tepsilon : 12.9%, \t r <=40 0.0, \t r > 40 56.86274509803921\n","[Episode 450]\t rewards globals : 4.191 \tavg rewards : 4.510,\tavg loss: : 0.061551,\tbuffer size : 4087,\tepsilon : 11.7%, \t r <=40 0.0, \t r > 40 45.09803921568628\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 7.4\n","[task_2_tmax40] 100 run(s) avg rewards : 8.1\n","Point: 7.75\n","[Episode 500]\t rewards globals : 4.431 \tavg rewards : 6.667,\tavg loss: : 0.057963,\tbuffer size : 4576,\tepsilon : 10.5%, \t r <=40 0.0, \t r > 40 66.66666666666666\n","[Episode 550]\t rewards globals : 4.537 \tavg rewards : 5.686,\tavg loss: : 0.054673,\tbuffer size : 5051,\tepsilon : 9.5%, \t r <=40 0.0, \t r > 40 56.86274509803921\n","[Episode 600]\t rewards globals : 4.725 \tavg rewards : 6.667,\tavg loss: : 0.052001,\tbuffer size : 5566,\tepsilon : 8.6%, \t r <=40 0.0, \t r > 40 66.66666666666666\n","[Episode 650]\t rewards globals : 4.869 \tavg rewards : 6.667,\tavg loss: : 0.051867,\tbuffer size : 6087,\tepsilon : 7.8%, \t r <=40 0.0, \t r > 40 66.66666666666666\n","[Episode 700]\t rewards globals : 5.007 \tavg rewards : 6.667,\tavg loss: : 0.050082,\tbuffer size : 6576,\tepsilon : 7.1%, \t r <=40 0.0, \t r > 40 66.66666666666666\n","[Episode 750]\t rewards globals : 5.153 \tavg rewards : 7.255,\tavg loss: : 0.048638,\tbuffer size : 7094,\tepsilon : 6.5%, \t r <=40 0.0, \t r > 40 72.54901960784314\n","[Episode 800]\t rewards globals : 5.306 \tavg rewards : 7.647,\tavg loss: : 0.047173,\tbuffer size : 7625,\tepsilon : 5.9%, \t r <=40 0.0, \t r > 40 76.47058823529412\n","[Episode 850]\t rewards globals : 5.405 \tavg rewards : 6.863,\tavg loss: : 0.046893,\tbuffer size : 8129,\tepsilon : 5.4%, \t r <=40 0.0, \t r > 40 68.62745098039215\n","[Episode 900]\t rewards globals : 5.516 \tavg rewards : 7.451,\tavg loss: : 0.045957,\tbuffer size : 8658,\tepsilon : 4.9%, \t r <=40 0.0, \t r > 40 74.50980392156863\n","[Episode 950]\t rewards globals : 5.605 \tavg rewards : 7.255,\tavg loss: : 0.045034,\tbuffer size : 9173,\tepsilon : 4.5%, \t r <=40 0.0, \t r > 40 72.54901960784314\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 8.1\n","[task_2_tmax40] 100 run(s) avg rewards : 8.0\n","Point: 8.05\n","[Episode 1000]\t rewards globals : 5.724 \tavg rewards : 8.039,\tavg loss: : 0.044274,\tbuffer size : 9682,\tepsilon : 4.1%, \t r <=40 0.0, \t r > 40 80.3921568627451\n","[Episode 1050]\t rewards globals : 5.814 \tavg rewards : 7.647,\tavg loss: : 0.044212,\tbuffer size : 10228,\tepsilon : 3.8%, \t r <=40 0.0, \t r > 40 76.47058823529412\n","[Episode 1100]\t rewards globals : 5.867 \tavg rewards : 6.863,\tavg loss: : 0.043773,\tbuffer size : 10737,\tepsilon : 3.5%, \t r <=40 0.0, \t r > 40 68.62745098039215\n","[Episode 1150]\t rewards globals : 5.899 \tavg rewards : 6.667,\tavg loss: : 0.043370,\tbuffer size : 11260,\tepsilon : 3.3%, \t r <=40 0.0, \t r > 40 66.66666666666666\n","[Episode 1200]\t rewards globals : 5.970 \tavg rewards : 7.647,\tavg loss: : 0.042532,\tbuffer size : 11742,\tepsilon : 3.0%, \t r <=40 0.0, \t r > 40 76.47058823529412\n","[Episode 1250]\t rewards globals : 6.043 \tavg rewards : 7.843,\tavg loss: : 0.042765,\tbuffer size : 12265,\tepsilon : 2.8%, \t r <=40 0.0, \t r > 40 78.43137254901961\n","[Episode 1300]\t rewards globals : 6.103 \tavg rewards : 7.647,\tavg loss: : 0.042249,\tbuffer size : 12796,\tepsilon : 2.6%, \t r <=40 0.0, \t r > 40 76.47058823529412\n","[Episode 1350]\t rewards globals : 6.136 \tavg rewards : 7.059,\tavg loss: : 0.041719,\tbuffer size : 13307,\tepsilon : 2.4%, \t r <=40 0.0, \t r > 40 70.58823529411765\n","[Episode 1400]\t rewards globals : 6.174 \tavg rewards : 7.255,\tavg loss: : 0.041501,\tbuffer size : 13775,\tepsilon : 2.3%, \t r <=40 0.0, \t r > 40 72.54901960784314\n","[Episode 1450]\t rewards globals : 6.258 \tavg rewards : 8.627,\tavg loss: : 0.041518,\tbuffer size : 14290,\tepsilon : 2.2%, \t r <=40 0.0, \t r > 40 86.27450980392157\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 8.6\n","[task_2_tmax40] 100 run(s) avg rewards : 7.4\n","Point: 8.0\n","[Episode 1500]\t rewards globals : 6.316 \tavg rewards : 8.039,\tavg loss: : 0.041081,\tbuffer size : 14827,\tepsilon : 2.0%, \t r <=40 0.0, \t r > 40 80.3921568627451\n","[Episode 1550]\t rewards globals : 6.364 \tavg rewards : 7.647,\tavg loss: : 0.040629,\tbuffer size : 15351,\tepsilon : 1.9%, \t r <=40 0.0, \t r > 40 76.47058823529412\n","[Episode 1600]\t rewards globals : 6.408 \tavg rewards : 7.843,\tavg loss: : 0.040042,\tbuffer size : 15870,\tepsilon : 1.8%, \t r <=40 0.0, \t r > 40 78.43137254901961\n","[Episode 1650]\t rewards globals : 6.451 \tavg rewards : 7.843,\tavg loss: : 0.040243,\tbuffer size : 16375,\tepsilon : 1.7%, \t r <=40 0.0, \t r > 40 78.43137254901961\n","[Episode 1700]\t rewards globals : 6.479 \tavg rewards : 7.451,\tavg loss: : 0.040082,\tbuffer size : 16883,\tepsilon : 1.7%, \t r <=40 0.0, \t r > 40 74.50980392156863\n","[Episode 1750]\t rewards globals : 6.522 \tavg rewards : 8.039,\tavg loss: : 0.039761,\tbuffer size : 17360,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 80.3921568627451\n","[Episode 1800]\t rewards globals : 6.585 \tavg rewards : 8.824,\tavg loss: : 0.039278,\tbuffer size : 17871,\tepsilon : 1.5%, \t r <=40 0.0, \t r > 40 88.23529411764706\n","[Episode 1850]\t rewards globals : 6.634 \tavg rewards : 8.431,\tavg loss: : 0.039452,\tbuffer size : 18386,\tepsilon : 1.5%, \t r <=40 0.0, \t r > 40 84.31372549019608\n","[Episode 1900]\t rewards globals : 6.644 \tavg rewards : 7.059,\tavg loss: : 0.039193,\tbuffer size : 18860,\tepsilon : 1.4%, \t r <=40 0.0, \t r > 40 70.58823529411765\n","[Episode 1950]\t rewards globals : 6.653 \tavg rewards : 7.059,\tavg loss: : 0.039075,\tbuffer size : 19375,\tepsilon : 1.4%, \t r <=40 0.0, \t r > 40 70.58823529411765\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 8.3\n","[task_2_tmax40] 100 run(s) avg rewards : 7.6\n","Point: 7.95\n","[Episode 2000]\t rewards globals : 6.712 \tavg rewards : 8.824,\tavg loss: : 0.038843,\tbuffer size : 19896,\tepsilon : 1.3%, \t r <=40 0.0, \t r > 40 88.23529411764706\n","[Episode 2050]\t rewards globals : 6.709 \tavg rewards : 6.667,\tavg loss: : 0.039062,\tbuffer size : 20395,\tepsilon : 1.3%, \t r <=40 0.0, \t r > 40 66.66666666666666\n","[Episode 2100]\t rewards globals : 6.740 \tavg rewards : 7.843,\tavg loss: : 0.038891,\tbuffer size : 20898,\tepsilon : 1.3%, \t r <=40 0.0, \t r > 40 78.43137254901961\n","[Episode 2150]\t rewards globals : 6.764 \tavg rewards : 7.843,\tavg loss: : 0.038594,\tbuffer size : 21402,\tepsilon : 1.2%, \t r <=40 0.0, \t r > 40 78.43137254901961\n","[Episode 2200]\t rewards globals : 6.783 \tavg rewards : 7.647,\tavg loss: : 0.038384,\tbuffer size : 21900,\tepsilon : 1.2%, \t r <=40 0.0, \t r > 40 76.47058823529412\n","error while saving final\n","device is: cuda\n","config : \n"," x debut 25, y_debut 0, max ep 1000, max epsilon 0.3 \n"," Epsilon_decay 200, test_interval 500\n","Save_interval 1000, batch_size 128, buffer_limit 16000 \n"," Methode Mixed Monte Carlo + Conv DQN, gamma_nstep 0.8, nstep 4\n","Start learning from : x_debut 25 y_debut 0\n","[Episode 50]\t rewards globals : 4.118 \tavg rewards : 4.118,\tavg loss: : nan,\tbuffer size : 486,\tepsilon : 23.6%, \t r <=40 0.0, \t r > 40 41.17647058823529\n","[Episode 100]\t rewards globals : 5.149 \tavg rewards : 6.078,\tavg loss: : 0.303961,\tbuffer size : 1091,\tepsilon : 18.6%, \t r <=40 0.0, \t r > 40 60.78431372549019\n","[Episode 150]\t rewards globals : 5.563 \tavg rewards : 6.275,\tavg loss: : 0.154283,\tbuffer size : 1691,\tepsilon : 14.7%, \t r <=40 0.0, \t r > 40 62.745098039215684\n","[Episode 200]\t rewards globals : 5.920 \tavg rewards : 7.059,\tavg loss: : 0.109171,\tbuffer size : 2259,\tepsilon : 11.7%, \t r <=40 0.0, \t r > 40 70.58823529411765\n","[Episode 250]\t rewards globals : 6.135 \tavg rewards : 6.863,\tavg loss: : 0.091622,\tbuffer size : 2863,\tepsilon : 9.3%, \t r <=40 0.0, \t r > 40 68.62745098039215\n","[Episode 300]\t rewards globals : 6.379 \tavg rewards : 7.647,\tavg loss: : 0.078095,\tbuffer size : 3507,\tepsilon : 7.5%, \t r <=40 0.0, \t r > 40 76.47058823529412\n","[Episode 350]\t rewards globals : 6.467 \tavg rewards : 7.059,\tavg loss: : 0.070247,\tbuffer size : 4074,\tepsilon : 6.0%, \t r <=40 0.0, \t r > 40 70.58823529411765\n","[Episode 400]\t rewards globals : 6.658 \tavg rewards : 8.039,\tavg loss: : 0.063920,\tbuffer size : 4688,\tepsilon : 4.9%, \t r <=40 0.0, \t r > 40 80.3921568627451\n","[Episode 450]\t rewards globals : 6.785 \tavg rewards : 7.843,\tavg loss: : 0.060964,\tbuffer size : 5290,\tepsilon : 4.1%, \t r <=40 0.0, \t r > 40 78.43137254901961\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 9.6\n","[task_2_tmax40] 100 run(s) avg rewards : 9.0\n","Point: 9.3\n","Re do test: double check\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 8.8\n","[task_2_tmax40] 100 run(s) avg rewards : 9.4\n","Point: 9.100000000000001\n","Training should stop\n","error while saving final\n","device is: cuda\n","config : \n"," x debut 25, y_debut 1, max ep 1500, max epsilon 0.3 \n"," Epsilon_decay 300, test_interval 500\n","Save_interval 1000, batch_size 128, buffer_limit 20000 \n"," Methode Mixed Monte Carlo + Conv DQN, gamma_nstep 0.8, nstep 4\n","Start learning from : x_debut 25 y_debut 1\n","[Episode 50]\t rewards globals : 3.922 \tavg rewards : 3.922,\tavg loss: : nan,\tbuffer size : 483,\tepsilon : 25.5%, \t r <=40 0.0, \t r > 40 39.21568627450981\n","[Episode 100]\t rewards globals : 4.158 \tavg rewards : 4.510,\tavg loss: : 0.254362,\tbuffer size : 971,\tepsilon : 21.8%, \t r <=40 0.0, \t r > 40 45.09803921568628\n","[Episode 150]\t rewards globals : 4.570 \tavg rewards : 5.490,\tavg loss: : 0.126073,\tbuffer size : 1495,\tepsilon : 18.6%, \t r <=40 0.0, \t r > 40 54.90196078431373\n","[Episode 200]\t rewards globals : 4.876 \tavg rewards : 5.882,\tavg loss: : 0.092047,\tbuffer size : 2043,\tepsilon : 15.9%, \t r <=40 0.0, \t r > 40 58.82352941176471\n","[Episode 250]\t rewards globals : 5.060 \tavg rewards : 5.882,\tavg loss: : 0.078403,\tbuffer size : 2590,\tepsilon : 13.6%, \t r <=40 0.0, \t r > 40 58.82352941176471\n","[Episode 300]\t rewards globals : 5.415 \tavg rewards : 7.255,\tavg loss: : 0.068351,\tbuffer size : 3214,\tepsilon : 11.7%, \t r <=40 0.0, \t r > 40 72.54901960784314\n","[Episode 350]\t rewards globals : 5.641 \tavg rewards : 7.059,\tavg loss: : 0.061214,\tbuffer size : 3795,\tepsilon : 10.0%, \t r <=40 0.0, \t r > 40 70.58823529411765\n","[Episode 400]\t rewards globals : 5.761 \tavg rewards : 6.471,\tavg loss: : 0.057494,\tbuffer size : 4385,\tepsilon : 8.6%, \t r <=40 0.0, \t r > 40 64.70588235294117\n","[Episode 450]\t rewards globals : 5.854 \tavg rewards : 6.667,\tavg loss: : 0.056374,\tbuffer size : 4973,\tepsilon : 7.5%, \t r <=40 0.0, \t r > 40 66.66666666666666\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 8.6\n","[task_2_tmax40] 100 run(s) avg rewards : 8.2\n","Point: 8.399999999999999\n","[Episode 500]\t rewards globals : 6.028 \tavg rewards : 7.647,\tavg loss: : 0.053436,\tbuffer size : 5556,\tepsilon : 6.5%, \t r <=40 0.0, \t r > 40 76.47058823529412\n","[Episode 550]\t rewards globals : 6.152 \tavg rewards : 7.451,\tavg loss: : 0.050628,\tbuffer size : 6187,\tepsilon : 5.6%, \t r <=40 0.0, \t r > 40 74.50980392156863\n","[Episode 600]\t rewards globals : 6.223 \tavg rewards : 7.059,\tavg loss: : 0.048435,\tbuffer size : 6745,\tepsilon : 4.9%, \t r <=40 0.0, \t r > 40 70.58823529411765\n","[Episode 650]\t rewards globals : 6.283 \tavg rewards : 7.059,\tavg loss: : 0.048731,\tbuffer size : 7302,\tepsilon : 4.3%, \t r <=40 0.0, \t r > 40 70.58823529411765\n","[Episode 700]\t rewards globals : 6.377 \tavg rewards : 7.451,\tavg loss: : 0.046716,\tbuffer size : 7882,\tepsilon : 3.8%, \t r <=40 0.0, \t r > 40 74.50980392156863\n","[Episode 750]\t rewards globals : 6.405 \tavg rewards : 6.863,\tavg loss: : 0.045658,\tbuffer size : 8466,\tepsilon : 3.4%, \t r <=40 0.0, \t r > 40 68.62745098039215\n","[Episode 800]\t rewards globals : 6.529 \tavg rewards : 8.431,\tavg loss: : 0.044467,\tbuffer size : 9096,\tepsilon : 3.0%, \t r <=40 0.0, \t r > 40 84.31372549019608\n","[Episode 850]\t rewards globals : 6.639 \tavg rewards : 8.235,\tavg loss: : 0.044172,\tbuffer size : 9721,\tepsilon : 2.7%, \t r <=40 0.0, \t r > 40 82.35294117647058\n","[Episode 900]\t rewards globals : 6.670 \tavg rewards : 7.255,\tavg loss: : 0.043201,\tbuffer size : 10280,\tepsilon : 2.4%, \t r <=40 0.0, \t r > 40 72.54901960784314\n","[Episode 950]\t rewards globals : 6.793 \tavg rewards : 9.020,\tavg loss: : 0.042684,\tbuffer size : 10916,\tepsilon : 2.2%, \t r <=40 0.0, \t r > 40 90.19607843137256\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 9.0\n","[task_2_tmax40] 100 run(s) avg rewards : 8.7\n","Point: 8.85\n","Re do test: double check\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 8.8\n","[task_2_tmax40] 100 run(s) avg rewards : 9.3\n","Point: 9.05\n","Training should stop\n","error while saving final\n","device is: cuda\n","config : \n"," x debut 25, y_debut 2, max ep 2000, max epsilon 0.3 \n"," Epsilon_decay 400, test_interval 500\n","Save_interval 1000, batch_size 128, buffer_limit 24000 \n"," Methode Mixed Monte Carlo + Conv DQN, gamma_nstep 0.8, nstep 4\n","Start learning from : x_debut 25 y_debut 2\n","[Episode 50]\t rewards globals : 2.745 \tavg rewards : 2.745,\tavg loss: : nan,\tbuffer size : 442,\tepsilon : 26.6%, \t r <=40 0.0, \t r > 40 27.450980392156865\n","[Episode 100]\t rewards globals : 3.168 \tavg rewards : 3.725,\tavg loss: : 0.369573,\tbuffer size : 917,\tepsilon : 23.6%, \t r <=40 0.0, \t r > 40 37.254901960784316\n","[Episode 150]\t rewards globals : 3.642 \tavg rewards : 4.510,\tavg loss: : 0.163064,\tbuffer size : 1411,\tepsilon : 20.9%, \t r <=40 0.0, \t r > 40 45.09803921568628\n","[Episode 200]\t rewards globals : 3.831 \tavg rewards : 4.314,\tavg loss: : 0.114904,\tbuffer size : 1856,\tepsilon : 18.6%, \t r <=40 0.0, \t r > 40 43.13725490196079\n","[Episode 250]\t rewards globals : 3.984 \tavg rewards : 4.706,\tavg loss: : 0.097821,\tbuffer size : 2337,\tepsilon : 16.5%, \t r <=40 0.0, \t r > 40 47.05882352941176\n","[Episode 300]\t rewards globals : 4.020 \tavg rewards : 4.314,\tavg loss: : 0.086612,\tbuffer size : 2865,\tepsilon : 14.7%, \t r <=40 0.0, \t r > 40 43.13725490196079\n","[Episode 350]\t rewards globals : 4.274 \tavg rewards : 5.882,\tavg loss: : 0.077517,\tbuffer size : 3367,\tepsilon : 13.1%, \t r <=40 0.0, \t r > 40 58.82352941176471\n","[Episode 400]\t rewards globals : 4.389 \tavg rewards : 5.294,\tavg loss: : 0.072151,\tbuffer size : 3933,\tepsilon : 11.7%, \t r <=40 0.0, \t r > 40 52.94117647058824\n","[Episode 450]\t rewards globals : 4.523 \tavg rewards : 5.686,\tavg loss: : 0.068810,\tbuffer size : 4496,\tepsilon : 10.4%, \t r <=40 0.0, \t r > 40 56.86274509803921\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 7.1\n","[task_2_tmax40] 100 run(s) avg rewards : 7.7\n","Point: 7.4\n","[Episode 500]\t rewards globals : 4.631 \tavg rewards : 5.490,\tavg loss: : 0.064659,\tbuffer size : 5030,\tepsilon : 9.3%, \t r <=40 0.0, \t r > 40 54.90196078431373\n","[Episode 550]\t rewards globals : 4.828 \tavg rewards : 6.863,\tavg loss: : 0.061832,\tbuffer size : 5597,\tepsilon : 8.3%, \t r <=40 0.0, \t r > 40 68.62745098039215\n","[Episode 600]\t rewards globals : 4.942 \tavg rewards : 6.275,\tavg loss: : 0.058970,\tbuffer size : 6168,\tepsilon : 7.5%, \t r <=40 0.0, \t r > 40 62.745098039215684\n","[Episode 650]\t rewards globals : 5.069 \tavg rewards : 6.471,\tavg loss: : 0.057764,\tbuffer size : 6688,\tepsilon : 6.7%, \t r <=40 0.0, \t r > 40 64.70588235294117\n","[Episode 700]\t rewards globals : 5.235 \tavg rewards : 7.255,\tavg loss: : 0.055865,\tbuffer size : 7259,\tepsilon : 6.0%, \t r <=40 0.0, \t r > 40 72.54901960784314\n","[Episode 750]\t rewards globals : 5.313 \tavg rewards : 6.471,\tavg loss: : 0.054278,\tbuffer size : 7832,\tepsilon : 5.4%, \t r <=40 0.0, \t r > 40 64.70588235294117\n","[Episode 800]\t rewards globals : 5.418 \tavg rewards : 6.863,\tavg loss: : 0.053150,\tbuffer size : 8424,\tepsilon : 4.9%, \t r <=40 0.0, \t r > 40 68.62745098039215\n","[Episode 850]\t rewards globals : 5.570 \tavg rewards : 8.039,\tavg loss: : 0.052829,\tbuffer size : 9059,\tepsilon : 4.5%, \t r <=40 0.0, \t r > 40 80.3921568627451\n","[Episode 900]\t rewards globals : 5.683 \tavg rewards : 7.647,\tavg loss: : 0.051461,\tbuffer size : 9642,\tepsilon : 4.1%, \t r <=40 0.0, \t r > 40 76.47058823529412\n","[Episode 950]\t rewards globals : 5.815 \tavg rewards : 8.235,\tavg loss: : 0.050099,\tbuffer size : 10235,\tepsilon : 3.7%, \t r <=40 0.0, \t r > 40 82.35294117647058\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 8.5\n","[task_2_tmax40] 100 run(s) avg rewards : 8.7\n","Point: 8.6\n","Re do test: double check\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 7.6\n","[task_2_tmax40] 100 run(s) avg rewards : 7.9\n","Point: 7.75\n","[Episode 1000]\t rewards globals : 5.904 \tavg rewards : 7.647,\tavg loss: : 0.049182,\tbuffer size : 10840,\tepsilon : 3.4%, \t r <=40 0.0, \t r > 40 76.47058823529412\n","[Episode 1050]\t rewards globals : 6.004 \tavg rewards : 8.039,\tavg loss: : 0.049127,\tbuffer size : 11453,\tepsilon : 3.1%, \t r <=40 0.0, \t r > 40 80.3921568627451\n","[Episode 1100]\t rewards globals : 6.049 \tavg rewards : 7.059,\tavg loss: : 0.048219,\tbuffer size : 12054,\tepsilon : 2.9%, \t r <=40 0.0, \t r > 40 70.58823529411765\n","[Episode 1150]\t rewards globals : 6.116 \tavg rewards : 7.647,\tavg loss: : 0.047460,\tbuffer size : 12652,\tepsilon : 2.6%, \t r <=40 0.0, \t r > 40 76.47058823529412\n","[Episode 1200]\t rewards globals : 6.195 \tavg rewards : 7.843,\tavg loss: : 0.046589,\tbuffer size : 13271,\tepsilon : 2.4%, \t r <=40 0.0, \t r > 40 78.43137254901961\n","[Episode 1250]\t rewards globals : 6.275 \tavg rewards : 8.235,\tavg loss: : 0.046639,\tbuffer size : 13893,\tepsilon : 2.3%, \t r <=40 0.0, \t r > 40 82.35294117647058\n","[Episode 1300]\t rewards globals : 6.357 \tavg rewards : 8.431,\tavg loss: : 0.045767,\tbuffer size : 14513,\tepsilon : 2.1%, \t r <=40 0.0, \t r > 40 84.31372549019608\n","[Episode 1350]\t rewards globals : 6.462 \tavg rewards : 9.020,\tavg loss: : 0.045132,\tbuffer size : 15178,\tepsilon : 2.0%, \t r <=40 0.0, \t r > 40 90.19607843137256\n","[Episode 1400]\t rewards globals : 6.510 \tavg rewards : 7.843,\tavg loss: : 0.044549,\tbuffer size : 15791,\tepsilon : 1.9%, \t r <=40 0.0, \t r > 40 78.43137254901961\n","[Episode 1450]\t rewards globals : 6.561 \tavg rewards : 8.039,\tavg loss: : 0.044621,\tbuffer size : 16433,\tepsilon : 1.8%, \t r <=40 0.0, \t r > 40 80.3921568627451\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 8.2\n","[task_2_tmax40] 100 run(s) avg rewards : 7.8\n","Point: 8.0\n","[Episode 1500]\t rewards globals : 6.576 \tavg rewards : 7.059,\tavg loss: : 0.044628,\tbuffer size : 17086,\tepsilon : 1.7%, \t r <=40 0.0, \t r > 40 70.58823529411765\n","[Episode 1550]\t rewards globals : 6.622 \tavg rewards : 8.039,\tavg loss: : 0.044219,\tbuffer size : 17693,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 80.3921568627451\n","[Episode 1600]\t rewards globals : 6.677 \tavg rewards : 8.431,\tavg loss: : 0.043724,\tbuffer size : 18318,\tepsilon : 1.5%, \t r <=40 0.0, \t r > 40 84.31372549019608\n","[Episode 1650]\t rewards globals : 6.735 \tavg rewards : 8.627,\tavg loss: : 0.043870,\tbuffer size : 18934,\tepsilon : 1.5%, \t r <=40 0.0, \t r > 40 86.27450980392157\n","[Episode 1700]\t rewards globals : 6.778 \tavg rewards : 8.235,\tavg loss: : 0.043411,\tbuffer size : 19539,\tepsilon : 1.4%, \t r <=40 0.0, \t r > 40 82.35294117647058\n","[Episode 1750]\t rewards globals : 6.790 \tavg rewards : 7.255,\tavg loss: : 0.043154,\tbuffer size : 20154,\tepsilon : 1.4%, \t r <=40 0.0, \t r > 40 72.54901960784314\n","[Episode 1800]\t rewards globals : 6.841 \tavg rewards : 8.627,\tavg loss: : 0.042838,\tbuffer size : 20824,\tepsilon : 1.3%, \t r <=40 0.0, \t r > 40 86.27450980392157\n","[Episode 1850]\t rewards globals : 6.872 \tavg rewards : 8.039,\tavg loss: : 0.042918,\tbuffer size : 21432,\tepsilon : 1.3%, \t r <=40 0.0, \t r > 40 80.3921568627451\n","[Episode 1900]\t rewards globals : 6.896 \tavg rewards : 7.647,\tavg loss: : 0.042777,\tbuffer size : 22053,\tepsilon : 1.3%, \t r <=40 0.0, \t r > 40 76.47058823529412\n","[Episode 1950]\t rewards globals : 6.925 \tavg rewards : 7.843,\tavg loss: : 0.042577,\tbuffer size : 22660,\tepsilon : 1.2%, \t r <=40 0.0, \t r > 40 78.43137254901961\n","error while saving final\n","device is: cuda\n","config : \n"," x debut 25, y_debut 3, max ep 2500, max epsilon 0.3 \n"," Epsilon_decay 500, test_interval 500\n","Save_interval 1000, batch_size 128, buffer_limit 28000 \n"," Methode Mixed Monte Carlo + Conv DQN, gamma_nstep 0.8, nstep 4\n","Start learning from : x_debut 25 y_debut 3\n","[Episode 50]\t rewards globals : 4.510 \tavg rewards : 4.510,\tavg loss: : nan,\tbuffer size : 621,\tepsilon : 27.2%, \t r <=40 0.0, \t r > 40 45.09803921568628\n","[Episode 100]\t rewards globals : 3.663 \tavg rewards : 2.745,\tavg loss: : 0.207134,\tbuffer size : 1128,\tepsilon : 24.7%, \t r <=40 0.0, \t r > 40 27.450980392156865\n","[Episode 150]\t rewards globals : 3.841 \tavg rewards : 4.118,\tavg loss: : 0.129157,\tbuffer size : 1713,\tepsilon : 22.5%, \t r <=40 0.0, \t r > 40 41.17647058823529\n","[Episode 200]\t rewards globals : 3.632 \tavg rewards : 2.941,\tavg loss: : 0.101227,\tbuffer size : 2325,\tepsilon : 20.4%, \t r <=40 0.0, \t r > 40 29.411764705882355\n","[Episode 250]\t rewards globals : 3.665 \tavg rewards : 3.725,\tavg loss: : 0.088339,\tbuffer size : 2817,\tepsilon : 18.6%, \t r <=40 0.0, \t r > 40 37.254901960784316\n","[Episode 300]\t rewards globals : 3.887 \tavg rewards : 4.902,\tavg loss: : 0.079402,\tbuffer size : 3438,\tepsilon : 16.9%, \t r <=40 0.0, \t r > 40 49.01960784313725\n","[Episode 350]\t rewards globals : 3.960 \tavg rewards : 4.314,\tavg loss: : 0.073343,\tbuffer size : 4019,\tepsilon : 15.4%, \t r <=40 0.0, \t r > 40 43.13725490196079\n","[Episode 400]\t rewards globals : 4.190 \tavg rewards : 5.686,\tavg loss: : 0.068790,\tbuffer size : 4614,\tepsilon : 14.0%, \t r <=40 0.0, \t r > 40 56.86274509803921\n","[Episode 450]\t rewards globals : 4.191 \tavg rewards : 4.118,\tavg loss: : 0.067356,\tbuffer size : 5249,\tepsilon : 12.8%, \t r <=40 0.0, \t r > 40 41.17647058823529\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 7.7\n","[task_2_tmax40] 100 run(s) avg rewards : 7.5\n","Point: 7.6\n","[Episode 500]\t rewards globals : 4.232 \tavg rewards : 4.706,\tavg loss: : 0.063916,\tbuffer size : 5819,\tepsilon : 11.7%, \t r <=40 0.0, \t r > 40 47.05882352941176\n","[Episode 550]\t rewards globals : 4.374 \tavg rewards : 5.686,\tavg loss: : 0.061275,\tbuffer size : 6476,\tepsilon : 10.7%, \t r <=40 0.0, \t r > 40 56.86274509803921\n","[Episode 600]\t rewards globals : 4.542 \tavg rewards : 6.275,\tavg loss: : 0.059160,\tbuffer size : 7066,\tepsilon : 9.7%, \t r <=40 0.0, \t r > 40 62.745098039215684\n","[Episode 650]\t rewards globals : 4.578 \tavg rewards : 4.902,\tavg loss: : 0.059174,\tbuffer size : 7654,\tepsilon : 8.9%, \t r <=40 0.0, \t r > 40 49.01960784313725\n","[Episode 700]\t rewards globals : 4.779 \tavg rewards : 7.255,\tavg loss: : 0.057158,\tbuffer size : 8268,\tepsilon : 8.2%, \t r <=40 0.0, \t r > 40 72.54901960784314\n","[Episode 750]\t rewards globals : 4.807 \tavg rewards : 5.294,\tavg loss: : 0.055253,\tbuffer size : 8835,\tepsilon : 7.5%, \t r <=40 0.0, \t r > 40 52.94117647058824\n","[Episode 800]\t rewards globals : 4.906 \tavg rewards : 6.275,\tavg loss: : 0.053816,\tbuffer size : 9464,\tepsilon : 6.9%, \t r <=40 0.0, \t r > 40 62.745098039215684\n","[Episode 850]\t rewards globals : 5.029 \tavg rewards : 6.863,\tavg loss: : 0.053477,\tbuffer size : 10085,\tepsilon : 6.3%, \t r <=40 0.0, \t r > 40 68.62745098039215\n","[Episode 900]\t rewards globals : 5.117 \tavg rewards : 6.471,\tavg loss: : 0.052267,\tbuffer size : 10726,\tepsilon : 5.8%, \t r <=40 0.0, \t r > 40 64.70588235294117\n","[Episode 950]\t rewards globals : 5.205 \tavg rewards : 6.863,\tavg loss: : 0.051332,\tbuffer size : 11340,\tepsilon : 5.3%, \t r <=40 0.0, \t r > 40 68.62745098039215\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 8.0\n","[task_2_tmax40] 100 run(s) avg rewards : 8.3\n","Point: 8.15\n","[Episode 1000]\t rewards globals : 5.285 \tavg rewards : 6.667,\tavg loss: : 0.050236,\tbuffer size : 11951,\tepsilon : 4.9%, \t r <=40 0.0, \t r > 40 66.66666666666666\n","[Episode 1050]\t rewards globals : 5.366 \tavg rewards : 7.059,\tavg loss: : 0.050596,\tbuffer size : 12575,\tepsilon : 4.6%, \t r <=40 0.0, \t r > 40 70.58823529411765\n","[Episode 1100]\t rewards globals : 5.431 \tavg rewards : 6.667,\tavg loss: : 0.050081,\tbuffer size : 13238,\tepsilon : 4.2%, \t r <=40 0.0, \t r > 40 66.66666666666666\n","[Episode 1150]\t rewards globals : 5.500 \tavg rewards : 7.059,\tavg loss: : 0.049187,\tbuffer size : 13847,\tepsilon : 3.9%, \t r <=40 0.0, \t r > 40 70.58823529411765\n","[Episode 1200]\t rewards globals : 5.554 \tavg rewards : 6.667,\tavg loss: : 0.048617,\tbuffer size : 14492,\tepsilon : 3.6%, \t r <=40 0.0, \t r > 40 66.66666666666666\n","[Episode 1250]\t rewards globals : 5.620 \tavg rewards : 7.255,\tavg loss: : 0.048975,\tbuffer size : 15151,\tepsilon : 3.4%, \t r <=40 0.0, \t r > 40 72.54901960784314\n","[Episode 1300]\t rewards globals : 5.673 \tavg rewards : 7.059,\tavg loss: : 0.048392,\tbuffer size : 15729,\tepsilon : 3.2%, \t r <=40 0.0, \t r > 40 70.58823529411765\n","[Episode 1350]\t rewards globals : 5.729 \tavg rewards : 7.255,\tavg loss: : 0.047835,\tbuffer size : 16354,\tepsilon : 2.9%, \t r <=40 0.0, \t r > 40 72.54901960784314\n","[Episode 1400]\t rewards globals : 5.803 \tavg rewards : 7.647,\tavg loss: : 0.047258,\tbuffer size : 16983,\tepsilon : 2.8%, \t r <=40 0.0, \t r > 40 76.47058823529412\n","[Episode 1450]\t rewards globals : 5.851 \tavg rewards : 7.255,\tavg loss: : 0.047563,\tbuffer size : 17594,\tepsilon : 2.6%, \t r <=40 0.0, \t r > 40 72.54901960784314\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 8.2\n","[task_2_tmax40] 100 run(s) avg rewards : 7.6\n","Point: 7.8999999999999995\n","[Episode 1500]\t rewards globals : 5.896 \tavg rewards : 7.255,\tavg loss: : 0.047088,\tbuffer size : 18189,\tepsilon : 2.4%, \t r <=40 0.0, \t r > 40 72.54901960784314\n","[Episode 1550]\t rewards globals : 5.945 \tavg rewards : 7.255,\tavg loss: : 0.046527,\tbuffer size : 18790,\tepsilon : 2.3%, \t r <=40 0.0, \t r > 40 72.54901960784314\n","[Episode 1600]\t rewards globals : 6.015 \tavg rewards : 8.235,\tavg loss: : 0.046189,\tbuffer size : 19405,\tepsilon : 2.2%, \t r <=40 0.0, \t r > 40 82.35294117647058\n","[Episode 1650]\t rewards globals : 6.045 \tavg rewards : 7.059,\tavg loss: : 0.046479,\tbuffer size : 20028,\tepsilon : 2.1%, \t r <=40 0.0, \t r > 40 70.58823529411765\n","[Episode 1700]\t rewards globals : 6.079 \tavg rewards : 7.255,\tavg loss: : 0.046172,\tbuffer size : 20619,\tepsilon : 2.0%, \t r <=40 0.0, \t r > 40 72.54901960784314\n","[Episode 1750]\t rewards globals : 6.105 \tavg rewards : 7.059,\tavg loss: : 0.045900,\tbuffer size : 21211,\tepsilon : 1.9%, \t r <=40 0.0, \t r > 40 70.58823529411765\n","[Episode 1800]\t rewards globals : 6.141 \tavg rewards : 7.451,\tavg loss: : 0.045559,\tbuffer size : 21835,\tepsilon : 1.8%, \t r <=40 0.0, \t r > 40 74.50980392156863\n","[Episode 1850]\t rewards globals : 6.132 \tavg rewards : 5.882,\tavg loss: : 0.045958,\tbuffer size : 22457,\tepsilon : 1.7%, \t r <=40 0.0, \t r > 40 58.82352941176471\n","[Episode 1900]\t rewards globals : 6.155 \tavg rewards : 7.059,\tavg loss: : 0.045790,\tbuffer size : 23058,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 70.58823529411765\n","[Episode 1950]\t rewards globals : 6.212 \tavg rewards : 8.431,\tavg loss: : 0.045522,\tbuffer size : 23633,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 84.31372549019608\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 8.2\n","[task_2_tmax40] 100 run(s) avg rewards : 7.6\n","Point: 7.8999999999999995\n","[Episode 2000]\t rewards globals : 6.272 \tavg rewards : 8.431,\tavg loss: : 0.045200,\tbuffer size : 24234,\tepsilon : 1.5%, \t r <=40 0.0, \t r > 40 84.31372549019608\n","[Episode 2050]\t rewards globals : 6.309 \tavg rewards : 7.843,\tavg loss: : 0.045460,\tbuffer size : 24819,\tepsilon : 1.5%, \t r <=40 0.0, \t r > 40 78.43137254901961\n","[Episode 2100]\t rewards globals : 6.354 \tavg rewards : 8.235,\tavg loss: : 0.045162,\tbuffer size : 25460,\tepsilon : 1.4%, \t r <=40 0.0, \t r > 40 82.35294117647058\n","[Episode 2150]\t rewards globals : 6.388 \tavg rewards : 7.843,\tavg loss: : 0.044888,\tbuffer size : 26071,\tepsilon : 1.4%, \t r <=40 0.0, \t r > 40 78.43137254901961\n","[Episode 2200]\t rewards globals : 6.411 \tavg rewards : 7.451,\tavg loss: : 0.044735,\tbuffer size : 26683,\tepsilon : 1.4%, \t r <=40 0.0, \t r > 40 74.50980392156863\n","[Episode 2250]\t rewards globals : 6.424 \tavg rewards : 7.059,\tavg loss: : 0.045071,\tbuffer size : 27301,\tepsilon : 1.3%, \t r <=40 0.0, \t r > 40 70.58823529411765\n","[Episode 2300]\t rewards globals : 6.449 \tavg rewards : 7.451,\tavg loss: : 0.045061,\tbuffer size : 27892,\tepsilon : 1.3%, \t r <=40 0.0, \t r > 40 74.50980392156863\n","[Episode 2350]\t rewards globals : 6.487 \tavg rewards : 8.235,\tavg loss: : 0.044925,\tbuffer size : 28000,\tepsilon : 1.3%, \t r <=40 0.0, \t r > 40 82.35294117647058\n","[Episode 2400]\t rewards globals : 6.518 \tavg rewards : 8.039,\tavg loss: : 0.044613,\tbuffer size : 28000,\tepsilon : 1.2%, \t r <=40 0.0, \t r > 40 80.3921568627451\n","[Episode 2450]\t rewards globals : 6.561 \tavg rewards : 8.627,\tavg loss: : 0.044731,\tbuffer size : 28000,\tepsilon : 1.2%, \t r <=40 0.0, \t r > 40 86.27450980392157\n","error while saving final\n","device is: cuda\n","config : \n"," x debut 25, y_debut 4, max ep 3000, max epsilon 0.3 \n"," Epsilon_decay 600, test_interval 500\n","Save_interval 1000, batch_size 128, buffer_limit 32000 \n"," Methode Mixed Monte Carlo + Conv DQN, gamma_nstep 0.8, nstep 4\n","Start learning from : x_debut 25 y_debut 4\n","[Episode 50]\t rewards globals : 2.157 \tavg rewards : 2.157,\tavg loss: : nan,\tbuffer size : 546,\tepsilon : 27.7%, \t r <=40 0.0, \t r > 40 21.568627450980394\n","[Episode 100]\t rewards globals : 1.980 \tavg rewards : 1.765,\tavg loss: : 0.260606,\tbuffer size : 1027,\tepsilon : 25.5%, \t r <=40 0.0, \t r > 40 17.647058823529413\n","[Episode 150]\t rewards globals : 1.656 \tavg rewards : 0.980,\tavg loss: : 0.148988,\tbuffer size : 1508,\tepsilon : 23.6%, \t r <=40 0.0, \t r > 40 9.803921568627452\n","[Episode 200]\t rewards globals : 1.990 \tavg rewards : 3.137,\tavg loss: : 0.112259,\tbuffer size : 2110,\tepsilon : 21.8%, \t r <=40 0.0, \t r > 40 31.372549019607842\n","[Episode 250]\t rewards globals : 1.952 \tavg rewards : 1.765,\tavg loss: : 0.097999,\tbuffer size : 2656,\tepsilon : 20.1%, \t r <=40 0.0, \t r > 40 17.647058823529413\n","[Episode 300]\t rewards globals : 2.126 \tavg rewards : 2.941,\tavg loss: : 0.087017,\tbuffer size : 3183,\tepsilon : 18.6%, \t r <=40 0.0, \t r > 40 29.411764705882355\n","[Episode 350]\t rewards globals : 2.479 \tavg rewards : 4.706,\tavg loss: : 0.078494,\tbuffer size : 3799,\tepsilon : 17.2%, \t r <=40 0.0, \t r > 40 47.05882352941176\n","[Episode 400]\t rewards globals : 2.569 \tavg rewards : 3.333,\tavg loss: : 0.073360,\tbuffer size : 4382,\tepsilon : 15.9%, \t r <=40 0.0, \t r > 40 33.33333333333333\n","[Episode 450]\t rewards globals : 2.639 \tavg rewards : 3.137,\tavg loss: : 0.071711,\tbuffer size : 4968,\tepsilon : 14.7%, \t r <=40 0.0, \t r > 40 31.372549019607842\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 5.9\n","[task_2_tmax40] 100 run(s) avg rewards : 6.6\n","Point: 6.25\n","[Episode 500]\t rewards globals : 2.635 \tavg rewards : 2.549,\tavg loss: : 0.068391,\tbuffer size : 5548,\tepsilon : 13.6%, \t r <=40 0.0, \t r > 40 25.49019607843137\n","[Episode 550]\t rewards globals : 2.595 \tavg rewards : 2.157,\tavg loss: : 0.065644,\tbuffer size : 6141,\tepsilon : 12.6%, \t r <=40 0.0, \t r > 40 21.568627450980394\n","[Episode 600]\t rewards globals : 2.712 \tavg rewards : 3.922,\tavg loss: : 0.063919,\tbuffer size : 6729,\tepsilon : 11.7%, \t r <=40 0.0, \t r > 40 39.21568627450981\n","[Episode 650]\t rewards globals : 2.842 \tavg rewards : 4.314,\tavg loss: : 0.063707,\tbuffer size : 7431,\tepsilon : 10.8%, \t r <=40 0.0, \t r > 40 43.13725490196079\n","[Episode 700]\t rewards globals : 2.910 \tavg rewards : 3.725,\tavg loss: : 0.062167,\tbuffer size : 8143,\tepsilon : 10.0%, \t r <=40 0.0, \t r > 40 37.254901960784316\n","[Episode 750]\t rewards globals : 2.983 \tavg rewards : 3.922,\tavg loss: : 0.060742,\tbuffer size : 8767,\tepsilon : 9.3%, \t r <=40 0.0, \t r > 40 39.21568627450981\n","[Episode 800]\t rewards globals : 3.059 \tavg rewards : 4.314,\tavg loss: : 0.059327,\tbuffer size : 9385,\tepsilon : 8.6%, \t r <=40 0.0, \t r > 40 43.13725490196079\n","[Episode 850]\t rewards globals : 3.208 \tavg rewards : 5.686,\tavg loss: : 0.059097,\tbuffer size : 10054,\tepsilon : 8.0%, \t r <=40 0.0, \t r > 40 56.86274509803921\n","[Episode 900]\t rewards globals : 3.396 \tavg rewards : 6.471,\tavg loss: : 0.057513,\tbuffer size : 10708,\tepsilon : 7.5%, \t r <=40 0.0, \t r > 40 64.70588235294117\n","[Episode 950]\t rewards globals : 3.575 \tavg rewards : 6.863,\tavg loss: : 0.056429,\tbuffer size : 11365,\tepsilon : 7.0%, \t r <=40 0.0, \t r > 40 68.62745098039215\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 6.5\n","[task_2_tmax40] 100 run(s) avg rewards : 7.2\n","Point: 6.85\n","[Episode 1000]\t rewards globals : 3.686 \tavg rewards : 5.686,\tavg loss: : 0.055667,\tbuffer size : 12069,\tepsilon : 6.5%, \t r <=40 0.0, \t r > 40 56.86274509803921\n","[Episode 1050]\t rewards globals : 3.758 \tavg rewards : 5.098,\tavg loss: : 0.055689,\tbuffer size : 12709,\tepsilon : 6.0%, \t r <=40 0.0, \t r > 40 50.98039215686274\n","[Episode 1100]\t rewards globals : 3.806 \tavg rewards : 4.902,\tavg loss: : 0.054830,\tbuffer size : 13312,\tepsilon : 5.6%, \t r <=40 0.0, \t r > 40 49.01960784313725\n","[Episode 1150]\t rewards globals : 3.884 \tavg rewards : 5.490,\tavg loss: : 0.054270,\tbuffer size : 13976,\tepsilon : 5.3%, \t r <=40 0.0, \t r > 40 54.90196078431373\n","[Episode 1200]\t rewards globals : 3.988 \tavg rewards : 6.471,\tavg loss: : 0.053752,\tbuffer size : 14681,\tepsilon : 4.9%, \t r <=40 0.0, \t r > 40 64.70588235294117\n","[Episode 1250]\t rewards globals : 4.101 \tavg rewards : 6.863,\tavg loss: : 0.053902,\tbuffer size : 15294,\tepsilon : 4.6%, \t r <=40 0.0, \t r > 40 68.62745098039215\n","[Episode 1300]\t rewards globals : 4.174 \tavg rewards : 6.078,\tavg loss: : 0.053292,\tbuffer size : 15919,\tepsilon : 4.3%, \t r <=40 0.0, \t r > 40 60.78431372549019\n","[Episode 1350]\t rewards globals : 4.241 \tavg rewards : 6.078,\tavg loss: : 0.052736,\tbuffer size : 16618,\tepsilon : 4.1%, \t r <=40 0.0, \t r > 40 60.78431372549019\n","[Episode 1400]\t rewards globals : 4.297 \tavg rewards : 5.686,\tavg loss: : 0.052220,\tbuffer size : 17297,\tepsilon : 3.8%, \t r <=40 0.0, \t r > 40 56.86274509803921\n","[Episode 1450]\t rewards globals : 4.356 \tavg rewards : 5.882,\tavg loss: : 0.052625,\tbuffer size : 18010,\tepsilon : 3.6%, \t r <=40 0.0, \t r > 40 58.82352941176471\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 6.7\n","[task_2_tmax40] 100 run(s) avg rewards : 7.7\n","Point: 7.2\n","[Episode 1500]\t rewards globals : 4.397 \tavg rewards : 5.686,\tavg loss: : 0.052174,\tbuffer size : 18622,\tepsilon : 3.4%, \t r <=40 0.0, \t r > 40 56.86274509803921\n","[Episode 1550]\t rewards globals : 4.449 \tavg rewards : 5.882,\tavg loss: : 0.051738,\tbuffer size : 19298,\tepsilon : 3.2%, \t r <=40 0.0, \t r > 40 58.82352941176471\n","[Episode 1600]\t rewards globals : 4.516 \tavg rewards : 6.667,\tavg loss: : 0.051540,\tbuffer size : 19977,\tepsilon : 3.0%, \t r <=40 0.0, \t r > 40 66.66666666666666\n","[Episode 1650]\t rewards globals : 4.555 \tavg rewards : 5.882,\tavg loss: : 0.052023,\tbuffer size : 20633,\tepsilon : 2.9%, \t r <=40 0.0, \t r > 40 58.82352941176471\n","[Episode 1700]\t rewards globals : 4.597 \tavg rewards : 5.882,\tavg loss: : 0.051774,\tbuffer size : 21369,\tepsilon : 2.7%, \t r <=40 0.0, \t r > 40 58.82352941176471\n","[Episode 1750]\t rewards globals : 4.615 \tavg rewards : 5.294,\tavg loss: : 0.051412,\tbuffer size : 22039,\tepsilon : 2.6%, \t r <=40 0.0, \t r > 40 52.94117647058824\n","[Episode 1800]\t rewards globals : 4.631 \tavg rewards : 5.098,\tavg loss: : 0.051264,\tbuffer size : 22752,\tepsilon : 2.4%, \t r <=40 0.0, \t r > 40 50.98039215686274\n","[Episode 1850]\t rewards globals : 4.689 \tavg rewards : 6.863,\tavg loss: : 0.051652,\tbuffer size : 23440,\tepsilon : 2.3%, \t r <=40 0.0, \t r > 40 68.62745098039215\n","[Episode 1900]\t rewards globals : 4.708 \tavg rewards : 5.294,\tavg loss: : 0.051572,\tbuffer size : 24094,\tepsilon : 2.2%, \t r <=40 0.0, \t r > 40 52.94117647058824\n","[Episode 1950]\t rewards globals : 4.751 \tavg rewards : 6.275,\tavg loss: : 0.051429,\tbuffer size : 24779,\tepsilon : 2.1%, \t r <=40 0.0, \t r > 40 62.745098039215684\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 6.9\n","[task_2_tmax40] 100 run(s) avg rewards : 6.7\n","Point: 6.800000000000001\n","[Episode 2000]\t rewards globals : 4.808 \tavg rewards : 6.863,\tavg loss: : 0.051175,\tbuffer size : 25473,\tepsilon : 2.0%, \t r <=40 0.0, \t r > 40 68.62745098039215\n","[Episode 2050]\t rewards globals : 4.827 \tavg rewards : 5.686,\tavg loss: : 0.051702,\tbuffer size : 26122,\tepsilon : 2.0%, \t r <=40 0.0, \t r > 40 56.86274509803921\n","[Episode 2100]\t rewards globals : 4.836 \tavg rewards : 5.294,\tavg loss: : 0.051589,\tbuffer size : 26751,\tepsilon : 1.9%, \t r <=40 0.0, \t r > 40 52.94117647058824\n","[Episode 2150]\t rewards globals : 4.872 \tavg rewards : 6.471,\tavg loss: : 0.051414,\tbuffer size : 27424,\tepsilon : 1.8%, \t r <=40 0.0, \t r > 40 64.70588235294117\n","[Episode 2200]\t rewards globals : 4.893 \tavg rewards : 5.686,\tavg loss: : 0.051248,\tbuffer size : 28093,\tepsilon : 1.7%, \t r <=40 0.0, \t r > 40 56.86274509803921\n","[Episode 2250]\t rewards globals : 4.944 \tavg rewards : 7.255,\tavg loss: : 0.051507,\tbuffer size : 28799,\tepsilon : 1.7%, \t r <=40 0.0, \t r > 40 72.54901960784314\n","[Episode 2300]\t rewards globals : 4.985 \tavg rewards : 6.863,\tavg loss: : 0.051421,\tbuffer size : 29475,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 68.62745098039215\n","[Episode 2350]\t rewards globals : 5.019 \tavg rewards : 6.667,\tavg loss: : 0.051268,\tbuffer size : 30168,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 66.66666666666666\n","[Episode 2400]\t rewards globals : 5.056 \tavg rewards : 6.863,\tavg loss: : 0.051117,\tbuffer size : 30844,\tepsilon : 1.5%, \t r <=40 0.0, \t r > 40 68.62745098039215\n","[Episode 2450]\t rewards globals : 5.063 \tavg rewards : 5.490,\tavg loss: : 0.051401,\tbuffer size : 31524,\tepsilon : 1.5%, \t r <=40 0.0, \t r > 40 54.90196078431373\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 7.5\n","[task_2_tmax40] 100 run(s) avg rewards : 6.8\n","Point: 7.15\n","[Episode 2500]\t rewards globals : 5.090 \tavg rewards : 6.471,\tavg loss: : 0.051191,\tbuffer size : 32000,\tepsilon : 1.4%, \t r <=40 0.0, \t r > 40 64.70588235294117\n","[Episode 2550]\t rewards globals : 5.131 \tavg rewards : 7.255,\tavg loss: : 0.050994,\tbuffer size : 32000,\tepsilon : 1.4%, \t r <=40 0.0, \t r > 40 72.54901960784314\n","[Episode 2600]\t rewards globals : 5.175 \tavg rewards : 7.451,\tavg loss: : 0.050817,\tbuffer size : 32000,\tepsilon : 1.4%, \t r <=40 0.0, \t r > 40 74.50980392156863\n","[Episode 2650]\t rewards globals : 5.213 \tavg rewards : 7.255,\tavg loss: : 0.051012,\tbuffer size : 32000,\tepsilon : 1.4%, \t r <=40 0.0, \t r > 40 72.54901960784314\n","[Episode 2700]\t rewards globals : 5.235 \tavg rewards : 6.275,\tavg loss: : 0.050942,\tbuffer size : 32000,\tepsilon : 1.3%, \t r <=40 0.0, \t r > 40 62.745098039215684\n","[Episode 2750]\t rewards globals : 5.256 \tavg rewards : 6.275,\tavg loss: : 0.050832,\tbuffer size : 32000,\tepsilon : 1.3%, \t r <=40 0.0, \t r > 40 62.745098039215684\n","[Episode 2800]\t rewards globals : 5.291 \tavg rewards : 7.059,\tavg loss: : 0.050657,\tbuffer size : 32000,\tepsilon : 1.3%, \t r <=40 0.0, \t r > 40 70.58823529411765\n","[Episode 2850]\t rewards globals : 5.321 \tavg rewards : 7.059,\tavg loss: : 0.050901,\tbuffer size : 32000,\tepsilon : 1.3%, \t r <=40 0.0, \t r > 40 70.58823529411765\n","[Episode 2900]\t rewards globals : 5.340 \tavg rewards : 6.471,\tavg loss: : 0.050834,\tbuffer size : 32000,\tepsilon : 1.2%, \t r <=40 0.0, \t r > 40 64.70588235294117\n","[Episode 2950]\t rewards globals : 5.361 \tavg rewards : 6.667,\tavg loss: : 0.050656,\tbuffer size : 32000,\tepsilon : 1.2%, \t r <=40 0.0, \t r > 40 66.66666666666666\n","error while saving final\n","device is: cuda\n","config : \n"," x debut 30, y_debut 0, max ep 1250, max epsilon 0.3 \n"," Epsilon_decay 250, test_interval 500\n","Save_interval 1000, batch_size 128, buffer_limit 20000 \n"," Methode Mixed Monte Carlo + Conv DQN, gamma_nstep 0.8, nstep 4\n","Start learning from : x_debut 30 y_debut 0\n","[Episode 50]\t rewards globals : 4.118 \tavg rewards : 4.118,\tavg loss: : nan,\tbuffer size : 618,\tepsilon : 24.7%, \t r <=40 0.0, \t r > 40 41.17647058823529\n","[Episode 100]\t rewards globals : 4.752 \tavg rewards : 5.294,\tavg loss: : 0.228206,\tbuffer size : 1284,\tepsilon : 20.4%, \t r <=40 0.0, \t r > 40 52.94117647058824\n","[Episode 150]\t rewards globals : 4.967 \tavg rewards : 5.294,\tavg loss: : 0.139116,\tbuffer size : 1939,\tepsilon : 16.9%, \t r <=40 0.0, \t r > 40 52.94117647058824\n","[Episode 200]\t rewards globals : 5.124 \tavg rewards : 5.686,\tavg loss: : 0.105905,\tbuffer size : 2573,\tepsilon : 14.0%, \t r <=40 0.0, \t r > 40 56.86274509803921\n","[Episode 250]\t rewards globals : 5.259 \tavg rewards : 5.882,\tavg loss: : 0.093672,\tbuffer size : 3246,\tepsilon : 11.7%, \t r <=40 0.0, \t r > 40 58.82352941176471\n","[Episode 300]\t rewards globals : 5.449 \tavg rewards : 6.275,\tavg loss: : 0.083495,\tbuffer size : 3934,\tepsilon : 9.7%, \t r <=40 0.0, \t r > 40 62.745098039215684\n","[Episode 350]\t rewards globals : 5.613 \tavg rewards : 6.667,\tavg loss: : 0.075423,\tbuffer size : 4633,\tepsilon : 8.2%, \t r <=40 0.0, \t r > 40 66.66666666666666\n","[Episode 400]\t rewards globals : 5.810 \tavg rewards : 7.059,\tavg loss: : 0.070978,\tbuffer size : 5399,\tepsilon : 6.9%, \t r <=40 0.0, \t r > 40 70.58823529411765\n","[Episode 450]\t rewards globals : 6.053 \tavg rewards : 8.039,\tavg loss: : 0.069228,\tbuffer size : 6166,\tepsilon : 5.8%, \t r <=40 0.0, \t r > 40 80.3921568627451\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 8.6\n","[task_2_tmax40] 100 run(s) avg rewards : 7.9\n","Point: 8.25\n","[Episode 500]\t rewards globals : 6.267 \tavg rewards : 8.235,\tavg loss: : 0.064977,\tbuffer size : 6931,\tepsilon : 4.9%, \t r <=40 0.0, \t r > 40 82.35294117647058\n","[Episode 550]\t rewards globals : 6.298 \tavg rewards : 6.667,\tavg loss: : 0.062490,\tbuffer size : 7680,\tepsilon : 4.2%, \t r <=40 0.0, \t r > 40 66.66666666666666\n","[Episode 600]\t rewards globals : 6.522 \tavg rewards : 9.020,\tavg loss: : 0.059456,\tbuffer size : 8510,\tepsilon : 3.6%, \t r <=40 0.0, \t r > 40 90.19607843137256\n","[Episode 650]\t rewards globals : 6.621 \tavg rewards : 7.843,\tavg loss: : 0.059351,\tbuffer size : 9298,\tepsilon : 3.2%, \t r <=40 0.0, \t r > 40 78.43137254901961\n","[Episode 700]\t rewards globals : 6.762 \tavg rewards : 8.627,\tavg loss: : 0.057217,\tbuffer size : 10110,\tepsilon : 2.8%, \t r <=40 0.0, \t r > 40 86.27450980392157\n","[Episode 750]\t rewards globals : 6.831 \tavg rewards : 7.843,\tavg loss: : 0.055592,\tbuffer size : 10894,\tepsilon : 2.4%, \t r <=40 0.0, \t r > 40 78.43137254901961\n","[Episode 800]\t rewards globals : 6.954 \tavg rewards : 8.627,\tavg loss: : 0.054274,\tbuffer size : 11746,\tepsilon : 2.2%, \t r <=40 0.0, \t r > 40 86.27450980392157\n","[Episode 850]\t rewards globals : 6.980 \tavg rewards : 7.451,\tavg loss: : 0.054548,\tbuffer size : 12505,\tepsilon : 2.0%, \t r <=40 0.0, \t r > 40 74.50980392156863\n","[Episode 900]\t rewards globals : 7.092 \tavg rewards : 9.020,\tavg loss: : 0.053332,\tbuffer size : 13308,\tepsilon : 1.8%, \t r <=40 0.0, \t r > 40 90.19607843137256\n","[Episode 950]\t rewards globals : 7.161 \tavg rewards : 8.431,\tavg loss: : 0.052400,\tbuffer size : 14093,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 84.31372549019608\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 9.1\n","[task_2_tmax40] 100 run(s) avg rewards : 8.8\n","Point: 8.95\n","Re do test: double check\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 8.4\n","[task_2_tmax40] 100 run(s) avg rewards : 8.3\n","Point: 8.350000000000001\n","[Episode 1000]\t rewards globals : 7.243 \tavg rewards : 8.824,\tavg loss: : 0.051281,\tbuffer size : 14922,\tepsilon : 1.5%, \t r <=40 0.0, \t r > 40 88.23529411764706\n","[Episode 1050]\t rewards globals : 7.307 \tavg rewards : 8.627,\tavg loss: : 0.051338,\tbuffer size : 15693,\tepsilon : 1.4%, \t r <=40 0.0, \t r > 40 86.27450980392157\n","[Episode 1100]\t rewards globals : 7.348 \tavg rewards : 8.235,\tavg loss: : 0.050393,\tbuffer size : 16442,\tepsilon : 1.4%, \t r <=40 0.0, \t r > 40 82.35294117647058\n","[Episode 1150]\t rewards globals : 7.394 \tavg rewards : 8.431,\tavg loss: : 0.049532,\tbuffer size : 17284,\tepsilon : 1.3%, \t r <=40 0.0, \t r > 40 84.31372549019608\n","[Episode 1200]\t rewards globals : 7.435 \tavg rewards : 8.431,\tavg loss: : 0.048850,\tbuffer size : 18111,\tepsilon : 1.2%, \t r <=40 0.0, \t r > 40 84.31372549019608\n","error while saving final\n","device is: cuda\n","config : \n"," x debut 30, y_debut 1, max ep 1750, max epsilon 0.3 \n"," Epsilon_decay 350, test_interval 500\n","Save_interval 1000, batch_size 128, buffer_limit 24000 \n"," Methode Mixed Monte Carlo + Conv DQN, gamma_nstep 0.8, nstep 4\n","Start learning from : x_debut 30 y_debut 1\n","[Episode 50]\t rewards globals : 3.333 \tavg rewards : 3.333,\tavg loss: : nan,\tbuffer size : 566,\tepsilon : 26.1%, \t r <=40 0.0, \t r > 40 33.33333333333333\n","[Episode 100]\t rewards globals : 2.970 \tavg rewards : 2.745,\tavg loss: : 0.234840,\tbuffer size : 1125,\tepsilon : 22.8%, \t r <=40 0.0, \t r > 40 27.450980392156865\n","[Episode 150]\t rewards globals : 3.311 \tavg rewards : 3.922,\tavg loss: : 0.136601,\tbuffer size : 1756,\tepsilon : 19.9%, \t r <=40 0.0, \t r > 40 39.21568627450981\n","[Episode 200]\t rewards globals : 3.831 \tavg rewards : 5.294,\tavg loss: : 0.106070,\tbuffer size : 2473,\tepsilon : 17.4%, \t r <=40 0.0, \t r > 40 52.94117647058824\n","[Episode 250]\t rewards globals : 4.024 \tavg rewards : 4.902,\tavg loss: : 0.094656,\tbuffer size : 3128,\tepsilon : 15.2%, \t r <=40 0.0, \t r > 40 49.01960784313725\n","[Episode 300]\t rewards globals : 4.319 \tavg rewards : 5.882,\tavg loss: : 0.083648,\tbuffer size : 3861,\tepsilon : 13.3%, \t r <=40 0.0, \t r > 40 58.82352941176471\n","[Episode 350]\t rewards globals : 4.587 \tavg rewards : 6.275,\tavg loss: : 0.076145,\tbuffer size : 4623,\tepsilon : 11.7%, \t r <=40 0.0, \t r > 40 62.745098039215684\n","[Episode 400]\t rewards globals : 4.888 \tavg rewards : 7.059,\tavg loss: : 0.070917,\tbuffer size : 5374,\tepsilon : 10.2%, \t r <=40 0.0, \t r > 40 70.58823529411765\n","[Episode 450]\t rewards globals : 5.055 \tavg rewards : 6.275,\tavg loss: : 0.069409,\tbuffer size : 6134,\tepsilon : 9.0%, \t r <=40 0.0, \t r > 40 62.745098039215684\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 7.5\n","[task_2_tmax40] 100 run(s) avg rewards : 8.4\n","Point: 7.95\n","[Episode 500]\t rewards globals : 5.269 \tavg rewards : 7.255,\tavg loss: : 0.066085,\tbuffer size : 6893,\tepsilon : 7.9%, \t r <=40 0.0, \t r > 40 72.54901960784314\n","[Episode 550]\t rewards globals : 5.372 \tavg rewards : 6.275,\tavg loss: : 0.062860,\tbuffer size : 7634,\tepsilon : 7.0%, \t r <=40 0.0, \t r > 40 62.745098039215684\n","[Episode 600]\t rewards globals : 5.441 \tavg rewards : 6.078,\tavg loss: : 0.060587,\tbuffer size : 8360,\tepsilon : 6.2%, \t r <=40 0.0, \t r > 40 60.78431372549019\n","[Episode 650]\t rewards globals : 5.576 \tavg rewards : 7.059,\tavg loss: : 0.061049,\tbuffer size : 9134,\tepsilon : 5.5%, \t r <=40 0.0, \t r > 40 70.58823529411765\n","[Episode 700]\t rewards globals : 5.692 \tavg rewards : 7.059,\tavg loss: : 0.059153,\tbuffer size : 9887,\tepsilon : 4.9%, \t r <=40 0.0, \t r > 40 70.58823529411765\n","[Episode 750]\t rewards globals : 5.766 \tavg rewards : 6.863,\tavg loss: : 0.057675,\tbuffer size : 10636,\tepsilon : 4.4%, \t r <=40 0.0, \t r > 40 68.62745098039215\n","[Episode 800]\t rewards globals : 5.918 \tavg rewards : 8.235,\tavg loss: : 0.056044,\tbuffer size : 11414,\tepsilon : 3.9%, \t r <=40 0.0, \t r > 40 82.35294117647058\n","[Episode 850]\t rewards globals : 6.016 \tavg rewards : 7.647,\tavg loss: : 0.056490,\tbuffer size : 12182,\tepsilon : 3.6%, \t r <=40 0.0, \t r > 40 76.47058823529412\n","[Episode 900]\t rewards globals : 6.115 \tavg rewards : 7.843,\tavg loss: : 0.055299,\tbuffer size : 12934,\tepsilon : 3.2%, \t r <=40 0.0, \t r > 40 78.43137254901961\n","[Episode 950]\t rewards globals : 6.204 \tavg rewards : 7.647,\tavg loss: : 0.054530,\tbuffer size : 13719,\tepsilon : 2.9%, \t r <=40 0.0, \t r > 40 76.47058823529412\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 8.0\n","[task_2_tmax40] 100 run(s) avg rewards : 9.1\n","Point: 8.55\n","Re do test: double check\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 9.0\n","[task_2_tmax40] 100 run(s) avg rewards : 8.6\n","Point: 8.8\n","Training should stop\n","error while saving final\n","device is: cuda\n","config : \n"," x debut 30, y_debut 2, max ep 2250, max epsilon 0.3 \n"," Epsilon_decay 450, test_interval 500\n","Save_interval 1000, batch_size 128, buffer_limit 28000 \n"," Methode Mixed Monte Carlo + Conv DQN, gamma_nstep 0.8, nstep 4\n","Start learning from : x_debut 30 y_debut 2\n","[Episode 50]\t rewards globals : 2.353 \tavg rewards : 2.353,\tavg loss: : nan,\tbuffer size : 464,\tepsilon : 27.0%, \t r <=40 0.0, \t r > 40 23.52941176470588\n","[Episode 100]\t rewards globals : 2.673 \tavg rewards : 3.137,\tavg loss: : 0.376175,\tbuffer size : 997,\tepsilon : 24.2%, \t r <=40 0.0, \t r > 40 31.372549019607842\n","[Episode 150]\t rewards globals : 2.781 \tavg rewards : 2.941,\tavg loss: : 0.186599,\tbuffer size : 1588,\tepsilon : 21.8%, \t r <=40 0.0, \t r > 40 29.411764705882355\n","[Episode 200]\t rewards globals : 2.786 \tavg rewards : 2.745,\tavg loss: : 0.135657,\tbuffer size : 2223,\tepsilon : 19.6%, \t r <=40 0.0, \t r > 40 27.450980392156865\n","[Episode 250]\t rewards globals : 2.908 \tavg rewards : 3.333,\tavg loss: : 0.116651,\tbuffer size : 2853,\tepsilon : 17.6%, \t r <=40 0.0, \t r > 40 33.33333333333333\n","[Episode 300]\t rewards globals : 2.924 \tavg rewards : 2.941,\tavg loss: : 0.101429,\tbuffer size : 3529,\tepsilon : 15.9%, \t r <=40 0.0, \t r > 40 29.411764705882355\n","[Episode 350]\t rewards globals : 3.077 \tavg rewards : 3.922,\tavg loss: : 0.090498,\tbuffer size : 4135,\tepsilon : 14.3%, \t r <=40 0.0, \t r > 40 39.21568627450981\n","[Episode 400]\t rewards globals : 3.317 \tavg rewards : 4.902,\tavg loss: : 0.083288,\tbuffer size : 4821,\tepsilon : 12.9%, \t r <=40 0.0, \t r > 40 49.01960784313725\n","[Episode 450]\t rewards globals : 3.392 \tavg rewards : 4.118,\tavg loss: : 0.081122,\tbuffer size : 5501,\tepsilon : 11.7%, \t r <=40 0.0, \t r > 40 41.17647058823529\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 7.6\n","[task_2_tmax40] 100 run(s) avg rewards : 6.8\n","Point: 7.199999999999999\n","[Episode 500]\t rewards globals : 3.673 \tavg rewards : 6.078,\tavg loss: : 0.077064,\tbuffer size : 6267,\tepsilon : 10.5%, \t r <=40 0.0, \t r > 40 60.78431372549019\n","[Episode 550]\t rewards globals : 3.811 \tavg rewards : 5.098,\tavg loss: : 0.073745,\tbuffer size : 6898,\tepsilon : 9.5%, \t r <=40 0.0, \t r > 40 50.98039215686274\n","[Episode 600]\t rewards globals : 3.910 \tavg rewards : 4.902,\tavg loss: : 0.070814,\tbuffer size : 7584,\tepsilon : 8.6%, \t r <=40 0.0, \t r > 40 49.01960784313725\n","[Episode 650]\t rewards globals : 3.932 \tavg rewards : 4.118,\tavg loss: : 0.070294,\tbuffer size : 8226,\tepsilon : 7.8%, \t r <=40 0.0, \t r > 40 41.17647058823529\n","[Episode 700]\t rewards globals : 4.037 \tavg rewards : 5.294,\tavg loss: : 0.068153,\tbuffer size : 8980,\tepsilon : 7.1%, \t r <=40 0.0, \t r > 40 52.94117647058824\n","[Episode 750]\t rewards globals : 4.168 \tavg rewards : 5.882,\tavg loss: : 0.066025,\tbuffer size : 9736,\tepsilon : 6.5%, \t r <=40 0.0, \t r > 40 58.82352941176471\n","[Episode 800]\t rewards globals : 4.332 \tavg rewards : 6.667,\tavg loss: : 0.064419,\tbuffer size : 10431,\tepsilon : 5.9%, \t r <=40 0.0, \t r > 40 66.66666666666666\n","[Episode 850]\t rewards globals : 4.454 \tavg rewards : 6.471,\tavg loss: : 0.064371,\tbuffer size : 11169,\tepsilon : 5.4%, \t r <=40 0.0, \t r > 40 64.70588235294117\n","[Episode 900]\t rewards globals : 4.539 \tavg rewards : 5.882,\tavg loss: : 0.063148,\tbuffer size : 11901,\tepsilon : 4.9%, \t r <=40 0.0, \t r > 40 58.82352941176471\n","[Episode 950]\t rewards globals : 4.616 \tavg rewards : 6.078,\tavg loss: : 0.061992,\tbuffer size : 12626,\tepsilon : 4.5%, \t r <=40 0.0, \t r > 40 60.78431372549019\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 7.8\n","[task_2_tmax40] 100 run(s) avg rewards : 8.4\n","Point: 8.1\n","[Episode 1000]\t rewards globals : 4.675 \tavg rewards : 5.882,\tavg loss: : 0.060716,\tbuffer size : 13363,\tepsilon : 4.1%, \t r <=40 0.0, \t r > 40 58.82352941176471\n","[Episode 1050]\t rewards globals : 4.729 \tavg rewards : 5.882,\tavg loss: : 0.061101,\tbuffer size : 14106,\tepsilon : 3.8%, \t r <=40 0.0, \t r > 40 58.82352941176471\n","[Episode 1100]\t rewards globals : 4.823 \tavg rewards : 6.667,\tavg loss: : 0.060307,\tbuffer size : 14844,\tepsilon : 3.5%, \t r <=40 0.0, \t r > 40 66.66666666666666\n","[Episode 1150]\t rewards globals : 4.891 \tavg rewards : 6.471,\tavg loss: : 0.059503,\tbuffer size : 15604,\tepsilon : 3.3%, \t r <=40 0.0, \t r > 40 64.70588235294117\n","[Episode 1200]\t rewards globals : 4.988 \tavg rewards : 7.255,\tavg loss: : 0.058661,\tbuffer size : 16412,\tepsilon : 3.0%, \t r <=40 0.0, \t r > 40 72.54901960784314\n","[Episode 1250]\t rewards globals : 5.068 \tavg rewards : 7.059,\tavg loss: : 0.058878,\tbuffer size : 17081,\tepsilon : 2.8%, \t r <=40 0.0, \t r > 40 70.58823529411765\n","[Episode 1300]\t rewards globals : 5.135 \tavg rewards : 6.667,\tavg loss: : 0.058115,\tbuffer size : 17815,\tepsilon : 2.6%, \t r <=40 0.0, \t r > 40 66.66666666666666\n","[Episode 1350]\t rewards globals : 5.181 \tavg rewards : 6.471,\tavg loss: : 0.057584,\tbuffer size : 18506,\tepsilon : 2.4%, \t r <=40 0.0, \t r > 40 64.70588235294117\n","[Episode 1400]\t rewards globals : 5.225 \tavg rewards : 6.471,\tavg loss: : 0.056971,\tbuffer size : 19258,\tepsilon : 2.3%, \t r <=40 0.0, \t r > 40 64.70588235294117\n","[Episode 1450]\t rewards globals : 5.307 \tavg rewards : 7.647,\tavg loss: : 0.057153,\tbuffer size : 20057,\tepsilon : 2.2%, \t r <=40 0.0, \t r > 40 76.47058823529412\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 7.8\n","[task_2_tmax40] 100 run(s) avg rewards : 6.8\n","Point: 7.3\n","[Episode 1500]\t rewards globals : 5.336 \tavg rewards : 6.275,\tavg loss: : 0.056625,\tbuffer size : 20806,\tepsilon : 2.0%, \t r <=40 0.0, \t r > 40 62.745098039215684\n","[Episode 1550]\t rewards globals : 5.377 \tavg rewards : 6.667,\tavg loss: : 0.056231,\tbuffer size : 21581,\tepsilon : 1.9%, \t r <=40 0.0, \t r > 40 66.66666666666666\n","[Episode 1600]\t rewards globals : 5.428 \tavg rewards : 6.863,\tavg loss: : 0.055642,\tbuffer size : 22332,\tepsilon : 1.8%, \t r <=40 0.0, \t r > 40 68.62745098039215\n","[Episode 1650]\t rewards globals : 5.482 \tavg rewards : 7.255,\tavg loss: : 0.055784,\tbuffer size : 23042,\tepsilon : 1.7%, \t r <=40 0.0, \t r > 40 72.54901960784314\n","[Episode 1700]\t rewards globals : 5.573 \tavg rewards : 8.431,\tavg loss: : 0.055347,\tbuffer size : 23816,\tepsilon : 1.7%, \t r <=40 0.0, \t r > 40 84.31372549019608\n","[Episode 1750]\t rewards globals : 5.625 \tavg rewards : 7.451,\tavg loss: : 0.054902,\tbuffer size : 24586,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 74.50980392156863\n","[Episode 1800]\t rewards globals : 5.686 \tavg rewards : 7.843,\tavg loss: : 0.054698,\tbuffer size : 25339,\tepsilon : 1.5%, \t r <=40 0.0, \t r > 40 78.43137254901961\n","[Episode 1850]\t rewards globals : 5.743 \tavg rewards : 7.647,\tavg loss: : 0.055087,\tbuffer size : 26145,\tepsilon : 1.5%, \t r <=40 0.0, \t r > 40 76.47058823529412\n","[Episode 1900]\t rewards globals : 5.771 \tavg rewards : 6.863,\tavg loss: : 0.054981,\tbuffer size : 26863,\tepsilon : 1.4%, \t r <=40 0.0, \t r > 40 68.62745098039215\n","[Episode 1950]\t rewards globals : 5.823 \tavg rewards : 7.843,\tavg loss: : 0.054741,\tbuffer size : 27683,\tepsilon : 1.4%, \t r <=40 0.0, \t r > 40 78.43137254901961\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 8.1\n","[task_2_tmax40] 100 run(s) avg rewards : 7.8\n","Point: 7.949999999999999\n","[Episode 2000]\t rewards globals : 5.867 \tavg rewards : 7.647,\tavg loss: : 0.054476,\tbuffer size : 28000,\tepsilon : 1.3%, \t r <=40 0.0, \t r > 40 76.47058823529412\n","[Episode 2050]\t rewards globals : 5.919 \tavg rewards : 8.039,\tavg loss: : 0.054826,\tbuffer size : 28000,\tepsilon : 1.3%, \t r <=40 0.0, \t r > 40 80.3921568627451\n","[Episode 2100]\t rewards globals : 5.959 \tavg rewards : 7.647,\tavg loss: : 0.054581,\tbuffer size : 28000,\tepsilon : 1.3%, \t r <=40 0.0, \t r > 40 76.47058823529412\n","[Episode 2150]\t rewards globals : 5.993 \tavg rewards : 7.451,\tavg loss: : 0.054459,\tbuffer size : 28000,\tepsilon : 1.2%, \t r <=40 0.0, \t r > 40 74.50980392156863\n","[Episode 2200]\t rewards globals : 6.015 \tavg rewards : 7.059,\tavg loss: : 0.054217,\tbuffer size : 28000,\tepsilon : 1.2%, \t r <=40 0.0, \t r > 40 70.58823529411765\n","error while saving final\n","device is: cuda\n","config : \n"," x debut 30, y_debut 3, max ep 2750, max epsilon 0.3 \n"," Epsilon_decay 550, test_interval 500\n","Save_interval 1000, batch_size 128, buffer_limit 32000 \n"," Methode Mixed Monte Carlo + Conv DQN, gamma_nstep 0.8, nstep 4\n","Start learning from : x_debut 30 y_debut 3\n","[Episode 50]\t rewards globals : 2.745 \tavg rewards : 2.745,\tavg loss: : 2.282401,\tbuffer size : 642,\tepsilon : 27.5%, \t r <=40 0.0, \t r > 40 27.450980392156865\n","[Episode 100]\t rewards globals : 2.970 \tavg rewards : 3.137,\tavg loss: : 0.180729,\tbuffer size : 1294,\tepsilon : 25.2%, \t r <=40 0.0, \t r > 40 31.372549019607842\n","[Episode 150]\t rewards globals : 2.914 \tavg rewards : 2.941,\tavg loss: : 0.117195,\tbuffer size : 1953,\tepsilon : 23.1%, \t r <=40 0.0, \t r > 40 29.411764705882355\n","[Episode 200]\t rewards globals : 3.035 \tavg rewards : 3.333,\tavg loss: : 0.095721,\tbuffer size : 2617,\tepsilon : 21.2%, \t r <=40 0.0, \t r > 40 33.33333333333333\n","[Episode 250]\t rewards globals : 3.068 \tavg rewards : 3.137,\tavg loss: : 0.090364,\tbuffer size : 3307,\tepsilon : 19.4%, \t r <=40 0.0, \t r > 40 31.372549019607842\n","[Episode 300]\t rewards globals : 3.223 \tavg rewards : 4.118,\tavg loss: : 0.081749,\tbuffer size : 4066,\tepsilon : 17.8%, \t r <=40 0.0, \t r > 40 41.17647058823529\n","[Episode 350]\t rewards globals : 3.191 \tavg rewards : 3.137,\tavg loss: : 0.075024,\tbuffer size : 4694,\tepsilon : 16.3%, \t r <=40 0.0, \t r > 40 31.372549019607842\n","[Episode 400]\t rewards globals : 3.541 \tavg rewards : 6.078,\tavg loss: : 0.069737,\tbuffer size : 5442,\tepsilon : 15.0%, \t r <=40 0.0, \t r > 40 60.78431372549019\n","[Episode 450]\t rewards globals : 3.636 \tavg rewards : 4.314,\tavg loss: : 0.069660,\tbuffer size : 6197,\tepsilon : 13.8%, \t r <=40 0.0, \t r > 40 43.13725490196079\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 7.7\n","[task_2_tmax40] 100 run(s) avg rewards : 7.5\n","Point: 7.6\n","[Episode 500]\t rewards globals : 3.673 \tavg rewards : 3.922,\tavg loss: : 0.066773,\tbuffer size : 6899,\tepsilon : 12.7%, \t r <=40 0.0, \t r > 40 39.21568627450981\n","[Episode 550]\t rewards globals : 3.757 \tavg rewards : 4.706,\tavg loss: : 0.064353,\tbuffer size : 7628,\tepsilon : 11.7%, \t r <=40 0.0, \t r > 40 47.05882352941176\n","[Episode 600]\t rewards globals : 3.927 \tavg rewards : 5.882,\tavg loss: : 0.062257,\tbuffer size : 8398,\tepsilon : 10.7%, \t r <=40 0.0, \t r > 40 58.82352941176471\n","[Episode 650]\t rewards globals : 4.101 \tavg rewards : 6.275,\tavg loss: : 0.063005,\tbuffer size : 9179,\tepsilon : 9.9%, \t r <=40 0.0, \t r > 40 62.745098039215684\n","[Episode 700]\t rewards globals : 4.151 \tavg rewards : 4.902,\tavg loss: : 0.061543,\tbuffer size : 9930,\tepsilon : 9.1%, \t r <=40 0.0, \t r > 40 49.01960784313725\n","[Episode 750]\t rewards globals : 4.261 \tavg rewards : 5.686,\tavg loss: : 0.060627,\tbuffer size : 10675,\tepsilon : 8.4%, \t r <=40 0.0, \t r > 40 56.86274509803921\n","[Episode 800]\t rewards globals : 4.332 \tavg rewards : 5.294,\tavg loss: : 0.059658,\tbuffer size : 11453,\tepsilon : 7.8%, \t r <=40 0.0, \t r > 40 52.94117647058824\n","[Episode 850]\t rewards globals : 4.407 \tavg rewards : 5.686,\tavg loss: : 0.060561,\tbuffer size : 12254,\tepsilon : 7.2%, \t r <=40 0.0, \t r > 40 56.86274509803921\n","[Episode 900]\t rewards globals : 4.528 \tavg rewards : 6.471,\tavg loss: : 0.059699,\tbuffer size : 13041,\tepsilon : 6.6%, \t r <=40 0.0, \t r > 40 64.70588235294117\n","[Episode 950]\t rewards globals : 4.627 \tavg rewards : 6.275,\tavg loss: : 0.058715,\tbuffer size : 13823,\tepsilon : 6.2%, \t r <=40 0.0, \t r > 40 62.745098039215684\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 7.4\n","[task_2_tmax40] 100 run(s) avg rewards : 7.8\n","Point: 7.6\n","[Episode 1000]\t rewards globals : 4.745 \tavg rewards : 7.059,\tavg loss: : 0.057867,\tbuffer size : 14653,\tepsilon : 5.7%, \t r <=40 0.0, \t r > 40 70.58823529411765\n","[Episode 1050]\t rewards globals : 4.853 \tavg rewards : 6.863,\tavg loss: : 0.058717,\tbuffer size : 15455,\tepsilon : 5.3%, \t r <=40 0.0, \t r > 40 68.62745098039215\n","[Episode 1100]\t rewards globals : 4.914 \tavg rewards : 6.078,\tavg loss: : 0.058112,\tbuffer size : 16261,\tepsilon : 4.9%, \t r <=40 0.0, \t r > 40 60.78431372549019\n","[Episode 1150]\t rewards globals : 4.987 \tavg rewards : 6.667,\tavg loss: : 0.057363,\tbuffer size : 17093,\tepsilon : 4.6%, \t r <=40 0.0, \t r > 40 66.66666666666666\n","[Episode 1200]\t rewards globals : 5.062 \tavg rewards : 6.667,\tavg loss: : 0.056715,\tbuffer size : 17868,\tepsilon : 4.3%, \t r <=40 0.0, \t r > 40 66.66666666666666\n","[Episode 1250]\t rewards globals : 5.172 \tavg rewards : 7.647,\tavg loss: : 0.057304,\tbuffer size : 18711,\tepsilon : 4.0%, \t r <=40 0.0, \t r > 40 76.47058823529412\n","[Episode 1300]\t rewards globals : 5.242 \tavg rewards : 7.059,\tavg loss: : 0.056608,\tbuffer size : 19502,\tepsilon : 3.7%, \t r <=40 0.0, \t r > 40 70.58823529411765\n","[Episode 1350]\t rewards globals : 5.285 \tavg rewards : 6.471,\tavg loss: : 0.056162,\tbuffer size : 20328,\tepsilon : 3.5%, \t r <=40 0.0, \t r > 40 64.70588235294117\n","[Episode 1400]\t rewards globals : 5.375 \tavg rewards : 7.843,\tavg loss: : 0.055594,\tbuffer size : 21140,\tepsilon : 3.3%, \t r <=40 0.0, \t r > 40 78.43137254901961\n","[Episode 1450]\t rewards globals : 5.403 \tavg rewards : 6.078,\tavg loss: : 0.056210,\tbuffer size : 21933,\tepsilon : 3.1%, \t r <=40 0.0, \t r > 40 60.78431372549019\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 7.7\n","[task_2_tmax40] 100 run(s) avg rewards : 7.8\n","Point: 7.75\n","[Episode 1500]\t rewards globals : 5.483 \tavg rewards : 7.647,\tavg loss: : 0.055896,\tbuffer size : 22762,\tepsilon : 2.9%, \t r <=40 0.0, \t r > 40 76.47058823529412\n","[Episode 1550]\t rewards globals : 5.513 \tavg rewards : 6.471,\tavg loss: : 0.055419,\tbuffer size : 23567,\tepsilon : 2.7%, \t r <=40 0.0, \t r > 40 64.70588235294117\n","[Episode 1600]\t rewards globals : 5.578 \tavg rewards : 7.451,\tavg loss: : 0.055210,\tbuffer size : 24362,\tepsilon : 2.6%, \t r <=40 0.0, \t r > 40 74.50980392156863\n","[Episode 1650]\t rewards globals : 5.615 \tavg rewards : 6.863,\tavg loss: : 0.055581,\tbuffer size : 25168,\tepsilon : 2.4%, \t r <=40 0.0, \t r > 40 68.62745098039215\n","[Episode 1700]\t rewards globals : 5.644 \tavg rewards : 6.667,\tavg loss: : 0.055319,\tbuffer size : 25988,\tepsilon : 2.3%, \t r <=40 0.0, \t r > 40 66.66666666666666\n","[Episode 1750]\t rewards globals : 5.682 \tavg rewards : 7.059,\tavg loss: : 0.055157,\tbuffer size : 26800,\tepsilon : 2.2%, \t r <=40 0.0, \t r > 40 70.58823529411765\n","[Episode 1800]\t rewards globals : 5.752 \tavg rewards : 8.039,\tavg loss: : 0.054838,\tbuffer size : 27616,\tepsilon : 2.1%, \t r <=40 0.0, \t r > 40 80.3921568627451\n","[Episode 1850]\t rewards globals : 5.786 \tavg rewards : 6.863,\tavg loss: : 0.055170,\tbuffer size : 28458,\tepsilon : 2.0%, \t r <=40 0.0, \t r > 40 68.62745098039215\n","[Episode 1900]\t rewards globals : 5.823 \tavg rewards : 7.255,\tavg loss: : 0.055000,\tbuffer size : 29255,\tepsilon : 1.9%, \t r <=40 0.0, \t r > 40 72.54901960784314\n","[Episode 1950]\t rewards globals : 5.879 \tavg rewards : 8.039,\tavg loss: : 0.054655,\tbuffer size : 30044,\tepsilon : 1.8%, \t r <=40 0.0, \t r > 40 80.3921568627451\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 8.7\n","[task_2_tmax40] 100 run(s) avg rewards : 7.5\n","Point: 8.1\n","[Episode 2000]\t rewards globals : 5.907 \tavg rewards : 6.863,\tavg loss: : 0.054307,\tbuffer size : 30818,\tepsilon : 1.8%, \t r <=40 0.0, \t r > 40 68.62745098039215\n","[Episode 2050]\t rewards globals : 5.963 \tavg rewards : 8.235,\tavg loss: : 0.054458,\tbuffer size : 31615,\tepsilon : 1.7%, \t r <=40 0.0, \t r > 40 82.35294117647058\n","[Episode 2100]\t rewards globals : 5.997 \tavg rewards : 7.451,\tavg loss: : 0.054493,\tbuffer size : 32000,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 74.50980392156863\n","[Episode 2150]\t rewards globals : 6.053 \tavg rewards : 8.431,\tavg loss: : 0.054449,\tbuffer size : 32000,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 84.31372549019608\n","[Episode 2200]\t rewards globals : 6.102 \tavg rewards : 8.039,\tavg loss: : 0.054362,\tbuffer size : 32000,\tepsilon : 1.5%, \t r <=40 0.0, \t r > 40 80.3921568627451\n","[Episode 2250]\t rewards globals : 6.153 \tavg rewards : 8.431,\tavg loss: : 0.054566,\tbuffer size : 32000,\tepsilon : 1.5%, \t r <=40 0.0, \t r > 40 84.31372549019608\n","[Episode 2300]\t rewards globals : 6.163 \tavg rewards : 6.667,\tavg loss: : 0.054370,\tbuffer size : 32000,\tepsilon : 1.4%, \t r <=40 0.0, \t r > 40 66.66666666666666\n","[Episode 2350]\t rewards globals : 6.185 \tavg rewards : 7.255,\tavg loss: : 0.054253,\tbuffer size : 32000,\tepsilon : 1.4%, \t r <=40 0.0, \t r > 40 72.54901960784314\n","[Episode 2400]\t rewards globals : 6.197 \tavg rewards : 6.863,\tavg loss: : 0.054109,\tbuffer size : 32000,\tepsilon : 1.4%, \t r <=40 0.0, \t r > 40 68.62745098039215\n","[Episode 2450]\t rewards globals : 6.230 \tavg rewards : 7.843,\tavg loss: : 0.054513,\tbuffer size : 32000,\tepsilon : 1.3%, \t r <=40 0.0, \t r > 40 78.43137254901961\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 7.9\n","[task_2_tmax40] 100 run(s) avg rewards : 7.3\n","Point: 7.6\n","[Episode 2500]\t rewards globals : 6.230 \tavg rewards : 6.275,\tavg loss: : 0.054464,\tbuffer size : 32000,\tepsilon : 1.3%, \t r <=40 0.0, \t r > 40 62.745098039215684\n","[Episode 2550]\t rewards globals : 6.249 \tavg rewards : 7.059,\tavg loss: : 0.054412,\tbuffer size : 32000,\tepsilon : 1.3%, \t r <=40 0.0, \t r > 40 70.58823529411765\n","[Episode 2600]\t rewards globals : 6.263 \tavg rewards : 7.059,\tavg loss: : 0.054344,\tbuffer size : 32000,\tepsilon : 1.3%, \t r <=40 0.0, \t r > 40 70.58823529411765\n","[Episode 2650]\t rewards globals : 6.300 \tavg rewards : 8.039,\tavg loss: : 0.054576,\tbuffer size : 32000,\tepsilon : 1.2%, \t r <=40 0.0, \t r > 40 80.3921568627451\n","[Episode 2700]\t rewards globals : 6.338 \tavg rewards : 8.431,\tavg loss: : 0.054513,\tbuffer size : 32000,\tepsilon : 1.2%, \t r <=40 0.0, \t r > 40 84.31372549019608\n","error while saving final\n","device is: cuda\n","config : \n"," x debut 30, y_debut 4, max ep 3250, max epsilon 0.3 \n"," Epsilon_decay 650, test_interval 500\n","Save_interval 1000, batch_size 128, buffer_limit 36000 \n"," Methode Mixed Monte Carlo + Conv DQN, gamma_nstep 0.8, nstep 4\n","Start learning from : x_debut 30 y_debut 4\n","[Episode 50]\t rewards globals : 1.373 \tavg rewards : 1.373,\tavg loss: : nan,\tbuffer size : 516,\tepsilon : 27.9%, \t r <=40 0.0, \t r > 40 13.725490196078432\n","[Episode 100]\t rewards globals : 1.584 \tavg rewards : 1.765,\tavg loss: : 0.266242,\tbuffer size : 1112,\tepsilon : 25.9%, \t r <=40 0.0, \t r > 40 17.647058823529413\n","[Episode 150]\t rewards globals : 1.589 \tavg rewards : 1.569,\tavg loss: : 0.145319,\tbuffer size : 1696,\tepsilon : 24.0%, \t r <=40 0.0, \t r > 40 15.686274509803921\n","[Episode 200]\t rewards globals : 1.592 \tavg rewards : 1.569,\tavg loss: : 0.110503,\tbuffer size : 2296,\tepsilon : 22.3%, \t r <=40 0.0, \t r > 40 15.686274509803921\n","[Episode 250]\t rewards globals : 1.912 \tavg rewards : 3.137,\tavg loss: : 0.097149,\tbuffer size : 2938,\tepsilon : 20.7%, \t r <=40 0.0, \t r > 40 31.372549019607842\n","[Episode 300]\t rewards globals : 1.960 \tavg rewards : 2.157,\tavg loss: : 0.086877,\tbuffer size : 3530,\tepsilon : 19.3%, \t r <=40 0.0, \t r > 40 21.568627450980394\n","[Episode 350]\t rewards globals : 1.937 \tavg rewards : 1.765,\tavg loss: : 0.079385,\tbuffer size : 4160,\tepsilon : 17.9%, \t r <=40 0.0, \t r > 40 17.647058823529413\n","[Episode 400]\t rewards globals : 1.895 \tavg rewards : 1.569,\tavg loss: : 0.073530,\tbuffer size : 4838,\tepsilon : 16.7%, \t r <=40 0.0, \t r > 40 15.686274509803921\n","[Episode 450]\t rewards globals : 2.151 \tavg rewards : 4.118,\tavg loss: : 0.072728,\tbuffer size : 5645,\tepsilon : 15.5%, \t r <=40 0.0, \t r > 40 41.17647058823529\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 3.8\n","[task_2_tmax40] 100 run(s) avg rewards : 4.5\n","Point: 4.15\n","[Episode 500]\t rewards globals : 2.056 \tavg rewards : 1.176,\tavg loss: : 0.069979,\tbuffer size : 6294,\tepsilon : 14.4%, \t r <=40 0.0, \t r > 40 11.76470588235294\n","[Episode 550]\t rewards globals : 1.996 \tavg rewards : 1.373,\tavg loss: : 0.067015,\tbuffer size : 6999,\tepsilon : 13.4%, \t r <=40 0.0, \t r > 40 13.725490196078432\n","[Episode 600]\t rewards globals : 2.047 \tavg rewards : 2.549,\tavg loss: : 0.065210,\tbuffer size : 7782,\tepsilon : 12.5%, \t r <=40 0.0, \t r > 40 25.49019607843137\n","[Episode 650]\t rewards globals : 2.120 \tavg rewards : 2.941,\tavg loss: : 0.065410,\tbuffer size : 8575,\tepsilon : 11.7%, \t r <=40 0.0, \t r > 40 29.411764705882355\n","[Episode 700]\t rewards globals : 2.225 \tavg rewards : 3.529,\tavg loss: : 0.064447,\tbuffer size : 9441,\tepsilon : 10.9%, \t r <=40 0.0, \t r > 40 35.294117647058826\n","[Episode 750]\t rewards globals : 2.317 \tavg rewards : 3.725,\tavg loss: : 0.063000,\tbuffer size : 10248,\tepsilon : 10.1%, \t r <=40 0.0, \t r > 40 37.254901960784316\n","[Episode 800]\t rewards globals : 2.360 \tavg rewards : 2.941,\tavg loss: : 0.061415,\tbuffer size : 11029,\tepsilon : 9.5%, \t r <=40 0.0, \t r > 40 29.411764705882355\n","[Episode 850]\t rewards globals : 2.432 \tavg rewards : 3.529,\tavg loss: : 0.062726,\tbuffer size : 11837,\tepsilon : 8.8%, \t r <=40 0.0, \t r > 40 35.294117647058826\n","[Episode 900]\t rewards globals : 2.553 \tavg rewards : 4.510,\tavg loss: : 0.061794,\tbuffer size : 12643,\tepsilon : 8.3%, \t r <=40 0.0, \t r > 40 45.09803921568628\n","[Episode 950]\t rewards globals : 2.608 \tavg rewards : 3.725,\tavg loss: : 0.061039,\tbuffer size : 13442,\tepsilon : 7.7%, \t r <=40 0.0, \t r > 40 37.254901960784316\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 4.5\n","[task_2_tmax40] 100 run(s) avg rewards : 4.8\n","Point: 4.65\n","[Episode 1000]\t rewards globals : 2.607 \tavg rewards : 2.549,\tavg loss: : 0.060193,\tbuffer size : 14156,\tepsilon : 7.2%, \t r <=40 0.0, \t r > 40 25.49019607843137\n","[Episode 1050]\t rewards globals : 2.664 \tavg rewards : 3.725,\tavg loss: : 0.060800,\tbuffer size : 14947,\tepsilon : 6.8%, \t r <=40 0.0, \t r > 40 37.254901960784316\n","[Episode 1100]\t rewards globals : 2.752 \tavg rewards : 4.510,\tavg loss: : 0.059933,\tbuffer size : 15741,\tepsilon : 6.3%, \t r <=40 0.0, \t r > 40 45.09803921568628\n","[Episode 1150]\t rewards globals : 2.876 \tavg rewards : 5.490,\tavg loss: : 0.059370,\tbuffer size : 16506,\tepsilon : 5.9%, \t r <=40 0.0, \t r > 40 54.90196078431373\n","[Episode 1200]\t rewards globals : 2.914 \tavg rewards : 3.922,\tavg loss: : 0.058653,\tbuffer size : 17329,\tepsilon : 5.6%, \t r <=40 0.0, \t r > 40 39.21568627450981\n","[Episode 1250]\t rewards globals : 2.974 \tavg rewards : 4.314,\tavg loss: : 0.058966,\tbuffer size : 18171,\tepsilon : 5.2%, \t r <=40 0.0, \t r > 40 43.13725490196079\n","[Episode 1300]\t rewards globals : 2.975 \tavg rewards : 2.941,\tavg loss: : 0.058668,\tbuffer size : 18943,\tepsilon : 4.9%, \t r <=40 0.0, \t r > 40 29.411764705882355\n","[Episode 1350]\t rewards globals : 3.020 \tavg rewards : 4.314,\tavg loss: : 0.058447,\tbuffer size : 19779,\tepsilon : 4.6%, \t r <=40 0.0, \t r > 40 43.13725490196079\n","[Episode 1400]\t rewards globals : 3.048 \tavg rewards : 3.725,\tavg loss: : 0.057977,\tbuffer size : 20599,\tepsilon : 4.4%, \t r <=40 0.0, \t r > 40 37.254901960784316\n","[Episode 1450]\t rewards globals : 3.074 \tavg rewards : 3.725,\tavg loss: : 0.058619,\tbuffer size : 21447,\tepsilon : 4.1%, \t r <=40 0.0, \t r > 40 37.254901960784316\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 5.1\n","[task_2_tmax40] 100 run(s) avg rewards : 4.5\n","Point: 4.8\n","[Episode 1500]\t rewards globals : 3.118 \tavg rewards : 4.314,\tavg loss: : 0.058285,\tbuffer size : 22323,\tepsilon : 3.9%, \t r <=40 0.0, \t r > 40 43.13725490196079\n","[Episode 1550]\t rewards globals : 3.198 \tavg rewards : 5.686,\tavg loss: : 0.057961,\tbuffer size : 23185,\tepsilon : 3.7%, \t r <=40 0.0, \t r > 40 56.86274509803921\n","[Episode 1600]\t rewards globals : 3.217 \tavg rewards : 3.922,\tavg loss: : 0.057686,\tbuffer size : 23973,\tepsilon : 3.5%, \t r <=40 0.0, \t r > 40 39.21568627450981\n","[Episode 1650]\t rewards globals : 3.240 \tavg rewards : 4.118,\tavg loss: : 0.058347,\tbuffer size : 24792,\tepsilon : 3.3%, \t r <=40 0.0, \t r > 40 41.17647058823529\n","[Episode 1700]\t rewards globals : 3.298 \tavg rewards : 5.294,\tavg loss: : 0.058165,\tbuffer size : 25609,\tepsilon : 3.1%, \t r <=40 0.0, \t r > 40 52.94117647058824\n","[Episode 1750]\t rewards globals : 3.330 \tavg rewards : 4.314,\tavg loss: : 0.057982,\tbuffer size : 26445,\tepsilon : 3.0%, \t r <=40 0.0, \t r > 40 43.13725490196079\n","[Episode 1800]\t rewards globals : 3.365 \tavg rewards : 4.510,\tavg loss: : 0.057820,\tbuffer size : 27294,\tepsilon : 2.8%, \t r <=40 0.0, \t r > 40 45.09803921568628\n","[Episode 1850]\t rewards globals : 3.441 \tavg rewards : 6.275,\tavg loss: : 0.058261,\tbuffer size : 28176,\tepsilon : 2.7%, \t r <=40 0.0, \t r > 40 62.745098039215684\n","[Episode 1900]\t rewards globals : 3.519 \tavg rewards : 6.471,\tavg loss: : 0.058178,\tbuffer size : 29010,\tepsilon : 2.6%, \t r <=40 0.0, \t r > 40 64.70588235294117\n","[Episode 1950]\t rewards globals : 3.526 \tavg rewards : 3.922,\tavg loss: : 0.058010,\tbuffer size : 29843,\tepsilon : 2.4%, \t r <=40 0.0, \t r > 40 39.21568627450981\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 6.4\n","[task_2_tmax40] 100 run(s) avg rewards : 5.9\n","Point: 6.15\n","[Episode 2000]\t rewards globals : 3.558 \tavg rewards : 4.902,\tavg loss: : 0.057734,\tbuffer size : 30667,\tepsilon : 2.3%, \t r <=40 0.0, \t r > 40 49.01960784313725\n","[Episode 2050]\t rewards globals : 3.584 \tavg rewards : 4.706,\tavg loss: : 0.058070,\tbuffer size : 31538,\tepsilon : 2.2%, \t r <=40 0.0, \t r > 40 47.05882352941176\n","[Episode 2100]\t rewards globals : 3.617 \tavg rewards : 4.902,\tavg loss: : 0.057985,\tbuffer size : 32415,\tepsilon : 2.1%, \t r <=40 0.0, \t r > 40 49.01960784313725\n","[Episode 2150]\t rewards globals : 3.654 \tavg rewards : 5.098,\tavg loss: : 0.058010,\tbuffer size : 33311,\tepsilon : 2.1%, \t r <=40 0.0, \t r > 40 50.98039215686274\n","[Episode 2200]\t rewards globals : 3.698 \tavg rewards : 5.686,\tavg loss: : 0.057848,\tbuffer size : 34127,\tepsilon : 2.0%, \t r <=40 0.0, \t r > 40 56.86274509803921\n","[Episode 2250]\t rewards globals : 3.736 \tavg rewards : 5.294,\tavg loss: : 0.058320,\tbuffer size : 34948,\tepsilon : 1.9%, \t r <=40 0.0, \t r > 40 52.94117647058824\n","[Episode 2300]\t rewards globals : 3.764 \tavg rewards : 4.902,\tavg loss: : 0.058186,\tbuffer size : 35792,\tepsilon : 1.8%, \t r <=40 0.0, \t r > 40 49.01960784313725\n","[Episode 2350]\t rewards globals : 3.790 \tavg rewards : 5.098,\tavg loss: : 0.058071,\tbuffer size : 36000,\tepsilon : 1.8%, \t r <=40 0.0, \t r > 40 50.98039215686274\n","[Episode 2400]\t rewards globals : 3.828 \tavg rewards : 5.490,\tavg loss: : 0.057892,\tbuffer size : 36000,\tepsilon : 1.7%, \t r <=40 0.0, \t r > 40 54.90196078431373\n","[Episode 2450]\t rewards globals : 3.831 \tavg rewards : 4.118,\tavg loss: : 0.058222,\tbuffer size : 36000,\tepsilon : 1.7%, \t r <=40 0.0, \t r > 40 41.17647058823529\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 5.7\n","[task_2_tmax40] 100 run(s) avg rewards : 6.0\n","Point: 5.85\n","[Episode 2500]\t rewards globals : 3.846 \tavg rewards : 4.510,\tavg loss: : 0.058330,\tbuffer size : 36000,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 45.09803921568628\n","[Episode 2550]\t rewards globals : 3.897 \tavg rewards : 6.275,\tavg loss: : 0.058308,\tbuffer size : 36000,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 62.745098039215684\n","[Episode 2600]\t rewards globals : 3.925 \tavg rewards : 5.490,\tavg loss: : 0.058214,\tbuffer size : 36000,\tepsilon : 1.5%, \t r <=40 0.0, \t r > 40 54.90196078431373\n","[Episode 2650]\t rewards globals : 3.942 \tavg rewards : 4.902,\tavg loss: : 0.058612,\tbuffer size : 36000,\tepsilon : 1.5%, \t r <=40 0.0, \t r > 40 49.01960784313725\n","[Episode 2700]\t rewards globals : 3.961 \tavg rewards : 4.902,\tavg loss: : 0.058730,\tbuffer size : 36000,\tepsilon : 1.5%, \t r <=40 0.0, \t r > 40 49.01960784313725\n","[Episode 2750]\t rewards globals : 4.009 \tavg rewards : 6.667,\tavg loss: : 0.058749,\tbuffer size : 36000,\tepsilon : 1.4%, \t r <=40 0.0, \t r > 40 66.66666666666666\n","[Episode 2800]\t rewards globals : 4.045 \tavg rewards : 6.078,\tavg loss: : 0.058712,\tbuffer size : 36000,\tepsilon : 1.4%, \t r <=40 0.0, \t r > 40 60.78431372549019\n","[Episode 2850]\t rewards globals : 4.090 \tavg rewards : 6.667,\tavg loss: : 0.059090,\tbuffer size : 36000,\tepsilon : 1.4%, \t r <=40 0.0, \t r > 40 66.66666666666666\n","[Episode 2900]\t rewards globals : 4.102 \tavg rewards : 4.706,\tavg loss: : 0.059052,\tbuffer size : 36000,\tepsilon : 1.3%, \t r <=40 0.0, \t r > 40 47.05882352941176\n","[Episode 2950]\t rewards globals : 4.110 \tavg rewards : 4.706,\tavg loss: : 0.059043,\tbuffer size : 36000,\tepsilon : 1.3%, \t r <=40 0.0, \t r > 40 47.05882352941176\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 5.8\n","[task_2_tmax40] 100 run(s) avg rewards : 5.6\n","Point: 5.699999999999999\n","[Episode 3000]\t rewards globals : 4.119 \tavg rewards : 4.510,\tavg loss: : 0.058999,\tbuffer size : 36000,\tepsilon : 1.3%, \t r <=40 0.0, \t r > 40 45.09803921568628\n","[Episode 3050]\t rewards globals : 4.130 \tavg rewards : 4.706,\tavg loss: : 0.059439,\tbuffer size : 36000,\tepsilon : 1.3%, \t r <=40 0.0, \t r > 40 47.05882352941176\n","[Episode 3100]\t rewards globals : 4.147 \tavg rewards : 5.294,\tavg loss: : 0.059448,\tbuffer size : 36000,\tepsilon : 1.2%, \t r <=40 0.0, \t r > 40 52.94117647058824\n","[Episode 3150]\t rewards globals : 4.148 \tavg rewards : 4.314,\tavg loss: : 0.059426,\tbuffer size : 36000,\tepsilon : 1.2%, \t r <=40 0.0, \t r > 40 43.13725490196079\n","[Episode 3200]\t rewards globals : 4.171 \tavg rewards : 5.490,\tavg loss: : 0.059381,\tbuffer size : 36000,\tepsilon : 1.2%, \t r <=40 0.0, \t r > 40 54.90196078431373\n","error while saving final\n","device is: cuda\n","config : \n"," x debut 30, y_debut 5, max ep 3750, max epsilon 0.3 \n"," Epsilon_decay 750, test_interval 500\n","Save_interval 1000, batch_size 128, buffer_limit 40000 \n"," Methode Mixed Monte Carlo + Conv DQN, gamma_nstep 0.8, nstep 4\n","Start learning from : x_debut 30 y_debut 5\n","[Episode 50]\t rewards globals : 1.765 \tavg rewards : 1.765,\tavg loss: : nan,\tbuffer size : 620,\tepsilon : 28.1%, \t r <=40 0.0, \t r > 40 17.647058823529413\n","[Episode 100]\t rewards globals : 1.584 \tavg rewards : 1.373,\tavg loss: : 0.174743,\tbuffer size : 1214,\tepsilon : 26.4%, \t r <=40 0.0, \t r > 40 13.725490196078432\n","[Episode 150]\t rewards globals : 1.656 \tavg rewards : 1.961,\tavg loss: : 0.109839,\tbuffer size : 1869,\tepsilon : 24.7%, \t r <=40 0.0, \t r > 40 19.607843137254903\n","[Episode 200]\t rewards globals : 1.791 \tavg rewards : 2.157,\tavg loss: : 0.088347,\tbuffer size : 2612,\tepsilon : 23.2%, \t r <=40 0.0, \t r > 40 21.568627450980394\n","[Episode 250]\t rewards globals : 1.514 \tavg rewards : 0.392,\tavg loss: : 0.081585,\tbuffer size : 3314,\tepsilon : 21.8%, \t r <=40 0.0, \t r > 40 3.9215686274509802\n","[Episode 300]\t rewards globals : 1.462 \tavg rewards : 1.176,\tavg loss: : 0.074168,\tbuffer size : 4078,\tepsilon : 20.4%, \t r <=40 0.0, \t r > 40 11.76470588235294\n","[Episode 350]\t rewards globals : 1.425 \tavg rewards : 1.176,\tavg loss: : 0.068247,\tbuffer size : 4693,\tepsilon : 19.2%, \t r <=40 0.0, \t r > 40 11.76470588235294\n","[Episode 400]\t rewards globals : 1.397 \tavg rewards : 1.176,\tavg loss: : 0.064133,\tbuffer size : 5381,\tepsilon : 18.0%, \t r <=40 0.0, \t r > 40 11.76470588235294\n","[Episode 450]\t rewards globals : 1.397 \tavg rewards : 1.373,\tavg loss: : 0.064442,\tbuffer size : 6097,\tepsilon : 16.9%, \t r <=40 0.0, \t r > 40 13.725490196078432\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 3.5\n","[task_2_tmax40] 100 run(s) avg rewards : 1.6\n","Point: 2.55\n","[Episode 500]\t rewards globals : 1.397 \tavg rewards : 1.373,\tavg loss: : 0.061923,\tbuffer size : 6855,\tepsilon : 15.9%, \t r <=40 0.0, \t r > 40 13.725490196078432\n","[Episode 550]\t rewards globals : 1.379 \tavg rewards : 1.176,\tavg loss: : 0.059698,\tbuffer size : 7621,\tepsilon : 14.9%, \t r <=40 0.0, \t r > 40 11.76470588235294\n","[Episode 600]\t rewards globals : 1.398 \tavg rewards : 1.569,\tavg loss: : 0.058034,\tbuffer size : 8318,\tepsilon : 14.0%, \t r <=40 0.0, \t r > 40 15.686274509803921\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cjrdeWB-JA1x","colab_type":"code","outputId":"72888116-9cc6-47f6-af92-5beb74a6986f","executionInfo":{"status":"ok","timestamp":1587459585845,"user_tz":-480,"elapsed":4793,"user":{"displayName":"N","photoUrl":"","userId":"08619141603155082384"}},"colab":{"base_uri":"https://localhost:8080/","height":402}},"source":["# Authentification / Initialization of workspace \n","\n","import os\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)\n","\n","# Create repo folders \n","\n","task_1 = False\n","# If False task_2 workplace will be downloaded\n","\n","if task_1:\n","  number_task = 1\n","else:\n","  number_task = 2\n","\n","root = '/content'\n","local_download = os.path.join(root,'HW3_Task{}'.format(number_task))\n","\n","if not(os.path.exists(local_download)): \n","  os.mkdir(local_download)\n","\n","local_download_agent = os.path.join(local_download,'agent')\n","\n","if not(os.path.exists(local_download_agent)): \n","  os.mkdir(local_download_agent)\n","\n","  # Download files \n","\n","\n","def download_list(file_list,path_name):\n","    \n","  error_l = [];\n","    \n","  for f in file_list:\n","    # 3. Create & download by id.\n","  \n","    print('file found : title: %s, id: %s' % (f['title'], f['id']))\n","    try:\n","      #print('title: %s, id: %s' % (f['title'], f['id']))\n","      fname = os.path.join(path_name, f['title'])\n","      \n","      # Download only .py files\n","      if fname[-3:] == \".py\":\n","        print('downloading to {}'.format(fname))\n","        f_ = drive.CreateFile({'id': f['id']})\n","        f_.GetContentFile(fname)\n","      \n","    except:\n","      print(\"there is an error\")          \n","      error_l.append(fname)\n","\n","\n","if task_1:\n","  # Initial folder\n","  local_download_path = local_download_agent\n","\n","  # Agent files \n","  file_list = drive.ListFile(\n","        {'q': \"'1ktZR8KIIWEi8Cre92SFomSEchWleHtSu' in parents\"}).GetList()\n","\n","  download_list(file_list,local_download_path)\n","\n","\n","  # Initial files\n","  local_download_path = local_download\n","\n","  file_list = drive.ListFile(\n","        {'q': \"'1x4sJIKHA6NZ5y78AnOJeCZP8abI4gLLO' in parents\"}).GetList()\n","\n","  download_list(file_list,local_download_path)\n","\n","else:\n","\n","  # Initial folder\n","  local_download_path = local_download_agent\n","\n","  # Agent files \n","  file_list = drive.ListFile(\n","        {'q': \"'1PUrHkG6ki4JsiXtKOtvc2vAuR6IJMnCa' in parents\"}).GetList()\n","\n","  download_list(file_list,local_download_path)\n","\n","\n","  # Initial files\n","  local_download_path = local_download\n","\n","  file_list = drive.ListFile(\n","        {'q': \"'17wsroYnRmoTt-OIzDJRrUNvVFmohBVFR' in parents\"}).GetList()\n","\n","  download_list(file_list,local_download_path)\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["file found : title: models.py, id: 1BW6dbcbWvmC7JbqE78269K1IXIPwtlEi\n","downloading to /content/HW3_Task2/agent/models.py\n","file found : title: __pycache__, id: 1OfJrkmLCYqUS8vVjJnxgeZCvQEnQSQ4J\n","file found : title: env.py, id: 1bYr6NdzVOXe77n-gUac3VdGA8tnpS8HJ\n","downloading to /content/HW3_Task2/agent/env.py\n","file found : title: __init__.py, id: 1WO1O3RwqCpk7XdN07UUZbF6YTaj5mGGg\n","downloading to /content/HW3_Task2/agent/__init__.py\n","file found : title: model_last_20_05_actionR_cb_.textClipping, id: 1aESmsKWtCbxwgAAqKV9wP1eE4xDOCBFp\n","file found : title: model_last_20_05_actionR_cb_8_5000.pt, id: 1FdLXoWHZQz8l5hS76ygQJXe2HWa-wLJI\n","file found : title: model.pt, id: 1_hr_fysoSC6AXix1Qjqq0bBYk0KiSIcq\n","file found : title: .ipynb_checkpoints, id: 14rimDklDDBO4_OqcqfbiybhzNMxQQFSm\n","file found : title: ZZZ.zip, id: 1CVWs9pxeQ6FieIRMszIIfNK1my32N3Wl\n","file found : title: ZZZ, id: 10kHEkPFSjAINnDqi2s5KF7PxQN9bImk3\n","file found : title: try2 2, id: 1Ka1yYoo2aGsxPITkqKfBYB4AAHln0aMZ\n","file found : title: try2.zip, id: 1o5hCQ_s6UkmcbRNTkR8crcNcftp1IhZ7\n","file found : title: try2 2.zip, id: 1R8lmVqVxgvfuQ150WK4ZckNmfjvWmvW4\n","file found : title: try2, id: 1T1A0dN0enM4yy8DmA6A7HJOFyGQS0IlX\n","file found : title: A0212190W_Task2.zip, id: 1XJ9tRmPvxju3IjD76WkmINyUlYTIJvVm\n","file found : title: agent, id: 1PUrHkG6ki4JsiXtKOtvc2vAuR6IJMnCa\n","file found : title: setup.py, id: 1GrxqesqYqNJit3lNQABi9n3Wisa5zuir\n","downloading to /content/HW3_Task2/setup.py\n","file found : title: MANIFEST.in, id: 16_Iy5dYGYMUP2T9hl-OZdODIhV7dbgCf\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kM9mre8d4VL-","colab_type":"code","outputId":"f01cd041-0c8e-4969-f269-6cb5d7d970ef","executionInfo":{"status":"ok","timestamp":1587474178664,"user_tz":-480,"elapsed":3240927,"user":{"displayName":"N","photoUrl":"","userId":"08619141603155082384"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# TASK 2\n","\n","import sys\n","import time\n","\n","import torch\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n","\n","date_save = time.strftime(\"%d %H %M\")\n","model_path = os.path.join('HW3_Task2','agent', 'model_{}_{}.pt'.format(date_save,2))\n","save_path = os.path.join('HW3_Task2','agent', 'model_{}_{}.pt'.format(date_save,3))\n","\n","duree = 0 \n","nb_bad_init = 0 \n","# Try for 10 diferent inits\n","\n","for i in range(0,1):\n","  debut = time.time()\n","  # Try an init \n","  #!python3 HW3_Task2/agent/models.py --test --path=HW3_Task2/agent/model_last1_20_05_cb_8_1000.pt --savepath=HW3_Task2/agent/model_last1_20_05\n","  !python3 HW3_Task2/agent/models.py --train --path=HW3_Task2/agent/model_last_20_05_actionR_cb_8_19000.pt --savepath=HW3_Task2/agent/model_crop_21_05\n","  #!python3 HW3_Task2/agent/models.py --test --path=HW3_Task2/agent/model_last_20_05_actionR_cb_8_19000.pt --savepath=HW3_Task2/agent/model_last_20_05\n","  #!python3 HW3_Task2/agent/testenv.py\n","  duree = (time.time()-debut)\n","  # If good init exit loop\n","  print(duree)\n","  if duree > 10000:\n","    break\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["cuda\n","train mode\n","<class 'tuple'>\n","<class 'tuple'>\n","ConvDQN(\n","  (features): Sequential(\n","    (0): Conv2d(4, 32, kernel_size=(2, 2), stride=(1, 1))\n","    (1): ReLU()\n","    (2): Conv2d(32, 64, kernel_size=(2, 2), stride=(1, 1))\n","    (3): ReLU()\n","  )\n","  (layers): Sequential(\n","    (0): Linear(in_features=1536, out_features=256, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=256, out_features=5, bias=True)\n","  )\n",")\n","range change : x_min 4 x_max 4\n","environment change : i_1 4 j_1 1\n","[Episode 50]\t rewards globals : 2.549 \tavg rewards : 2.549,\tavg loss: : 6.447818,\tbuffer size : 142,\tepsilon : 84.8%, \t r <=40 0.0, \t r > 40 25.49019607843137\n","[Episode 100]\t rewards globals : 2.277 \tavg rewards : 1.961,\tavg loss: : 0.928923,\tbuffer size : 276,\tepsilon : 71.9%, \t r <=40 0.0, \t r > 40 19.607843137254903\n","[Episode 150]\t rewards globals : 3.179 \tavg rewards : 4.902,\tavg loss: : 1.364875,\tbuffer size : 409,\tepsilon : 61.0%, \t r <=40 0.0, \t r > 40 49.01960784313725\n","[Episode 200]\t rewards globals : 3.383 \tavg rewards : 4.118,\tavg loss: : 1.021311,\tbuffer size : 543,\tepsilon : 51.8%, \t r <=40 0.0, \t r > 40 41.17647058823529\n","[Episode 250]\t rewards globals : 3.386 \tavg rewards : 3.529,\tavg loss: : 1.079301,\tbuffer size : 664,\tepsilon : 44.0%, \t r <=40 0.0, \t r > 40 35.294117647058826\n","[Episode 300]\t rewards globals : 3.621 \tavg rewards : 4.902,\tavg loss: : 0.928514,\tbuffer size : 806,\tepsilon : 37.4%, \t r <=40 0.0, \t r > 40 49.01960784313725\n","[Episode 350]\t rewards globals : 3.818 \tavg rewards : 5.098,\tavg loss: : 0.895684,\tbuffer size : 939,\tepsilon : 31.8%, \t r <=40 0.0, \t r > 40 50.98039215686274\n","[Episode 400]\t rewards globals : 4.040 \tavg rewards : 5.686,\tavg loss: : 0.808171,\tbuffer size : 1091,\tepsilon : 27.1%, \t r <=40 0.0, \t r > 40 56.86274509803921\n","[Episode 450]\t rewards globals : 4.257 \tavg rewards : 5.882,\tavg loss: : 0.782512,\tbuffer size : 1239,\tepsilon : 23.1%, \t r <=40 0.0, \t r > 40 58.82352941176471\n","Test mode, runs: 100, i: 4, j: 1, final: False\n","[task_2_tmax50] 100 run(s) avg rewards : 7.3\n","[task_2_tmax40] 100 run(s) avg rewards : 7.8\n","Point: 7.55\n","Local runtime: 7.683159351348877 seconds --- fast\n","WARNING: do note that this might not reflect the runtime on the server.\n","[Episode 500]\t rewards globals : 4.511 \tavg rewards : 6.667,\tavg loss: : 0.729821,\tbuffer size : 1396,\tepsilon : 19.7%, \t r <=40 0.0, \t r > 40 66.66666666666666\n","[Episode 550]\t rewards globals : 4.701 \tavg rewards : 6.667,\tavg loss: : 0.699593,\tbuffer size : 1565,\tepsilon : 16.8%, \t r <=40 0.0, \t r > 40 66.66666666666666\n","[Episode 600]\t rewards globals : 4.859 \tavg rewards : 6.471,\tavg loss: : 0.664396,\tbuffer size : 1720,\tepsilon : 14.4%, \t r <=40 0.0, \t r > 40 64.70588235294117\n","[Episode 650]\t rewards globals : 5.054 \tavg rewards : 7.451,\tavg loss: : 0.646902,\tbuffer size : 1885,\tepsilon : 12.3%, \t r <=40 0.0, \t r > 40 74.50980392156863\n","[Episode 700]\t rewards globals : 5.150 \tavg rewards : 6.471,\tavg loss: : 0.624494,\tbuffer size : 2041,\tepsilon : 10.6%, \t r <=40 0.0, \t r > 40 64.70588235294117\n","[Episode 750]\t rewards globals : 5.286 \tavg rewards : 7.059,\tavg loss: : 0.610304,\tbuffer size : 2219,\tepsilon : 9.1%, \t r <=40 0.0, \t r > 40 70.58823529411765\n","[Episode 800]\t rewards globals : 5.381 \tavg rewards : 6.863,\tavg loss: : 0.590378,\tbuffer size : 2380,\tepsilon : 7.9%, \t r <=40 0.0, \t r > 40 68.62745098039215\n","[Episode 850]\t rewards globals : 5.499 \tavg rewards : 7.451,\tavg loss: : 0.585492,\tbuffer size : 2548,\tepsilon : 6.8%, \t r <=40 0.0, \t r > 40 74.50980392156863\n","[Episode 900]\t rewards globals : 5.616 \tavg rewards : 7.647,\tavg loss: : 0.568620,\tbuffer size : 2723,\tepsilon : 5.9%, \t r <=40 0.0, \t r > 40 76.47058823529412\n","[Episode 950]\t rewards globals : 5.720 \tavg rewards : 7.647,\tavg loss: : 0.560558,\tbuffer size : 2888,\tepsilon : 5.2%, \t r <=40 0.0, \t r > 40 76.47058823529412\n","range change : x_min 8 x_max 8\n","environment change : i_1 8 j_1 2\n","[Episode 50]\t rewards globals : 0.196 \tavg rewards : 0.196,\tavg loss: : 0.993283,\tbuffer size : 227,\tepsilon : 84.8%, \t r <=40 0.0, \t r > 40 1.9607843137254901\n","[Episode 100]\t rewards globals : 0.792 \tavg rewards : 1.373,\tavg loss: : 0.621378,\tbuffer size : 465,\tepsilon : 71.9%, \t r <=40 0.0, \t r > 40 13.725490196078432\n","[Episode 150]\t rewards globals : 0.795 \tavg rewards : 0.784,\tavg loss: : 0.626328,\tbuffer size : 693,\tepsilon : 61.0%, \t r <=40 0.0, \t r > 40 7.8431372549019605\n","[Episode 200]\t rewards globals : 0.995 \tavg rewards : 1.569,\tavg loss: : 0.568390,\tbuffer size : 920,\tepsilon : 51.8%, \t r <=40 0.0, \t r > 40 15.686274509803921\n","[Episode 250]\t rewards globals : 1.195 \tavg rewards : 1.961,\tavg loss: : 0.583533,\tbuffer size : 1158,\tepsilon : 44.0%, \t r <=40 0.0, \t r > 40 19.607843137254903\n","[Episode 300]\t rewards globals : 1.329 \tavg rewards : 1.961,\tavg loss: : 0.549751,\tbuffer size : 1428,\tepsilon : 37.4%, \t r <=40 0.0, \t r > 40 19.607843137254903\n","[Episode 350]\t rewards globals : 1.595 \tavg rewards : 3.137,\tavg loss: : 0.579047,\tbuffer size : 1675,\tepsilon : 31.8%, \t r <=40 0.0, \t r > 40 31.372549019607842\n","[Episode 400]\t rewards globals : 1.621 \tavg rewards : 1.765,\tavg loss: : 0.564775,\tbuffer size : 1943,\tepsilon : 27.1%, \t r <=40 0.0, \t r > 40 17.647058823529413\n","[Episode 450]\t rewards globals : 1.907 \tavg rewards : 4.118,\tavg loss: : 0.583289,\tbuffer size : 2235,\tepsilon : 23.1%, \t r <=40 0.0, \t r > 40 41.17647058823529\n","Test mode, runs: 100, i: 8, j: 2, final: False\n","[task_2_tmax50] 100 run(s) avg rewards : 6.0\n","[task_2_tmax40] 100 run(s) avg rewards : 5.7\n","Point: 5.85\n","Local runtime: 12.787241458892822 seconds --- fast\n","WARNING: do note that this might not reflect the runtime on the server.\n","[Episode 500]\t rewards globals : 1.976 \tavg rewards : 2.549,\tavg loss: : 0.559246,\tbuffer size : 2534,\tepsilon : 19.7%, \t r <=40 0.0, \t r > 40 25.49019607843137\n","[Episode 550]\t rewards globals : 2.123 \tavg rewards : 3.529,\tavg loss: : 0.576234,\tbuffer size : 2811,\tepsilon : 16.8%, \t r <=40 0.0, \t r > 40 35.294117647058826\n","[Episode 600]\t rewards globals : 2.346 \tavg rewards : 4.706,\tavg loss: : 0.561650,\tbuffer size : 3095,\tepsilon : 14.4%, \t r <=40 0.0, \t r > 40 47.05882352941176\n","[Episode 650]\t rewards globals : 2.427 \tavg rewards : 3.333,\tavg loss: : 0.578905,\tbuffer size : 3386,\tepsilon : 12.3%, \t r <=40 0.0, \t r > 40 33.33333333333333\n","[Episode 700]\t rewards globals : 2.525 \tavg rewards : 3.725,\tavg loss: : 0.565460,\tbuffer size : 3664,\tepsilon : 10.6%, \t r <=40 0.0, \t r > 40 37.254901960784316\n","[Episode 750]\t rewards globals : 2.703 \tavg rewards : 5.294,\tavg loss: : 0.580244,\tbuffer size : 3975,\tepsilon : 9.1%, \t r <=40 0.0, \t r > 40 52.94117647058824\n","[Episode 800]\t rewards globals : 2.821 \tavg rewards : 4.510,\tavg loss: : 0.574003,\tbuffer size : 4000,\tepsilon : 7.9%, \t r <=40 0.0, \t r > 40 45.09803921568628\n","[Episode 850]\t rewards globals : 2.985 \tavg rewards : 5.686,\tavg loss: : 0.583162,\tbuffer size : 4000,\tepsilon : 6.8%, \t r <=40 0.0, \t r > 40 56.86274509803921\n","[Episode 900]\t rewards globals : 3.074 \tavg rewards : 4.706,\tavg loss: : 0.572987,\tbuffer size : 4000,\tepsilon : 5.9%, \t r <=40 0.0, \t r > 40 47.05882352941176\n","[Episode 950]\t rewards globals : 3.197 \tavg rewards : 5.294,\tavg loss: : 0.580484,\tbuffer size : 4000,\tepsilon : 5.2%, \t r <=40 0.0, \t r > 40 52.94117647058824\n","environment change : i_1 8 j_1 2\n","Test mode, runs: 100, i: 8, j: 2, final: False\n","[task_2_tmax50] 100 run(s) avg rewards : 5.8\n","[task_2_tmax40] 100 run(s) avg rewards : 6.3\n","Point: 6.05\n","Local runtime: 13.540480136871338 seconds --- fast\n","WARNING: do note that this might not reflect the runtime on the server.\n","[Episode 1000]\t rewards globals : 3.287 \tavg rewards : 4.902,\tavg loss: : 0.572562,\tbuffer size : 4000,\tepsilon : 4.5%, \t r <=40 0.0, \t r > 40 49.01960784313725\n","[Episode 1050]\t rewards globals : 3.302 \tavg rewards : 3.529,\tavg loss: : 0.583368,\tbuffer size : 4000,\tepsilon : 4.0%, \t r <=40 0.0, \t r > 40 35.294117647058826\n","[Episode 1100]\t rewards globals : 3.415 \tavg rewards : 5.686,\tavg loss: : 0.577956,\tbuffer size : 4000,\tepsilon : 3.5%, \t r <=40 0.0, \t r > 40 56.86274509803921\n","[Episode 1150]\t rewards globals : 3.440 \tavg rewards : 4.118,\tavg loss: : 0.586564,\tbuffer size : 4000,\tepsilon : 3.1%, \t r <=40 0.0, \t r > 40 41.17647058823529\n","[Episode 1200]\t rewards globals : 3.505 \tavg rewards : 4.902,\tavg loss: : 0.581290,\tbuffer size : 4000,\tepsilon : 2.8%, \t r <=40 0.0, \t r > 40 49.01960784313725\n","[Episode 1250]\t rewards globals : 3.589 \tavg rewards : 5.490,\tavg loss: : 0.586694,\tbuffer size : 4000,\tepsilon : 2.5%, \t r <=40 0.0, \t r > 40 54.90196078431373\n","[Episode 1300]\t rewards globals : 3.689 \tavg rewards : 6.275,\tavg loss: : 0.580177,\tbuffer size : 4000,\tepsilon : 2.3%, \t r <=40 0.0, \t r > 40 62.745098039215684\n","[Episode 1350]\t rewards globals : 3.745 \tavg rewards : 5.294,\tavg loss: : 0.585755,\tbuffer size : 4000,\tepsilon : 2.1%, \t r <=40 0.0, \t r > 40 52.94117647058824\n","[Episode 1400]\t rewards globals : 3.797 \tavg rewards : 5.294,\tavg loss: : 0.580450,\tbuffer size : 4000,\tepsilon : 1.9%, \t r <=40 0.0, \t r > 40 52.94117647058824\n","[Episode 1450]\t rewards globals : 3.853 \tavg rewards : 5.490,\tavg loss: : 0.585155,\tbuffer size : 4000,\tepsilon : 1.8%, \t r <=40 0.0, \t r > 40 54.90196078431373\n","Test mode, runs: 100, i: 8, j: 2, final: False\n","[task_2_tmax50] 100 run(s) avg rewards : 5.5\n","[task_2_tmax40] 100 run(s) avg rewards : 6.6\n","Point: 6.05\n","Local runtime: 13.791950702667236 seconds --- fast\n","WARNING: do note that this might not reflect the runtime on the server.\n","[Episode 1500]\t rewards globals : 3.904 \tavg rewards : 5.490,\tavg loss: : 0.578470,\tbuffer size : 4000,\tepsilon : 1.7%, \t r <=40 0.0, \t r > 40 54.90196078431373\n","[Episode 1550]\t rewards globals : 3.933 \tavg rewards : 4.902,\tavg loss: : 0.585436,\tbuffer size : 4000,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 49.01960784313725\n","[Episode 1600]\t rewards globals : 4.004 \tavg rewards : 6.275,\tavg loss: : 0.579160,\tbuffer size : 4000,\tepsilon : 1.5%, \t r <=40 0.0, \t r > 40 62.745098039215684\n","[Episode 1650]\t rewards globals : 4.046 \tavg rewards : 5.490,\tavg loss: : 0.585012,\tbuffer size : 4000,\tepsilon : 1.4%, \t r <=40 0.0, \t r > 40 54.90196078431373\n","[Episode 1700]\t rewards globals : 4.062 \tavg rewards : 4.706,\tavg loss: : 0.580446,\tbuffer size : 4000,\tepsilon : 1.3%, \t r <=40 0.0, \t r > 40 47.05882352941176\n","[Episode 1750]\t rewards globals : 4.123 \tavg rewards : 6.078,\tavg loss: : 0.587470,\tbuffer size : 4000,\tepsilon : 1.3%, \t r <=40 0.0, \t r > 40 60.78431372549019\n","[Episode 1800]\t rewards globals : 4.148 \tavg rewards : 5.098,\tavg loss: : 0.582265,\tbuffer size : 4000,\tepsilon : 1.2%, \t r <=40 0.0, \t r > 40 50.98039215686274\n","[Episode 1850]\t rewards globals : 4.160 \tavg rewards : 4.510,\tavg loss: : 0.587200,\tbuffer size : 4000,\tepsilon : 1.2%, \t r <=40 0.0, \t r > 40 45.09803921568628\n","[Episode 1900]\t rewards globals : 4.198 \tavg rewards : 5.490,\tavg loss: : 0.582230,\tbuffer size : 4000,\tepsilon : 1.2%, \t r <=40 0.0, \t r > 40 54.90196078431373\n","[Episode 1950]\t rewards globals : 4.249 \tavg rewards : 6.275,\tavg loss: : 0.586915,\tbuffer size : 4000,\tepsilon : 1.1%, \t r <=40 0.0, \t r > 40 62.745098039215684\n","range change : x_min 13 x_max 13\n","environment change : i_1 13 j_1 5\n","[Episode 50]\t rewards globals : 0.000 \tavg rewards : 0.000,\tavg loss: : 0.878693,\tbuffer size : 322,\tepsilon : 84.8%, \t r <=40 0.0, \t r > 40 0.0\n","[Episode 100]\t rewards globals : 0.000 \tavg rewards : 0.000,\tavg loss: : 0.641083,\tbuffer size : 652,\tepsilon : 71.9%, \t r <=40 0.0, \t r > 40 0.0\n","[Episode 150]\t rewards globals : 0.066 \tavg rewards : 0.196,\tavg loss: : 0.637432,\tbuffer size : 952,\tepsilon : 61.0%, \t r <=40 0.0, \t r > 40 1.9607843137254901\n","[Episode 200]\t rewards globals : 0.050 \tavg rewards : 0.000,\tavg loss: : 0.585109,\tbuffer size : 1233,\tepsilon : 51.8%, \t r <=40 0.0, \t r > 40 0.0\n","[Episode 250]\t rewards globals : 0.120 \tavg rewards : 0.392,\tavg loss: : 0.635871,\tbuffer size : 1542,\tepsilon : 44.0%, \t r <=40 0.0, \t r > 40 3.9215686274509802\n","[Episode 300]\t rewards globals : 0.100 \tavg rewards : 0.000,\tavg loss: : 0.623936,\tbuffer size : 1877,\tepsilon : 37.4%, \t r <=40 0.0, \t r > 40 0.0\n","[Episode 350]\t rewards globals : 0.114 \tavg rewards : 0.196,\tavg loss: : 0.673504,\tbuffer size : 2188,\tepsilon : 31.8%, \t r <=40 0.0, \t r > 40 1.9607843137254901\n","[Episode 400]\t rewards globals : 0.100 \tavg rewards : 0.000,\tavg loss: : 0.663533,\tbuffer size : 2517,\tepsilon : 27.1%, \t r <=40 0.0, \t r > 40 0.0\n","[Episode 450]\t rewards globals : 0.133 \tavg rewards : 0.392,\tavg loss: : 0.701159,\tbuffer size : 2887,\tepsilon : 23.1%, \t r <=40 0.0, \t r > 40 3.9215686274509802\n","Test mode, runs: 100, i: 13, j: 5, final: False\n","[task_2_tmax50] 100 run(s) avg rewards : 0.8\n","[task_2_tmax40] 100 run(s) avg rewards : 0.4\n","Point: 0.6000000000000001\n","Local runtime: 18.25076985359192 seconds --- fast\n","WARNING: do note that this might not reflect the runtime on the server.\n","[Episode 500]\t rewards globals : 0.140 \tavg rewards : 0.196,\tavg loss: : 0.689375,\tbuffer size : 3203,\tepsilon : 19.7%, \t r <=40 0.0, \t r > 40 1.9607843137254901\n","[Episode 550]\t rewards globals : 0.127 \tavg rewards : 0.000,\tavg loss: : 0.726098,\tbuffer size : 3577,\tepsilon : 16.8%, \t r <=40 0.0, \t r > 40 0.0\n","[Episode 600]\t rewards globals : 0.116 \tavg rewards : 0.000,\tavg loss: : 0.720646,\tbuffer size : 3915,\tepsilon : 14.4%, \t r <=40 0.0, \t r > 40 0.0\n","[Episode 650]\t rewards globals : 0.230 \tavg rewards : 1.569,\tavg loss: : 0.757650,\tbuffer size : 4000,\tepsilon : 12.3%, \t r <=40 0.0, \t r > 40 15.686274509803921\n","[Episode 700]\t rewards globals : 0.228 \tavg rewards : 0.196,\tavg loss: : 0.744933,\tbuffer size : 4000,\tepsilon : 10.6%, \t r <=40 0.0, \t r > 40 1.9607843137254901\n","[Episode 750]\t rewards globals : 0.280 \tavg rewards : 0.980,\tavg loss: : 0.765596,\tbuffer size : 4000,\tepsilon : 9.1%, \t r <=40 0.0, \t r > 40 9.803921568627452\n","[Episode 800]\t rewards globals : 0.275 \tavg rewards : 0.196,\tavg loss: : 0.760699,\tbuffer size : 4000,\tepsilon : 7.9%, \t r <=40 0.0, \t r > 40 1.9607843137254901\n","[Episode 850]\t rewards globals : 0.259 \tavg rewards : 0.000,\tavg loss: : 0.781866,\tbuffer size : 4000,\tepsilon : 6.8%, \t r <=40 0.0, \t r > 40 0.0\n","[Episode 900]\t rewards globals : 0.255 \tavg rewards : 0.196,\tavg loss: : 0.775623,\tbuffer size : 4000,\tepsilon : 5.9%, \t r <=40 0.0, \t r > 40 1.9607843137254901\n","[Episode 950]\t rewards globals : 0.273 \tavg rewards : 0.588,\tavg loss: : 0.791280,\tbuffer size : 4000,\tepsilon : 5.2%, \t r <=40 0.0, \t r > 40 5.88235294117647\n","environment change : i_1 13 j_1 5\n","Test mode, runs: 100, i: 13, j: 5, final: False\n","[task_2_tmax50] 100 run(s) avg rewards : 0.9\n","[task_2_tmax40] 100 run(s) avg rewards : 0.7\n","Point: 0.8\n","Local runtime: 17.733238220214844 seconds --- fast\n","WARNING: do note that this might not reflect the runtime on the server.\n","[Episode 1000]\t rewards globals : 0.290 \tavg rewards : 0.588,\tavg loss: : 0.783777,\tbuffer size : 4000,\tepsilon : 4.5%, \t r <=40 0.0, \t r > 40 5.88235294117647\n","[Episode 1050]\t rewards globals : 0.285 \tavg rewards : 0.196,\tavg loss: : 0.794345,\tbuffer size : 4000,\tepsilon : 4.0%, \t r <=40 0.0, \t r > 40 1.9607843137254901\n","[Episode 1100]\t rewards globals : 0.300 \tavg rewards : 0.588,\tavg loss: : 0.781935,\tbuffer size : 4000,\tepsilon : 3.5%, \t r <=40 0.0, \t r > 40 5.88235294117647\n","[Episode 1150]\t rewards globals : 0.339 \tavg rewards : 1.176,\tavg loss: : 0.787661,\tbuffer size : 4000,\tepsilon : 3.1%, \t r <=40 0.0, \t r > 40 11.76470588235294\n","[Episode 1200]\t rewards globals : 0.350 \tavg rewards : 0.588,\tavg loss: : 0.775733,\tbuffer size : 4000,\tepsilon : 2.8%, \t r <=40 0.0, \t r > 40 5.88235294117647\n","[Episode 1250]\t rewards globals : 0.360 \tavg rewards : 0.588,\tavg loss: : 0.781787,\tbuffer size : 4000,\tepsilon : 2.5%, \t r <=40 0.0, \t r > 40 5.88235294117647\n","[Episode 1300]\t rewards globals : 0.354 \tavg rewards : 0.196,\tavg loss: : 0.771218,\tbuffer size : 4000,\tepsilon : 2.3%, \t r <=40 0.0, \t r > 40 1.9607843137254901\n","[Episode 1350]\t rewards globals : 0.385 \tavg rewards : 1.176,\tavg loss: : 0.775216,\tbuffer size : 4000,\tepsilon : 2.1%, \t r <=40 0.0, \t r > 40 11.76470588235294\n","[Episode 1400]\t rewards globals : 0.407 \tavg rewards : 0.980,\tavg loss: : 0.764922,\tbuffer size : 4000,\tepsilon : 1.9%, \t r <=40 0.0, \t r > 40 9.803921568627452\n","[Episode 1450]\t rewards globals : 0.414 \tavg rewards : 0.588,\tavg loss: : 0.768679,\tbuffer size : 4000,\tepsilon : 1.8%, \t r <=40 0.0, \t r > 40 5.88235294117647\n","Test mode, runs: 100, i: 13, j: 5, final: False\n","[task_2_tmax50] 100 run(s) avg rewards : 0.6\n","[task_2_tmax40] 100 run(s) avg rewards : 0.7\n","Point: 0.6499999999999999\n","Local runtime: 19.599080562591553 seconds --- fast\n","WARNING: do note that this might not reflect the runtime on the server.\n","[Episode 1500]\t rewards globals : 0.433 \tavg rewards : 0.980,\tavg loss: : 0.758578,\tbuffer size : 4000,\tepsilon : 1.7%, \t r <=40 0.0, \t r > 40 9.803921568627452\n","[Episode 1550]\t rewards globals : 0.451 \tavg rewards : 0.980,\tavg loss: : 0.761402,\tbuffer size : 4000,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 9.803921568627452\n","[Episode 1600]\t rewards globals : 0.468 \tavg rewards : 0.980,\tavg loss: : 0.751875,\tbuffer size : 4000,\tepsilon : 1.5%, \t r <=40 0.0, \t r > 40 9.803921568627452\n","[Episode 1650]\t rewards globals : 0.515 \tavg rewards : 1.961,\tavg loss: : 0.752048,\tbuffer size : 4000,\tepsilon : 1.4%, \t r <=40 0.0, \t r > 40 19.607843137254903\n","[Episode 1700]\t rewards globals : 0.511 \tavg rewards : 0.588,\tavg loss: : 0.741512,\tbuffer size : 4000,\tepsilon : 1.3%, \t r <=40 0.0, \t r > 40 5.88235294117647\n","[Episode 1750]\t rewards globals : 0.525 \tavg rewards : 0.980,\tavg loss: : 0.741776,\tbuffer size : 4000,\tepsilon : 1.3%, \t r <=40 0.0, \t r > 40 9.803921568627452\n","[Episode 1800]\t rewards globals : 0.533 \tavg rewards : 0.784,\tavg loss: : 0.731725,\tbuffer size : 4000,\tepsilon : 1.2%, \t r <=40 0.0, \t r > 40 7.8431372549019605\n","[Episode 1850]\t rewards globals : 0.567 \tavg rewards : 1.765,\tavg loss: : 0.731579,\tbuffer size : 4000,\tepsilon : 1.2%, \t r <=40 0.0, \t r > 40 17.647058823529413\n","[Episode 1900]\t rewards globals : 0.573 \tavg rewards : 0.784,\tavg loss: : 0.721502,\tbuffer size : 4000,\tepsilon : 1.2%, \t r <=40 0.0, \t r > 40 7.8431372549019605\n","[Episode 1950]\t rewards globals : 0.584 \tavg rewards : 0.980,\tavg loss: : 0.721316,\tbuffer size : 4000,\tepsilon : 1.1%, \t r <=40 0.0, \t r > 40 9.803921568627452\n","environment change : i_1 13 j_1 5\n","Test mode, runs: 100, i: 13, j: 5, final: False\n","[task_2_tmax50] 100 run(s) avg rewards : 1.2\n","[task_2_tmax40] 100 run(s) avg rewards : 1.8\n","Point: 1.5\n","Local runtime: 21.568793296813965 seconds --- fast\n","WARNING: do note that this might not reflect the runtime on the server.\n","[Episode 2000]\t rewards globals : 0.605 \tavg rewards : 1.373,\tavg loss: : 0.712395,\tbuffer size : 4000,\tepsilon : 1.1%, \t r <=40 0.0, \t r > 40 13.725490196078432\n","[Episode 2050]\t rewards globals : 0.624 \tavg rewards : 1.373,\tavg loss: : 0.712323,\tbuffer size : 4000,\tepsilon : 1.1%, \t r <=40 0.0, \t r > 40 13.725490196078432\n","[Episode 2100]\t rewards globals : 0.619 \tavg rewards : 0.392,\tavg loss: : 0.703554,\tbuffer size : 4000,\tepsilon : 1.1%, \t r <=40 0.0, \t r > 40 3.9215686274509802\n","[Episode 2150]\t rewards globals : 0.642 \tavg rewards : 1.569,\tavg loss: : 0.703978,\tbuffer size : 4000,\tepsilon : 1.1%, \t r <=40 0.0, \t r > 40 15.686274509803921\n","[Episode 2200]\t rewards globals : 0.654 \tavg rewards : 1.176,\tavg loss: : 0.696910,\tbuffer size : 4000,\tepsilon : 1.1%, \t r <=40 0.0, \t r > 40 11.76470588235294\n","[Episode 2250]\t rewards globals : 0.680 \tavg rewards : 1.765,\tavg loss: : 0.696495,\tbuffer size : 4000,\tepsilon : 1.1%, \t r <=40 0.0, \t r > 40 17.647058823529413\n","[Episode 2300]\t rewards globals : 0.700 \tavg rewards : 1.765,\tavg loss: : 0.689116,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 17.647058823529413\n","[Episode 2350]\t rewards globals : 0.710 \tavg rewards : 1.176,\tavg loss: : 0.689606,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 11.76470588235294\n","[Episode 2400]\t rewards globals : 0.721 \tavg rewards : 1.176,\tavg loss: : 0.682750,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 11.76470588235294\n","[Episode 2450]\t rewards globals : 0.743 \tavg rewards : 1.765,\tavg loss: : 0.684172,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 17.647058823529413\n","Test mode, runs: 100, i: 13, j: 5, final: False\n","[task_2_tmax50] 100 run(s) avg rewards : 1.5\n","[task_2_tmax40] 100 run(s) avg rewards : 2.5\n","Point: 2.0\n","Local runtime: 20.890100479125977 seconds --- fast\n","WARNING: do note that this might not reflect the runtime on the server.\n","[Episode 2500]\t rewards globals : 0.760 \tavg rewards : 1.569,\tavg loss: : 0.678077,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 15.686274509803921\n","[Episode 2550]\t rewards globals : 0.776 \tavg rewards : 1.569,\tavg loss: : 0.679808,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 15.686274509803921\n","[Episode 2600]\t rewards globals : 0.792 \tavg rewards : 1.569,\tavg loss: : 0.674386,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 15.686274509803921\n","[Episode 2650]\t rewards globals : 0.822 \tavg rewards : 2.353,\tavg loss: : 0.676676,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 23.52941176470588\n","[Episode 2700]\t rewards globals : 0.859 \tavg rewards : 2.745,\tavg loss: : 0.671424,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 27.450980392156865\n","[Episode 2750]\t rewards globals : 0.912 \tavg rewards : 3.725,\tavg loss: : 0.675009,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 37.254901960784316\n","[Episode 2800]\t rewards globals : 0.918 \tavg rewards : 1.176,\tavg loss: : 0.671246,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 11.76470588235294\n","[Episode 2850]\t rewards globals : 0.954 \tavg rewards : 2.941,\tavg loss: : 0.676579,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 29.411764705882355\n","[Episode 2900]\t rewards globals : 0.976 \tavg rewards : 2.353,\tavg loss: : 0.672960,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 23.52941176470588\n","[Episode 2950]\t rewards globals : 1.006 \tavg rewards : 2.745,\tavg loss: : 0.678076,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 27.450980392156865\n","range change : x_min 15 x_max 15\n","environment change : i_1 15 j_1 1\n","[Episode 50]\t rewards globals : 0.588 \tavg rewards : 0.588,\tavg loss: : 0.729004,\tbuffer size : 270,\tepsilon : 84.8%, \t r <=40 0.0, \t r > 40 5.88235294117647\n","[Episode 100]\t rewards globals : 1.188 \tavg rewards : 1.765,\tavg loss: : 0.462858,\tbuffer size : 602,\tepsilon : 71.9%, \t r <=40 0.0, \t r > 40 17.647058823529413\n","[Episode 150]\t rewards globals : 1.391 \tavg rewards : 1.765,\tavg loss: : 0.484264,\tbuffer size : 965,\tepsilon : 61.0%, \t r <=40 0.0, \t r > 40 17.647058823529413\n","[Episode 200]\t rewards globals : 1.692 \tavg rewards : 2.745,\tavg loss: : 0.447053,\tbuffer size : 1324,\tepsilon : 51.8%, \t r <=40 0.0, \t r > 40 27.450980392156865\n","[Episode 250]\t rewards globals : 1.873 \tavg rewards : 2.745,\tavg loss: : 0.506038,\tbuffer size : 1666,\tepsilon : 44.0%, \t r <=40 0.0, \t r > 40 27.450980392156865\n","[Episode 300]\t rewards globals : 1.894 \tavg rewards : 1.961,\tavg loss: : 0.483346,\tbuffer size : 2048,\tepsilon : 37.4%, \t r <=40 0.0, \t r > 40 19.607843137254903\n","[Episode 350]\t rewards globals : 1.880 \tavg rewards : 1.765,\tavg loss: : 0.562982,\tbuffer size : 2370,\tepsilon : 31.8%, \t r <=40 0.0, \t r > 40 17.647058823529413\n","[Episode 400]\t rewards globals : 1.945 \tavg rewards : 2.353,\tavg loss: : 0.565683,\tbuffer size : 2686,\tepsilon : 27.1%, \t r <=40 0.0, \t r > 40 23.52941176470588\n","[Episode 450]\t rewards globals : 2.018 \tavg rewards : 2.549,\tavg loss: : 0.638900,\tbuffer size : 3046,\tepsilon : 23.1%, \t r <=40 0.0, \t r > 40 25.49019607843137\n","Test mode, runs: 100, i: 15, j: 1, final: False\n","[task_2_tmax50] 100 run(s) avg rewards : 4.1\n","[task_2_tmax40] 100 run(s) avg rewards : 4.2\n","Point: 4.15\n","Local runtime: 17.735308408737183 seconds --- fast\n","WARNING: do note that this might not reflect the runtime on the server.\n","[Episode 500]\t rewards globals : 2.076 \tavg rewards : 2.745,\tavg loss: : 0.636172,\tbuffer size : 3384,\tepsilon : 19.7%, \t r <=40 0.0, \t r > 40 27.450980392156865\n","[Episode 550]\t rewards globals : 2.214 \tavg rewards : 3.725,\tavg loss: : 0.689901,\tbuffer size : 3755,\tepsilon : 16.8%, \t r <=40 0.0, \t r > 40 37.254901960784316\n","[Episode 600]\t rewards globals : 2.280 \tavg rewards : 2.941,\tavg loss: : 0.697881,\tbuffer size : 4000,\tepsilon : 14.4%, \t r <=40 0.0, \t r > 40 29.411764705882355\n","[Episode 650]\t rewards globals : 2.427 \tavg rewards : 4.118,\tavg loss: : 0.749976,\tbuffer size : 4000,\tepsilon : 12.3%, \t r <=40 0.0, \t r > 40 41.17647058823529\n","[Episode 700]\t rewards globals : 2.496 \tavg rewards : 3.529,\tavg loss: : 0.756354,\tbuffer size : 4000,\tepsilon : 10.6%, \t r <=40 0.0, \t r > 40 35.294117647058826\n","[Episode 750]\t rewards globals : 2.610 \tavg rewards : 4.118,\tavg loss: : 0.793239,\tbuffer size : 4000,\tepsilon : 9.1%, \t r <=40 0.0, \t r > 40 41.17647058823529\n","[Episode 800]\t rewards globals : 2.659 \tavg rewards : 3.333,\tavg loss: : 0.798408,\tbuffer size : 4000,\tepsilon : 7.9%, \t r <=40 0.0, \t r > 40 33.33333333333333\n","[Episode 850]\t rewards globals : 2.820 \tavg rewards : 5.294,\tavg loss: : 0.832373,\tbuffer size : 4000,\tepsilon : 6.8%, \t r <=40 0.0, \t r > 40 52.94117647058824\n","[Episode 900]\t rewards globals : 2.963 \tavg rewards : 5.490,\tavg loss: : 0.835922,\tbuffer size : 4000,\tepsilon : 5.9%, \t r <=40 0.0, \t r > 40 54.90196078431373\n","[Episode 950]\t rewards globals : 3.039 \tavg rewards : 4.510,\tavg loss: : 0.866642,\tbuffer size : 4000,\tepsilon : 5.2%, \t r <=40 0.0, \t r > 40 45.09803921568628\n","environment change : i_1 15 j_1 4\n","Test mode, runs: 100, i: 15, j: 4, final: False\n","[task_2_tmax50] 100 run(s) avg rewards : 1.5\n","[task_2_tmax40] 100 run(s) avg rewards : 1.6\n","Point: 1.55\n","Local runtime: 19.714349508285522 seconds --- fast\n","WARNING: do note that this might not reflect the runtime on the server.\n","[Episode 1000]\t rewards globals : 3.107 \tavg rewards : 4.314,\tavg loss: : 0.869278,\tbuffer size : 4000,\tepsilon : 4.5%, \t r <=40 0.0, \t r > 40 43.13725490196079\n","[Episode 1050]\t rewards globals : 3.054 \tavg rewards : 1.961,\tavg loss: : 0.893848,\tbuffer size : 4000,\tepsilon : 4.0%, \t r <=40 0.0, \t r > 40 19.607843137254903\n","[Episode 1100]\t rewards globals : 2.961 \tavg rewards : 0.980,\tavg loss: : 0.899807,\tbuffer size : 4000,\tepsilon : 3.5%, \t r <=40 0.0, \t r > 40 9.803921568627452\n","[Episode 1150]\t rewards globals : 2.945 \tavg rewards : 2.549,\tavg loss: : 0.919250,\tbuffer size : 4000,\tepsilon : 3.1%, \t r <=40 0.0, \t r > 40 25.49019607843137\n","[Episode 1200]\t rewards globals : 2.881 \tavg rewards : 1.373,\tavg loss: : 0.920199,\tbuffer size : 4000,\tepsilon : 2.8%, \t r <=40 0.0, \t r > 40 13.725490196078432\n","[Episode 1250]\t rewards globals : 2.862 \tavg rewards : 2.353,\tavg loss: : 0.941174,\tbuffer size : 4000,\tepsilon : 2.5%, \t r <=40 0.0, \t r > 40 23.52941176470588\n","[Episode 1300]\t rewards globals : 2.829 \tavg rewards : 1.961,\tavg loss: : 0.944730,\tbuffer size : 4000,\tepsilon : 2.3%, \t r <=40 0.0, \t r > 40 19.607843137254903\n","[Episode 1350]\t rewards globals : 2.828 \tavg rewards : 2.745,\tavg loss: : 0.967282,\tbuffer size : 4000,\tepsilon : 2.1%, \t r <=40 0.0, \t r > 40 27.450980392156865\n","[Episode 1400]\t rewards globals : 2.812 \tavg rewards : 2.353,\tavg loss: : 0.971915,\tbuffer size : 4000,\tepsilon : 1.9%, \t r <=40 0.0, \t r > 40 23.52941176470588\n","[Episode 1450]\t rewards globals : 2.812 \tavg rewards : 2.745,\tavg loss: : 0.991175,\tbuffer size : 4000,\tepsilon : 1.8%, \t r <=40 0.0, \t r > 40 27.450980392156865\n","Test mode, runs: 100, i: 15, j: 4, final: False\n","[task_2_tmax50] 100 run(s) avg rewards : 1.8\n","[task_2_tmax40] 100 run(s) avg rewards : 2.7\n","Point: 2.25\n","Local runtime: 22.236088752746582 seconds --- fast\n","WARNING: do note that this might not reflect the runtime on the server.\n","[Episode 1500]\t rewards globals : 2.818 \tavg rewards : 2.941,\tavg loss: : 0.993984,\tbuffer size : 4000,\tepsilon : 1.7%, \t r <=40 0.0, \t r > 40 29.411764705882355\n","[Episode 1550]\t rewards globals : 2.766 \tavg rewards : 1.176,\tavg loss: : 1.013600,\tbuffer size : 4000,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 11.76470588235294\n","[Episode 1600]\t rewards globals : 2.773 \tavg rewards : 2.941,\tavg loss: : 1.012333,\tbuffer size : 4000,\tepsilon : 1.5%, \t r <=40 0.0, \t r > 40 29.411764705882355\n","[Episode 1650]\t rewards globals : 2.786 \tavg rewards : 3.137,\tavg loss: : 1.024969,\tbuffer size : 4000,\tepsilon : 1.4%, \t r <=40 0.0, \t r > 40 31.372549019607842\n","[Episode 1700]\t rewards globals : 2.798 \tavg rewards : 3.137,\tavg loss: : 1.022122,\tbuffer size : 4000,\tepsilon : 1.3%, \t r <=40 0.0, \t r > 40 31.372549019607842\n","[Episode 1750]\t rewards globals : 2.770 \tavg rewards : 1.765,\tavg loss: : 1.033236,\tbuffer size : 4000,\tepsilon : 1.3%, \t r <=40 0.0, \t r > 40 17.647058823529413\n","[Episode 1800]\t rewards globals : 2.760 \tavg rewards : 2.353,\tavg loss: : 1.030176,\tbuffer size : 4000,\tepsilon : 1.2%, \t r <=40 0.0, \t r > 40 23.52941176470588\n","[Episode 1850]\t rewards globals : 2.750 \tavg rewards : 2.353,\tavg loss: : 1.040481,\tbuffer size : 4000,\tepsilon : 1.2%, \t r <=40 0.0, \t r > 40 23.52941176470588\n","[Episode 1900]\t rewards globals : 2.756 \tavg rewards : 2.941,\tavg loss: : 1.038011,\tbuffer size : 4000,\tepsilon : 1.2%, \t r <=40 0.0, \t r > 40 29.411764705882355\n","[Episode 1950]\t rewards globals : 2.773 \tavg rewards : 3.333,\tavg loss: : 1.048066,\tbuffer size : 4000,\tepsilon : 1.1%, \t r <=40 0.0, \t r > 40 33.33333333333333\n","environment change : i_1 15 j_1 6\n","Test mode, runs: 100, i: 15, j: 6, final: False\n","[task_2_tmax50] 100 run(s) avg rewards : 0.5\n","[task_2_tmax40] 100 run(s) avg rewards : 0.5\n","Point: 0.5\n","Local runtime: 23.61482048034668 seconds --- fast\n","WARNING: do note that this might not reflect the runtime on the server.\n","[Episode 2000]\t rewards globals : 2.779 \tavg rewards : 2.941,\tavg loss: : 1.048806,\tbuffer size : 4000,\tepsilon : 1.1%, \t r <=40 0.0, \t r > 40 29.411764705882355\n","[Episode 2050]\t rewards globals : 2.716 \tavg rewards : 0.196,\tavg loss: : 1.060612,\tbuffer size : 4000,\tepsilon : 1.1%, \t r <=40 0.0, \t r > 40 1.9607843137254901\n","[Episode 2100]\t rewards globals : 2.665 \tavg rewards : 0.588,\tavg loss: : 1.061515,\tbuffer size : 4000,\tepsilon : 1.1%, \t r <=40 0.0, \t r > 40 5.88235294117647\n","[Episode 2150]\t rewards globals : 2.617 \tavg rewards : 0.588,\tavg loss: : 1.070752,\tbuffer size : 4000,\tepsilon : 1.1%, \t r <=40 0.0, \t r > 40 5.88235294117647\n","[Episode 2200]\t rewards globals : 2.558 \tavg rewards : 0.000,\tavg loss: : 1.071456,\tbuffer size : 4000,\tepsilon : 1.1%, \t r <=40 0.0, \t r > 40 0.0\n","[Episode 2250]\t rewards globals : 2.501 \tavg rewards : 0.000,\tavg loss: : 1.077110,\tbuffer size : 4000,\tepsilon : 1.1%, \t r <=40 0.0, \t r > 40 0.0\n","[Episode 2300]\t rewards globals : 2.447 \tavg rewards : 0.000,\tavg loss: : 1.072673,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 0.0\n","[Episode 2350]\t rewards globals : 2.403 \tavg rewards : 0.392,\tavg loss: : 1.074372,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 3.9215686274509802\n","[Episode 2400]\t rewards globals : 2.357 \tavg rewards : 0.196,\tavg loss: : 1.069886,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 1.9607843137254901\n","[Episode 2450]\t rewards globals : 2.309 \tavg rewards : 0.000,\tavg loss: : 1.068750,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 0.0\n","Test mode, runs: 100, i: 15, j: 6, final: False\n","[task_2_tmax50] 100 run(s) avg rewards : 0.4\n","[task_2_tmax40] 100 run(s) avg rewards : 0.6\n","Point: 0.5\n","Local runtime: 27.213440418243408 seconds --- fast\n","WARNING: do note that this might not reflect the runtime on the server.\n","[Episode 2500]\t rewards globals : 2.275 \tavg rewards : 0.588,\tavg loss: : 1.062821,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 5.88235294117647\n","[Episode 2550]\t rewards globals : 2.242 \tavg rewards : 0.588,\tavg loss: : 1.060554,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 5.88235294117647\n","[Episode 2600]\t rewards globals : 2.199 \tavg rewards : 0.000,\tavg loss: : 1.053595,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 0.0\n","[Episode 2650]\t rewards globals : 2.161 \tavg rewards : 0.196,\tavg loss: : 1.050858,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 1.9607843137254901\n","[Episode 2700]\t rewards globals : 2.129 \tavg rewards : 0.392,\tavg loss: : 1.042588,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 3.9215686274509802\n","[Episode 2750]\t rewards globals : 2.105 \tavg rewards : 0.784,\tavg loss: : 1.039260,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 7.8431372549019605\n","[Episode 2800]\t rewards globals : 2.081 \tavg rewards : 0.784,\tavg loss: : 1.031711,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 7.8431372549019605\n","[Episode 2850]\t rewards globals : 2.048 \tavg rewards : 0.196,\tavg loss: : 1.026979,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 1.9607843137254901\n","[Episode 2900]\t rewards globals : 2.027 \tavg rewards : 0.784,\tavg loss: : 1.019265,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 7.8431372549019605\n","[Episode 2950]\t rewards globals : 2.009 \tavg rewards : 0.980,\tavg loss: : 1.015019,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 9.803921568627452\n","environment change : i_1 15 j_1 4\n","Test mode, runs: 100, i: 15, j: 4, final: False\n","[task_2_tmax50] 100 run(s) avg rewards : 3.4\n","[task_2_tmax40] 100 run(s) avg rewards : 3.8\n","Point: 3.5999999999999996\n","Local runtime: 26.679765701293945 seconds --- fast\n","WARNING: do note that this might not reflect the runtime on the server.\n","[Episode 3000]\t rewards globals : 1.986 \tavg rewards : 0.588,\tavg loss: : 1.006722,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 5.88235294117647\n","[Episode 3050]\t rewards globals : 2.009 \tavg rewards : 3.529,\tavg loss: : 1.003455,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 35.294117647058826\n","[Episode 3100]\t rewards globals : 2.022 \tavg rewards : 2.941,\tavg loss: : 0.996788,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 29.411764705882355\n","[Episode 3150]\t rewards globals : 2.041 \tavg rewards : 3.137,\tavg loss: : 0.995476,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 31.372549019607842\n","[Episode 3200]\t rewards globals : 2.059 \tavg rewards : 3.333,\tavg loss: : 0.990212,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 33.33333333333333\n","[Episode 3250]\t rewards globals : 2.086 \tavg rewards : 3.725,\tavg loss: : 0.993996,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 37.254901960784316\n","[Episode 3300]\t rewards globals : 2.121 \tavg rewards : 4.314,\tavg loss: : 0.991581,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 43.13725490196079\n","[Episode 3350]\t rewards globals : 2.140 \tavg rewards : 3.333,\tavg loss: : 0.996455,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 33.33333333333333\n","[Episode 3400]\t rewards globals : 2.155 \tavg rewards : 3.333,\tavg loss: : 0.995407,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 33.33333333333333\n","[Episode 3450]\t rewards globals : 2.185 \tavg rewards : 4.118,\tavg loss: : 0.999897,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 41.17647058823529\n","Test mode, runs: 100, i: 15, j: 4, final: False\n","[task_2_tmax50] 100 run(s) avg rewards : 4.0\n","[task_2_tmax40] 100 run(s) avg rewards : 4.4\n","Point: 4.2\n","Local runtime: 25.603877782821655 seconds --- fast\n","WARNING: do note that this might not reflect the runtime on the server.\n","[Episode 3500]\t rewards globals : 2.217 \tavg rewards : 4.314,\tavg loss: : 0.996941,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 43.13725490196079\n","[Episode 3550]\t rewards globals : 2.242 \tavg rewards : 3.922,\tavg loss: : 1.000596,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 39.21568627450981\n","[Episode 3600]\t rewards globals : 2.263 \tavg rewards : 3.725,\tavg loss: : 0.999330,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 37.254901960784316\n","[Episode 3650]\t rewards globals : 2.293 \tavg rewards : 4.314,\tavg loss: : 1.002829,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 43.13725490196079\n","[Episode 3700]\t rewards globals : 2.321 \tavg rewards : 4.510,\tavg loss: : 1.001166,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 45.09803921568628\n","[Episode 3750]\t rewards globals : 2.338 \tavg rewards : 3.529,\tavg loss: : 1.006489,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 35.294117647058826\n","[Episode 3800]\t rewards globals : 2.376 \tavg rewards : 5.098,\tavg loss: : 1.005647,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 50.98039215686274\n","[Episode 3850]\t rewards globals : 2.394 \tavg rewards : 3.725,\tavg loss: : 1.012614,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 37.254901960784316\n","[Episode 3900]\t rewards globals : 2.417 \tavg rewards : 4.314,\tavg loss: : 1.014364,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 43.13725490196079\n","[Episode 3950]\t rewards globals : 2.442 \tavg rewards : 4.510,\tavg loss: : 1.020712,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 45.09803921568628\n","range change : x_min 20 x_max 20\n","environment change : i_1 20 j_1 3\n","[Episode 50]\t rewards globals : 0.196 \tavg rewards : 0.196,\tavg loss: : 1.220917,\tbuffer size : 301,\tepsilon : 84.8%, \t r <=40 0.0, \t r > 40 1.9607843137254901\n","[Episode 100]\t rewards globals : 0.198 \tavg rewards : 0.392,\tavg loss: : 0.829597,\tbuffer size : 651,\tepsilon : 71.9%, \t r <=40 0.0, \t r > 40 3.9215686274509802\n","[Episode 150]\t rewards globals : 0.199 \tavg rewards : 0.196,\tavg loss: : 0.930445,\tbuffer size : 1035,\tepsilon : 61.0%, \t r <=40 0.0, \t r > 40 1.9607843137254901\n","[Episode 200]\t rewards globals : 0.199 \tavg rewards : 0.196,\tavg loss: : 0.878623,\tbuffer size : 1402,\tepsilon : 51.8%, \t r <=40 0.0, \t r > 40 1.9607843137254901\n","[Episode 250]\t rewards globals : 0.359 \tavg rewards : 0.980,\tavg loss: : 1.016453,\tbuffer size : 1822,\tepsilon : 44.0%, \t r <=40 0.0, \t r > 40 9.803921568627452\n","[Episode 300]\t rewards globals : 0.432 \tavg rewards : 0.784,\tavg loss: : 1.012063,\tbuffer size : 2260,\tepsilon : 37.4%, \t r <=40 0.0, \t r > 40 7.8431372549019605\n","[Episode 350]\t rewards globals : 0.484 \tavg rewards : 0.784,\tavg loss: : 1.120515,\tbuffer size : 2709,\tepsilon : 31.8%, \t r <=40 0.0, \t r > 40 7.8431372549019605\n","[Episode 400]\t rewards globals : 0.524 \tavg rewards : 0.784,\tavg loss: : 1.128662,\tbuffer size : 3111,\tepsilon : 27.1%, \t r <=40 0.0, \t r > 40 7.8431372549019605\n","[Episode 450]\t rewards globals : 0.576 \tavg rewards : 0.980,\tavg loss: : 1.224276,\tbuffer size : 3615,\tepsilon : 23.1%, \t r <=40 0.0, \t r > 40 9.803921568627452\n","Test mode, runs: 100, i: 20, j: 3, final: True\n","[task_2_tmax50] 100 run(s) avg rewards : 0.0\n","[task_2_tmax40] 100 run(s) avg rewards : 0.0\n","Point: 0.0\n","Local runtime: 34.18734407424927 seconds --- fast\n","WARNING: do note that this might not reflect the runtime on the server.\n","[Episode 500]\t rewards globals : 0.719 \tavg rewards : 1.961,\tavg loss: : 1.264732,\tbuffer size : 4000,\tepsilon : 19.7%, \t r <=40 0.0, \t r > 40 19.607843137254903\n","[Episode 550]\t rewards globals : 0.799 \tavg rewards : 1.765,\tavg loss: : 1.349336,\tbuffer size : 4000,\tepsilon : 16.8%, \t r <=40 0.0, \t r > 40 17.647058823529413\n","[Episode 600]\t rewards globals : 0.815 \tavg rewards : 1.176,\tavg loss: : 1.387471,\tbuffer size : 4000,\tepsilon : 14.4%, \t r <=40 0.0, \t r > 40 11.76470588235294\n","[Episode 650]\t rewards globals : 0.845 \tavg rewards : 1.176,\tavg loss: : 1.444045,\tbuffer size : 4000,\tepsilon : 12.3%, \t r <=40 0.0, \t r > 40 11.76470588235294\n","[Episode 700]\t rewards globals : 0.956 \tavg rewards : 2.353,\tavg loss: : 1.471403,\tbuffer size : 4000,\tepsilon : 10.6%, \t r <=40 0.0, \t r > 40 23.52941176470588\n","[Episode 750]\t rewards globals : 0.959 \tavg rewards : 0.980,\tavg loss: : 1.499299,\tbuffer size : 4000,\tepsilon : 9.1%, \t r <=40 0.0, \t r > 40 9.803921568627452\n","[Episode 800]\t rewards globals : 0.999 \tavg rewards : 1.569,\tavg loss: : 1.505292,\tbuffer size : 4000,\tepsilon : 7.9%, \t r <=40 0.0, \t r > 40 15.686274509803921\n","[Episode 850]\t rewards globals : 1.152 \tavg rewards : 3.529,\tavg loss: : 1.520556,\tbuffer size : 4000,\tepsilon : 6.8%, \t r <=40 0.0, \t r > 40 35.294117647058826\n","[Episode 900]\t rewards globals : 1.254 \tavg rewards : 3.137,\tavg loss: : 1.522617,\tbuffer size : 4000,\tepsilon : 5.9%, \t r <=40 0.0, \t r > 40 31.372549019607842\n","[Episode 950]\t rewards globals : 1.335 \tavg rewards : 2.745,\tavg loss: : 1.529167,\tbuffer size : 4000,\tepsilon : 5.2%, \t r <=40 0.0, \t r > 40 27.450980392156865\n","environment change : i_1 20 j_1 4\n","Test mode, runs: 100, i: 20, j: 4, final: True\n","[task_2_tmax50] 100 run(s) avg rewards : 0.0\n","[task_2_tmax40] 100 run(s) avg rewards : 0.0\n","Point: 0.0\n","Local runtime: 67.82712936401367 seconds --- fast\n","WARNING: do note that this might not reflect the runtime on the server.\n","[Episode 1000]\t rewards globals : 1.449 \tavg rewards : 3.725,\tavg loss: : 1.517214,\tbuffer size : 4000,\tepsilon : 4.5%, \t r <=40 0.0, \t r > 40 37.254901960784316\n","[Episode 1050]\t rewards globals : 1.437 \tavg rewards : 1.176,\tavg loss: : 1.506520,\tbuffer size : 4000,\tepsilon : 4.0%, \t r <=40 0.0, \t r > 40 11.76470588235294\n","[Episode 1100]\t rewards globals : 1.471 \tavg rewards : 2.157,\tavg loss: : 1.489302,\tbuffer size : 4000,\tepsilon : 3.5%, \t r <=40 0.0, \t r > 40 21.568627450980394\n","[Episode 1150]\t rewards globals : 1.477 \tavg rewards : 1.569,\tavg loss: : 1.480721,\tbuffer size : 4000,\tepsilon : 3.1%, \t r <=40 0.0, \t r > 40 15.686274509803921\n","[Episode 1200]\t rewards globals : 1.507 \tavg rewards : 2.157,\tavg loss: : 1.465729,\tbuffer size : 4000,\tepsilon : 2.8%, \t r <=40 0.0, \t r > 40 21.568627450980394\n","[Episode 1250]\t rewards globals : 1.495 \tavg rewards : 1.176,\tavg loss: : 1.455403,\tbuffer size : 4000,\tepsilon : 2.5%, \t r <=40 0.0, \t r > 40 11.76470588235294\n","[Episode 1300]\t rewards globals : 1.530 \tavg rewards : 2.353,\tavg loss: : 1.442633,\tbuffer size : 4000,\tepsilon : 2.3%, \t r <=40 0.0, \t r > 40 23.52941176470588\n","[Episode 1350]\t rewards globals : 1.562 \tavg rewards : 2.353,\tavg loss: : 1.434960,\tbuffer size : 4000,\tepsilon : 2.1%, \t r <=40 0.0, \t r > 40 23.52941176470588\n","[Episode 1400]\t rewards globals : 1.613 \tavg rewards : 2.941,\tavg loss: : 1.421070,\tbuffer size : 4000,\tepsilon : 1.9%, \t r <=40 0.0, \t r > 40 29.411764705882355\n","[Episode 1450]\t rewards globals : 1.633 \tavg rewards : 2.353,\tavg loss: : 1.414996,\tbuffer size : 4000,\tepsilon : 1.8%, \t r <=40 0.0, \t r > 40 23.52941176470588\n","Test mode, runs: 100, i: 20, j: 4, final: True\n","[task_2_tmax50] 100 run(s) avg rewards : 0.0\n","[task_2_tmax40] 100 run(s) avg rewards : 0.0\n","Point: 0.0\n","Local runtime: 73.08608984947205 seconds --- fast\n","WARNING: do note that this might not reflect the runtime on the server.\n","[Episode 1500]\t rewards globals : 1.692 \tavg rewards : 3.333,\tavg loss: : 1.406237,\tbuffer size : 4000,\tepsilon : 1.7%, \t r <=40 0.0, \t r > 40 33.33333333333333\n","[Episode 1550]\t rewards globals : 1.747 \tavg rewards : 3.333,\tavg loss: : 1.401946,\tbuffer size : 4000,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 33.33333333333333\n","[Episode 1600]\t rewards globals : 1.818 \tavg rewards : 3.922,\tavg loss: : 1.390063,\tbuffer size : 4000,\tepsilon : 1.5%, \t r <=40 0.0, \t r > 40 39.21568627450981\n","[Episode 1650]\t rewards globals : 1.866 \tavg rewards : 3.529,\tavg loss: : 1.382905,\tbuffer size : 4000,\tepsilon : 1.4%, \t r <=40 0.0, \t r > 40 35.294117647058826\n","[Episode 1700]\t rewards globals : 1.899 \tavg rewards : 3.137,\tavg loss: : 1.369561,\tbuffer size : 4000,\tepsilon : 1.3%, \t r <=40 0.0, \t r > 40 31.372549019607842\n","[Episode 1750]\t rewards globals : 1.925 \tavg rewards : 2.745,\tavg loss: : 1.361559,\tbuffer size : 4000,\tepsilon : 1.3%, \t r <=40 0.0, \t r > 40 27.450980392156865\n","[Episode 1800]\t rewards globals : 1.982 \tavg rewards : 4.118,\tavg loss: : 1.351034,\tbuffer size : 4000,\tepsilon : 1.2%, \t r <=40 0.0, \t r > 40 41.17647058823529\n","[Episode 1850]\t rewards globals : 2.010 \tavg rewards : 3.137,\tavg loss: : 1.344400,\tbuffer size : 4000,\tepsilon : 1.2%, \t r <=40 0.0, \t r > 40 31.372549019607842\n","[Episode 1900]\t rewards globals : 2.025 \tavg rewards : 2.549,\tavg loss: : 1.336653,\tbuffer size : 4000,\tepsilon : 1.2%, \t r <=40 0.0, \t r > 40 25.49019607843137\n","[Episode 1950]\t rewards globals : 2.060 \tavg rewards : 3.529,\tavg loss: : 1.332290,\tbuffer size : 4000,\tepsilon : 1.1%, \t r <=40 0.0, \t r > 40 35.294117647058826\n","environment change : i_1 20 j_1 8\n","Test mode, runs: 100, i: 20, j: 8, final: True\n","[task_2_tmax50] 100 run(s) avg rewards : 0.1\n","[task_2_tmax40] 100 run(s) avg rewards : 0.0\n","Point: 0.05\n","Local runtime: 72.60134696960449 seconds --- fast\n","WARNING: do note that this might not reflect the runtime on the server.\n","[Episode 2000]\t rewards globals : 2.049 \tavg rewards : 1.569,\tavg loss: : 1.321649,\tbuffer size : 4000,\tepsilon : 1.1%, \t r <=40 0.0, \t r > 40 15.686274509803921\n","[Episode 2050]\t rewards globals : 1.999 \tavg rewards : 0.000,\tavg loss: : 1.314689,\tbuffer size : 4000,\tepsilon : 1.1%, \t r <=40 0.0, \t r > 40 0.0\n","[Episode 2100]\t rewards globals : 1.951 \tavg rewards : 0.000,\tavg loss: : 1.304087,\tbuffer size : 4000,\tepsilon : 1.1%, \t r <=40 0.0, \t r > 40 0.0\n","[Episode 2150]\t rewards globals : 1.906 \tavg rewards : 0.000,\tavg loss: : 1.295895,\tbuffer size : 4000,\tepsilon : 1.1%, \t r <=40 0.0, \t r > 40 0.0\n","[Episode 2200]\t rewards globals : 1.863 \tavg rewards : 0.000,\tavg loss: : 1.284972,\tbuffer size : 4000,\tepsilon : 1.1%, \t r <=40 0.0, \t r > 40 0.0\n","[Episode 2250]\t rewards globals : 1.821 \tavg rewards : 0.000,\tavg loss: : 1.274311,\tbuffer size : 4000,\tepsilon : 1.1%, \t r <=40 0.0, \t r > 40 0.0\n","[Episode 2300]\t rewards globals : 1.782 \tavg rewards : 0.000,\tavg loss: : 1.262476,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 0.0\n","[Episode 2350]\t rewards globals : 1.744 \tavg rewards : 0.000,\tavg loss: : 1.250443,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 0.0\n","[Episode 2400]\t rewards globals : 1.708 \tavg rewards : 0.000,\tavg loss: : 1.237989,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 0.0\n","[Episode 2450]\t rewards globals : 1.673 \tavg rewards : 0.000,\tavg loss: : 1.226013,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 0.0\n","Test mode, runs: 100, i: 20, j: 8, final: True\n","[task_2_tmax50] 100 run(s) avg rewards : 0.0\n","[task_2_tmax40] 100 run(s) avg rewards : 0.0\n","Point: 0.0\n","Local runtime: 82.91138648986816 seconds --- fast\n","WARNING: do note that this might not reflect the runtime on the server.\n","[Episode 2500]\t rewards globals : 1.639 \tavg rewards : 0.000,\tavg loss: : 1.215174,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 0.0\n","[Episode 2550]\t rewards globals : 1.607 \tavg rewards : 0.000,\tavg loss: : 1.200935,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 0.0\n","[Episode 2600]\t rewards globals : 1.576 \tavg rewards : 0.000,\tavg loss: : 1.187759,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 0.0\n","[Episode 2650]\t rewards globals : 1.547 \tavg rewards : 0.000,\tavg loss: : 1.173102,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 0.0\n","[Episode 2700]\t rewards globals : 1.518 \tavg rewards : 0.000,\tavg loss: : 1.159274,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 0.0\n","[Episode 2750]\t rewards globals : 1.490 \tavg rewards : 0.000,\tavg loss: : 1.145251,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 0.0\n","[Episode 2800]\t rewards globals : 1.464 \tavg rewards : 0.000,\tavg loss: : 1.131296,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 0.0\n","[Episode 2850]\t rewards globals : 1.438 \tavg rewards : 0.000,\tavg loss: : 1.117253,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 0.0\n","[Episode 2900]\t rewards globals : 1.413 \tavg rewards : 0.000,\tavg loss: : 1.103660,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 0.0\n","[Episode 2950]\t rewards globals : 1.389 \tavg rewards : 0.000,\tavg loss: : 1.089774,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 0.0\n","environment change : i_1 20 j_1 5\n","Test mode, runs: 100, i: 20, j: 5, final: True\n","[task_2_tmax50] 100 run(s) avg rewards : 0.0\n","[task_2_tmax40] 100 run(s) avg rewards : 0.0\n","Point: 0.0\n","Local runtime: 85.02863955497742 seconds --- fast\n","WARNING: do note that this might not reflect the runtime on the server.\n","[Episode 3000]\t rewards globals : 1.366 \tavg rewards : 0.000,\tavg loss: : 1.076625,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 0.0\n","[Episode 3050]\t rewards globals : 1.344 \tavg rewards : 0.000,\tavg loss: : 1.063324,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 0.0\n","[Episode 3100]\t rewards globals : 1.322 \tavg rewards : 0.000,\tavg loss: : 1.050648,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 0.0\n","[Episode 3150]\t rewards globals : 1.301 \tavg rewards : 0.000,\tavg loss: : 1.037717,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 0.0\n","[Episode 3200]\t rewards globals : 1.284 \tavg rewards : 0.196,\tavg loss: : 1.025121,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 1.9607843137254901\n","[Episode 3250]\t rewards globals : 1.264 \tavg rewards : 0.000,\tavg loss: : 1.012046,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 0.0\n","[Episode 3300]\t rewards globals : 1.245 \tavg rewards : 0.000,\tavg loss: : 0.999134,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 0.0\n","[Episode 3350]\t rewards globals : 1.226 \tavg rewards : 0.000,\tavg loss: : 0.986345,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 0.0\n","[Episode 3400]\t rewards globals : 1.208 \tavg rewards : 0.000,\tavg loss: : 0.973865,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 0.0\n","[Episode 3450]\t rewards globals : 1.191 \tavg rewards : 0.000,\tavg loss: : 0.961645,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 0.0\n","Test mode, runs: 100, i: 20, j: 5, final: True\n","[task_2_tmax50] 100 run(s) avg rewards : 0.0\n","[task_2_tmax40] 100 run(s) avg rewards : 0.0\n","Point: 0.0\n","Local runtime: 85.14461660385132 seconds --- fast\n","WARNING: do note that this might not reflect the runtime on the server.\n","[Episode 3500]\t rewards globals : 1.174 \tavg rewards : 0.000,\tavg loss: : 0.949681,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 0.0\n","[Episode 3550]\t rewards globals : 1.157 \tavg rewards : 0.000,\tavg loss: : 0.937819,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 0.0\n","[Episode 3600]\t rewards globals : 1.141 \tavg rewards : 0.000,\tavg loss: : 0.926184,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 0.0\n","[Episode 3650]\t rewards globals : 1.126 \tavg rewards : 0.000,\tavg loss: : 0.914640,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 0.0\n","[Episode 3700]\t rewards globals : 1.116 \tavg rewards : 0.392,\tavg loss: : 0.903459,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 3.9215686274509802\n","[Episode 3750]\t rewards globals : 1.101 \tavg rewards : 0.000,\tavg loss: : 0.892576,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 0.0\n","[Episode 3800]\t rewards globals : 1.087 \tavg rewards : 0.000,\tavg loss: : 0.881817,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 0.0\n","[Episode 3850]\t rewards globals : 1.072 \tavg rewards : 0.000,\tavg loss: : 0.871311,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 0.0\n","[Episode 3900]\t rewards globals : 1.059 \tavg rewards : 0.000,\tavg loss: : 0.860980,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 0.0\n","[Episode 3950]\t rewards globals : 1.045 \tavg rewards : 0.000,\tavg loss: : 0.850807,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 0.0\n","environment change : i_1 20 j_1 1\n","Test mode, runs: 100, i: 20, j: 1, final: True\n","[task_2_tmax50] 100 run(s) avg rewards : 0.0\n","[task_2_tmax40] \n","3240.266247034073\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"atNXnxM_NcTN","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qS_dXtwjSEFH","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1RIkGEjdJw83","colab_type":"code","outputId":"0b5a4086-2396-4232-e1ac-464f9217e292","executionInfo":{"status":"ok","timestamp":1587479947718,"user_tz":-480,"elapsed":877,"user":{"displayName":"N","photoUrl":"","userId":"08619141603155082384"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["\n","from HW3_Task2.agent.env import construct_task2_env_0\n","env = construct_task2_env_0()\n","\n","state = env.reset()\n","env.render()\n","\n","import numpy as np\n","def pos_from_state(state):\n","  for i in range(0,10):\n","    column = (np.where(state[1,i,:] == 1.))\n","    if len(column[0]) > 0:\n","      column = column[0][0]\n","      row = i \n","      return(column,row)\n","\n","def state_from_pos(pos_x,pos_y,state):\n","  #Crop to keep a window [49,4]\n","  if pos_y > 7:\n","    y_max = 9\n","    y_min = 5\n","  elif pos_y < 2:\n","    y_max = 4\n","    y_min = 0\n","  else:\n","    y_max = pos_y + 2\n","    y_min = pos_y - 2\n","  \n","  if pos_x > 45:\n","    x_max = 49\n","    x_min = 40\n","  elif pos_x < 5:\n","    x_max = 9\n","    x_min = 0\n","  else:\n","    x_max = pos_x + 4\n","    x_min = pos_x - 5\n","\n","  return(state[:,y_min:(y_max+1),x_min:(x_max+1)])\n","\n","state,_,_,_ = env.step(0)\n","env.render()\n","state,_,_,_ = env.step(1)\n","env.render()\n","state,_,_,_ = env.step(2)\n","env.render()\n","state,_,_,_ = env.step(3)\n","env.render()\n","print(pos_from_state(state))\n","x,y = pos_from_state(state)\n","print(x,y)\n","state2 = state_from_pos(x,y,state)\n","print(state2)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["========================================================================================================================================================================================================\n","  F   -   -   6   -   -   -   -   -   -   -   -   -   -   1   -   -   -   -   O   -   -   -   -   -   -   5   -   -   -   2   -   -   -   -   -   -   4   -   -   -   -   -   -   -   -   -   -   -   3\n","  -   -   -   -  13   -   -   -   -  12   -   -   -   -   -   -   -   -   -   -  11   -   -   -  14   -   -   -   -   -   -   -   -   -   -   -   -   8   -   -   -  10   -   -   9   -   -   -   7   -\n","  -   -   -   -   -   -   -   -   -   -   -   -   -   -  20   -   -  19   -   -   -   -   -   -   -  16   -   -   -   -  15   -   -  18   -   -  17   -   -   -   -   -   -   -   -   -   -   -   -   -\n","  -   -   -  24   -  25   -   -   -   -   -   -  22  23   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -  26   -   -  21   -   -   -   -   -   -   -   -   -   -\n"," 32   -   -   -   -   -   -   -   -   -   -   -   -   -   -  31   -   -   -   -   -  29   -  28   -   -   -  27   -   -   -   -  33   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -  30\n","  -  39   -   -   -   -   -   -   -   -   -   -   -  38   -   -  36   -   -   -   -   -   -   -   -  34   -   -   -   -   -   -   -  37   -   -   -  41  35   -   -   -   -   -   -   -   -   -  40   -\n","  -   -   -   -   -   -   -   -   -   -   -   -  46   -  44   -   -  42   -   -   -  47   -   -   -   -   -   -   -   -   -   -   -  43   -   -  45   -   -   -   -   -   -   -   -   -   -   -   -   -\n","  -   -   -   -  49   -   -   -   -   -  50   -   -   -   -   -   -  52   -   -   -   -   -   -   -   -   -   -   -   -   -   -  53   -   -   -   -   -   -  51   -  54   -  48   -   -   -   -   -   -\n","  -   -   -   -   -   -   -  55   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -  57   -  56   -   -   -   -   -   -   -   -   -   -   -  60   -   -   -  59   -  58   -   -\n","  -   -  61   -   -   -   -   -  65   -   -   -   -   -  64  62   -   -  68   -   -  63   -   -   -  67   -   -  66   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   <\n","========================================================================================================================================================================================================\n","========================================================================================================================================================================================================\n","  F   6   ~   -   -   -   -   -   -   -   -   -   1   ~   -   -   -   O   ~   -   -   -   -   -   5   ~   -   -   -   2   -   -   -   -   -   4   ~   -   -   -   -   -   -   -   -   -   -   3   ~   -\n","  -   -  13   ~   -   -   -   -  12   -   -   -   -   -   -   -   -   -   -  11   -   -   -  14   -   -   -   -   -   -   -   -   -   -   -   8   ~   -   -   -  10   -   -   9   -   -   7   ~   -   -\n","  -   -   -   -   -   -   -   -   -   -   -   -   -  20   -   -  19   -   -   -   -   -   -   -  16   -   -   -   -  15   -   -  18   -   -  17   -   -   -   -   -   -   -   -   -   -   -   -   -   -\n"," 24   ~   ~  25   ~   -   -   -   -   -   -  22  23   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -  26   ~   -   -  21   -   -   -   -   -   -   -   -   -   -   -\n","  -   -   -   -   -   -   -   -   -   -   -   -   -  31   ~   -   -   -   -   -  29   -  28   -   -  27   ~   -   -   -   -  33   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -  30  32\n","  ~   -   -   -   -   -   -   -   -   -   -   -  38   -  36   ~   -   -   -   -   -   -   -   -  34   -   -   -   -   -   -  37   ~   -   -  41   ~  35   -   -   -   -   -   -   -   -   -  40   -  39\n","  -   -   -   -   -   -   -   -   -  46   ~  44   ~   ~   -  42   ~   -  47   ~   ~   -   -   -   -   -   -   -   -   -  43   ~   ~   -  45   ~   -   -   -   -   -   -   -   -   -   -   -   -   -   -\n","  -   -   -  49   -   -   -   -   -  50   -   -   -   -   -   -  52   -   -   -   -   -   -   -   -   -   -   -   -   -   -  53   -   -   -   -   -   -  51   -  54   -  48   -   -   -   -   -   -   -\n","  -   -   -   -   -  55   ~   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -  57   -  56   -   -   -   -   -   -   -   -   -   -  60   ~   -   -   -  59   -  58   -   <   -\n"," 61   ~   -   -   -   -  65   ~   -   -   -   -  64  62   ~   -  68   ~   -  63   ~   -   -  67   ~   -  66   ~   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -\n","========================================================================================================================================================================================================\n","========================================================================================================================================================================================================\n"," ~F   -   -   -   -   -   -   -   -   -   -   1   -   -   -   -   O   -   -   -   -   -   -   5   -   -   -   2   ~   -   -   -   -   4   ~   -   -   -   -   -   -   -   -   -   -   -   3   -   -   6\n"," 13   ~   -   -   -   -  12   ~   -   -   -   -   -   -   -   -   -  11   ~   -   -  14   ~   -   -   -   -   -   -   -   -   -   -   -   8   -   -   -   -  10   -   9   ~   -   -   7   -   -   -   -\n","  -   -   -   -   -   -   -   -   -   -   -   -  20   -   -  19   -   -   -   -   -   -   -  16   -   -   -   -  15   -   -  18   -   -  17   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -\n","  -   -  25   -   -   -   -   -  22   ~  23   ~   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -  26   ~   ~   -  21   ~   ~   -   -   -   -   -   -   -   -   -   -   -  24\n","  -   -   -   -   -   -   -   -   -   -   -  31   ~   -   -   -   -   -   -  29   -  28   -   -  27   -   -   -   -   -  33   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -  30  32   -\n","  -   -   -   -   -   -   -   -   -   -  38   ~   -  36   -   -   -   -   -   -   -   -   -  34   -   -   -   -   -   -  37   -   -  41   ~   -  35   -   -   -   -   -   -   -   -   -  40   -  39   -\n","  -   -   -   -   -   -   -  46   ~  44   ~   -   -  42   ~  47   ~   ~   -   -   -   -   -   -   -   -   -  43   ~   ~   -  45   ~   ~   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -\n","  -   -  49   -   -   -   -   -  50   -   -   -   -   -   -  52   -   -   -   -   -   -   -   -   -   -   -   -   -   -  53   -   -   -   -   -   -  51   -  54   -  48   -   -   -   -   -   -   -   -\n","  -   -   -  55   ~   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -  57   ~   -  56   -   -   -   -   -   -   -   -   -  60   ~   -   -   -  59   ~  58   ~   -   -   -   -\n","  -   -   -   -  65   ~   -   -   -   -  64  62   ~   -  68   ~   -  63   ~   -   -  67   ~   -  66   ~   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   <  61   ~\n","========================================================================================================================================================================================================\n","========================================================================================================================================================================================================\n","  F   -   -   -   -   -   -   -   -   -   1   -   -   -   -   O   -   -   -   -   -   5   ~   -   -   2   ~   -   -   -   -   4   ~   -   -   -   -   -   -   -   -   -   -   -   3   ~   -   -   6   -\n","  -   -   -   -   -  12   -   -   -   -   -   -   -   -   -  11   ~   -   -   -  14   -   -   -   -   -   -   -   -   -   -   -   8   ~   -   -   -   -  10   9   ~   -   -   -   7   -   -   -   -  13\n","  -   -   -   -   -   -   -   -   -   -   -  20   -   -  19   -   -   -   -   -   -   -  16   -   -   -   -  15   -   -  18   -   -  17   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -\n"," 25   ~   -   -   -  22   ~   ~   -  23   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -  26   ~   ~   -  21   ~   ~   -   -   -   -   -   -   -   -   -   -   -   -  24   ~   -\n","  -   -   -   -   -   -   -   -   -  31   ~   -   -   -   -   -   -  29   ~  28   ~   -  27   ~   -   -   -   -  33   ~   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -  30  32   -   -\n","  -   -   -   -   -   -   -   -   -  38   -  36   ~   -   -   -   -   -   -   -   -   -  34   -   -   -   -   -  37   ~   -  41   ~   -   -  35   -   -   -   -   -   -   -   -   -  40   -  39   -   -\n","  -   -   -   -  46   ~   ~  44   ~   -   -  42  47   ~   ~   -   -   -   -   -   -   -   -   -  43   ~   ~   -   -  45   ~   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -\n","  -  49   -   -   -   -   -  50   -   -   -   -   -   -  52   -   -   -   -   -   -   -   -   -   -   -   -   -   -  53   -   -   -   -   -   -  51   -  54   -  48   -   -   -   -   -   -   -   -   -\n","  -  55   ~   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -  57   ~   -   -  56   -   -   -   -   -   -   -   -   -  60   -   -   -   -  59   -  58   -   -   -   -   -   -\n","  -   -  65   ~   -   -   -   -  64  62   ~   -  68   ~   -  63   ~   -   -  67   ~   -  66   ~   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   <   -  61   ~   -   -\n","========================================================================================================================================================================================================\n","========================================================================================================================================================================================================\n","  F   -   -   -   -   -   -   -   1   ~   -   -   -   -   O   -   -   -   -   -   5   -   -   -   2   -   -   -   -   -   4   -   -   -   -   -   -   -   -   -   -   -   -   3   -   -   -   6   -   -\n","  -   -   -  12   ~   -   -   -   -   -   -   -   -  11   ~   -   -   -  14   ~   -   -   -   -   -   -   -   -   -   -   -   8   -   -   -   -  10   9   ~   -   -   -   7   ~   -   -   -  13   ~   -\n","  -   -   -   -   -   -   -   -   -   -  20   -   -  19   -   -   -   -   -   -   -  16   -   -   -   -  15   -   -  18   -   -  17   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -\n","  -   -   -  22   ~   -   -   -  23   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -  26   ~   -  21   ~   ~   -   -   -   -   -   -   -   -   -   -   -   -   -   -  24   -   -  25\n","  -   -   -   -   -   -   -  31   ~   -   -   -   -   -   -   -  29  28   ~   -  27   ~   -   -   -   -   -  33   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -  30   ~  32   -   -   -\n","  -   -   -   -   -   -   -  38   ~   -  36   -   -   -   -   -   -   -   -   -  34   ~   -   -   -   -  37   ~   -   -  41   -   -   -  35   -   -   -   -   -   -   -   -   -  40  39   ~   -   -   -\n","  -  46   ~   ~   -  44   ~   -   -  42  47   ~   -   -   -   -   -   -   -   -   -   -  43   ~   -   -  45   ~   ~   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -\n"," 49   -   -   -   -   -  50   -   -   -   -   -   -  52   -   -   -   -   -   -   -   -   -   -   -   -   -   -  53   -   -   -   -   -   -  51   -  54   -  48   -   -   -   -   -   -   -   -   -   -\n","  ~   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -  57   ~   -   -   -  56   -   -   -   -   -   -   -   -  60   ~   -   -   -  59   ~  58   ~   -   -   -   -   -   -  55\n"," 65   ~   -   -   -   -  64  62   ~   -  68   ~   -  63   ~   -   -  67   ~   -  66   ~   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   <   -  61   ~   -   -   -   -\n","========================================================================================================================================================================================================\n","(42, 9)\n","42 9\n","[[[0. 0. 0. 0. 0. 0. 0. 1. 1. 0.]\n","  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","  [1. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n","  [0. 0. 1. 0. 1. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n","\n"," [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n","\n"," [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n","\n"," [[0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n","  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","  [1. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n","  [0. 0. 1. 1. 1. 1. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0. 0. 1. 1. 0.]]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ru96YQP2MC35","colab_type":"code","outputId":"ce3c3f47-6759-4e93-cbf7-a017882fbdd8","executionInfo":{"status":"ok","timestamp":1587460644194,"user_tz":-480,"elapsed":781,"user":{"displayName":"N","photoUrl":"","userId":"08619141603155082384"}},"colab":{"base_uri":"https://localhost:8080/","height":650}},"source":["from HW3_Task2.agent.env import construct_task2_env_0\n","env = construct_task2_env_0()\n","\n","state = env.reset()\n","env.render()\n","for i in range(0,15):\n","  print(\"state {} {} {}\".format(\"all\",0,i), state[:,0,i])\n","print(state[0,9,48]) # Occupé = Voiture \n","print(state[1,9,49]) # Agent\n","print(state[2,9,47]) # Voiture\n","print(state[3,0,0]) # Goal\n","# 0 Occupé = Voiture \n","# 1 = agent\n","import numpy as np\n","print(state[1,9,:])\n","\n","for i in range(0,10):\n","  column = (np.where(state[1,i,:] == 1.))\n","  if len(column[0]) > 0:\n","    print(column[0][0],i)\n","\n","# Si > 47 -> y_max = 49 : y_min = 45\n","# Si < 3 -> y_max = 4 : y_min = 0\n","# Else -> y_max = y_agent + 2 / Y_min = y_agent - 2\n","\n","\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["========================================================================================================================================================================================================\n","  F   -   -   -   -   -   -   -   6   -   -   -   O   5   -   -   -   -   -   -   -   1   -   -   -   -   -   -   -   -   -   3   -   4   -   -   2   -   -   -   -   -   -   -   -   -   -   -   -   -\n","  -   -   -  13   -   -   -   -  14   -   -   -   -   -   -   8   -   -   -   -   -   9   -   -   -   -   -   7   -   -   -   -   -   -   -   -  10   -   -   -   -   -   -  12   -   -   -   -   -  11\n","  -   -   -   -   -  19   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -  17   -  16   -   -   -   -   -   -   -   -   -  18   -   -   -  20   -   -   -   -   -   -   -   -   -   -  15\n","  -   -   -   -   -   -   -   -   -   -   -  25   -  22   -  24   -   -   -   -   -   -  21   -   -  26   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -  23   -   -   -   -   -   -   -\n","  -  31   -   -   -   -   -  30  32  33   -   -   -   -   -   -   -  27   -   -   -   -   -   -   -  28   -   -   -   -   -   -   -   -   -   -  29   -   -   -   -   -   -   -   -   -   -   -   -   -\n","  -   -  41  38   -   -   -   -   -   -   -   -   -   -   -  35   -   -   -  37   -   -   -   -   -   -  36   -   -  39   -   -   -   -   -   -   -   -  34   -   -  40   -   -   -   -   -   -   -   -\n","  -   -   -   -   -   -   -   -  44   -   -   -   -   -   -  47  43  42   -  46   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -  45   -\n","  -   -   -   -  48   -  49   -   -   -   -  53   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -  50   -   -   -   -   -   -  51   -   -   -   -   -   -   -   -   -   -   -  54   -   -  52\n","  -   -   -   -   -   -   -   -   -   -  56   -   -   -   -  57   -  58   -   -   -   -   -   -   -  59   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -  60  55   -   -   -   -   -   -   -\n","  -   -  65   -   -   -   -   -   -   -   -   -   -  62   -   -  61   -   -   -   -   -   -   -   -   -   -   -   -  68   -  66   -   -   -   -   -   -   -   -   -   -   -   -  64   -  63  67   -   <\n","========================================================================================================================================================================================================\n","state all 0 0 [0. 0. 1. 0.]\n","state all 0 1 [0. 0. 0. 0.]\n","state all 0 2 [0. 0. 0. 0.]\n","state all 0 3 [0. 0. 0. 0.]\n","state all 0 4 [0. 0. 0. 0.]\n","state all 0 5 [0. 0. 0. 0.]\n","state all 0 6 [0. 0. 0. 0.]\n","state all 0 7 [0. 0. 0. 0.]\n","state all 0 8 [1. 0. 0. 0.]\n","state all 0 9 [0. 0. 0. 0.]\n","state all 0 10 [0. 0. 0. 0.]\n","state all 0 11 [0. 0. 0. 0.]\n","state all 0 12 [1. 0. 0. 0.]\n","state all 0 13 [1. 0. 0. 0.]\n","state all 0 14 [0. 0. 0. 0.]\n","0.0\n","1.0\n","0.0\n","0.0\n","[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 1.]\n","49 9\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"luZOdkPXSDQ6","colab_type":"code","outputId":"8028c895-cdaa-4913-808e-7a2cd682ea0a","executionInfo":{"status":"ok","timestamp":1587461215569,"user_tz":-480,"elapsed":1180,"user":{"displayName":"N","photoUrl":"","userId":"08619141603155082384"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["from HW3_Task2.agent.env import construct_task2_env_0\n","env = construct_task2_env_0()\n","\n","state = env.reset()\n","env.render()\n","\n","def pos_from_state(state):\n","  for i in range(0,10):\n","    column = (np.where(state[1,i,:] == 1.))\n","    if len(column[0]) > 0:\n","      column = column[0][0]\n","      row = i \n","      return(column,row)\n","\n","def state_from_pos(pos_x,pos_y,state):\n","  #Crop to keep a window [49,4]\n","  if pos_y > 8:\n","    y_max = 9\n","    y_min = 5\n","  elif pos_y < 2:\n","    y_max = 4\n","    y_min = 0\n","  else:\n","    y_max = pos_y + 2\n","    y_min = pos_y - 2\n","\n","  return(state[:,y_min:(y_max+1),:])\n","\n","pos_y,pos_x = pos_from_state(state)\n","next_state = state_from_pos(pos_x,pos_y,state)\n","\n","print(next_state)\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["========================================================================================================================================================================================================\n","  F   -   -   -   -   -   -   -   3   -   -   -   -   -   1   -   5   -   -   2   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   O   -   -   -   -   -   -   -   -   -   4   -   -   6   -\n","  -   -   -   -   -   -   -   -   -   -   -   -   -  14   -   -  13  12  10   -   -   -   -   -   -   -   -   -   -   -   8   -   -   -   7   -   -   -   -   -   9   -   -  11   -   -   -   -   -   -\n"," 17   -   -   -   -   -   -   -   -   -  18   -  20   -   -   -   -   -   -   -   -   -   -   -   -  19   -   -   -   -   -   -   -   -  16   -   -   -   -   -   -   -   -   -   -   -   -   -   -  15\n","  -   -   -   -   -   -   -   -   -   -   -   -   -   -  24  21   -   -   -   -  26   -   -  22  25   -   -   -   -   -   -   -   -  23   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -\n","  -  33   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -  31  30   -  29   -   -   -  27  28   -  32   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -\n","  -  40   -  39   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -  38   -   -   -   -   -  35   -  34   -  41   -  37   -   -   -   -   -   -   -   -   -  36   -   -   -   -   -   -   -\n","  -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -  45   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -  44   -  42  47   -   -  46  43   -   -   -   -   -   -   -   -\n","  -   -   -   -   -   -   -   -   -   -   -   -   -   -   -  51   -   -   -   -   -  50   -   -  49   -  52   -   -  48   -   -   -   -  54   -   -   -   -   -   -   -   -   -  53   -   -   -   -   -\n","  -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -  60   -   -  55   -   -   -   -   -   -   -  58   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -  56  57  59\n","  -   -  66  61  63  64   -  68   -   -   -   -   -   -  62  65   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -  67   -   -   -   -   -   -   -   -   <\n","========================================================================================================================================================================================================\n","[[[0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n","   0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n","   0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n","   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0.\n","   0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0.\n","   0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n","   0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n","   1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n","   0. 1. 1. 1.]\n","  [0. 0. 1. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.\n","   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n","   0. 0. 0. 0.]]\n","\n"," [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n","   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n","   0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n","   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n","   0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n","   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n","   0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n","   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n","   0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n","   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n","   0. 0. 0. 1.]]\n","\n"," [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n","   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n","   0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n","   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n","   0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n","   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n","   0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n","   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n","   0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n","   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n","   0. 0. 0. 0.]]\n","\n"," [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n","   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n","   0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n","   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n","   0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n","   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n","   0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n","   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n","   0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n","   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n","   0. 0. 0. 0.]]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_RduuyWID855","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}