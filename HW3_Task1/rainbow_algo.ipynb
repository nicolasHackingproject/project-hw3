{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"rainbow_algo.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"sCXRy7hMe_eN","colab_type":"text"},"source":["## 1. Read Me \n","**Run first cell :** install dependencies \n","\n","**Run second cell :** install files and local environment on the VM, choose which task you want to run by setting task_1 or task_2 value to true\n","\n","**Run third cell :** try diferent init of the agent. When one is found -> the agent is trained  "]},{"cell_type":"code","metadata":{"id":"8Rbvg4vCI9nE","colab_type":"code","outputId":"334e3de7-dbc2-458b-fa9d-764c5245d83d","executionInfo":{"status":"ok","timestamp":1587371444056,"user_tz":-480,"elapsed":9659,"user":{"displayName":"N","photoUrl":"","userId":"08619141603155082384"}},"colab":{"base_uri":"https://localhost:8080/","height":352}},"source":["# Dependencies\n","\n","!pip install torch numpy git+https://github.com/cs4246/gym-grid-driving.git\n","!pip install -U -q PyDrive"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting git+https://github.com/cs4246/gym-grid-driving.git\n","  Cloning https://github.com/cs4246/gym-grid-driving.git to /tmp/pip-req-build-us5n0iqd\n","  Running command git clone -q https://github.com/cs4246/gym-grid-driving.git /tmp/pip-req-build-us5n0iqd\n","Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.18.2)\n","Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (from gym-grid-driving==0.0.1) (0.17.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym->gym-grid-driving==0.0.1) (1.4.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym->gym-grid-driving==0.0.1) (1.12.0)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym->gym-grid-driving==0.0.1) (1.5.0)\n","Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym->gym-grid-driving==0.0.1) (1.3.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym->gym-grid-driving==0.0.1) (0.16.0)\n","Building wheels for collected packages: gym-grid-driving\n","  Building wheel for gym-grid-driving (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gym-grid-driving: filename=gym_grid_driving-0.0.1-cp36-none-any.whl size=8623 sha256=cff0f34df1b1c46037ad8ccc37274b123a3111da914d361f21dbff165ecfb950\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-uwnh8o36/wheels/e1/30/f2/157c0938ab9bfe9c10c29c9fcab8392f587c9d141f215b67ca\n","Successfully built gym-grid-driving\n","Installing collected packages: gym-grid-driving\n","Successfully installed gym-grid-driving-0.0.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SAAnK9T1iL8K","colab_type":"code","outputId":"f479707a-5526-4d8a-96ef-8fc93df33c17","executionInfo":{"status":"ok","timestamp":1587371043662,"user_tz":-480,"elapsed":3638,"user":{"displayName":"N","photoUrl":"","userId":"08619141603155082384"}},"colab":{"base_uri":"https://localhost:8080/","height":105}},"source":["!git clone https://github.com/Kaixhin/Rainbow.git"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Cloning into 'Rainbow'...\n","remote: Enumerating objects: 644, done.\u001b[K\n","remote: Total 644 (delta 0), reused 0 (delta 0), pack-reused 644\u001b[K\n","Receiving objects: 100% (644/644), 159.28 KiB | 4.30 MiB/s, done.\n","Resolving deltas: 100% (432/432), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cjrdeWB-JA1x","colab_type":"code","outputId":"05cd30bf-dcc8-44b8-b7d5-ebba342694ad","executionInfo":{"status":"ok","timestamp":1587371466558,"user_tz":-480,"elapsed":22487,"user":{"displayName":"N","photoUrl":"","userId":"08619141603155082384"}},"colab":{"base_uri":"https://localhost:8080/","height":350}},"source":["# Authentification / Initialization of workspace \n","\n","import os\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)\n","\n","# Create repo folders \n","\n","task_1 = False\n","# If False task_2 workplace will be downloaded\n","\n","if task_1:\n","  number_task = 1\n","else:\n","  number_task = 2\n","\n","root = '/content'\n","local_download = os.path.join(root,'HW3_Task{}'.format(number_task))\n","\n","if not(os.path.exists(local_download)): \n","  os.mkdir(local_download)\n","\n","local_download_agent = os.path.join(local_download,'agent')\n","\n","if not(os.path.exists(local_download_agent)): \n","  os.mkdir(local_download_agent)\n","\n","  # Download files \n","\n","\n","def download_list(file_list,path_name):\n","    \n","  error_l = [];\n","    \n","  for f in file_list:\n","    # 3. Create & download by id.\n","  \n","    print('file found : title: %s, id: %s' % (f['title'], f['id']))\n","    try:\n","      #print('title: %s, id: %s' % (f['title'], f['id']))\n","      fname = os.path.join(path_name, f['title'])\n","      \n","      # Download only .py files\n","      if fname[-3:] == \".py\":\n","        print('downloading to {}'.format(fname))\n","        f_ = drive.CreateFile({'id': f['id']})\n","        f_.GetContentFile(fname)\n","      \n","    except:\n","      print(\"there is an error\")          \n","      error_l.append(fname)\n","\n","\n","if task_1:\n","  # Initial folder\n","  local_download_path = local_download_agent\n","\n","  # Agent files \n","  file_list = drive.ListFile(\n","        {'q': \"'1ktZR8KIIWEi8Cre92SFomSEchWleHtSu' in parents\"}).GetList()\n","\n","  download_list(file_list,local_download_path)\n","\n","\n","  # Initial files\n","  local_download_path = local_download\n","\n","  file_list = drive.ListFile(\n","        {'q': \"'1x4sJIKHA6NZ5y78AnOJeCZP8abI4gLLO' in parents\"}).GetList()\n","\n","  download_list(file_list,local_download_path)\n","\n","else:\n","\n","  # Initial folder\n","  local_download_path = local_download_agent\n","\n","  # Agent files \n","  file_list = drive.ListFile(\n","        {'q': \"'1PUrHkG6ki4JsiXtKOtvc2vAuR6IJMnCa' in parents\"}).GetList()\n","\n","  download_list(file_list,local_download_path)\n","\n","\n","  # Initial files\n","  local_download_path = local_download\n","\n","  file_list = drive.ListFile(\n","        {'q': \"'17wsroYnRmoTt-OIzDJRrUNvVFmohBVFR' in parents\"}).GetList()\n","\n","  download_list(file_list,local_download_path)\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["file found : title: __init__.py, id: 1WO1O3RwqCpk7XdN07UUZbF6YTaj5mGGg\n","downloading to /content/HW3_Task2/agent/__init__.py\n","file found : title: models.py, id: 1BW6dbcbWvmC7JbqE78269K1IXIPwtlEi\n","downloading to /content/HW3_Task2/agent/models.py\n","file found : title: model.pt, id: 1_hr_fysoSC6AXix1Qjqq0bBYk0KiSIcq\n","file found : title: .ipynb_checkpoints, id: 14rimDklDDBO4_OqcqfbiybhzNMxQQFSm\n","file found : title: env.py, id: 1bYr6NdzVOXe77n-gUac3VdGA8tnpS8HJ\n","downloading to /content/HW3_Task2/agent/env.py\n","file found : title: ZZZ.zip, id: 1CVWs9pxeQ6FieIRMszIIfNK1my32N3Wl\n","file found : title: ZZZ, id: 10kHEkPFSjAINnDqi2s5KF7PxQN9bImk3\n","file found : title: try2 2, id: 1Ka1yYoo2aGsxPITkqKfBYB4AAHln0aMZ\n","file found : title: try2.zip, id: 1o5hCQ_s6UkmcbRNTkR8crcNcftp1IhZ7\n","file found : title: try2 2.zip, id: 1R8lmVqVxgvfuQ150WK4ZckNmfjvWmvW4\n","file found : title: try2, id: 1T1A0dN0enM4yy8DmA6A7HJOFyGQS0IlX\n","file found : title: A0212190W_Task2.zip, id: 1XJ9tRmPvxju3IjD76WkmINyUlYTIJvVm\n","file found : title: agent, id: 1PUrHkG6ki4JsiXtKOtvc2vAuR6IJMnCa\n","file found : title: setup.py, id: 1GrxqesqYqNJit3lNQABi9n3Wisa5zuir\n","downloading to /content/HW3_Task2/setup.py\n","file found : title: MANIFEST.in, id: 16_Iy5dYGYMUP2T9hl-OZdODIhV7dbgCf\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kM9mre8d4VL-","colab_type":"code","outputId":"d9a9e980-4069-4b72-de74-03139eea438f","executionInfo":{"status":"error","timestamp":1587371501807,"user_tz":-480,"elapsed":3039,"user":{"displayName":"N","photoUrl":"","userId":"08619141603155082384"}},"colab":{"base_uri":"https://localhost:8080/","height":473}},"source":["# TASK 2\n","\n","import sys\n","import time\n","\n","\n","date_save = time.strftime(\"%d %H %M\")\n","model_path = os.path.join('HW3_Task2','agent', 'model_{}_{}.pt'.format(date_save,2))\n","save_path = os.path.join('HW3_Task2','agent', 'model_{}_{}.pt'.format(date_save,3))\n","\n","duree = 0 \n","nb_bad_init = 0 \n","# Try for 10 diferent inits\n","\n","for i in range(0,1):\n","  debut = time.time()\n","  # Try an init \n","  !python3 HW3_Task2/agent/models.py --pretrain --path=HW3_Task2/agent/model_cb_19_19_55_45_49_7.pt --savepath=HW3_Task2/agent/model_20_05_\n","  #!python3 HW3_Task2/agent/testenv.py\n","  duree = (time.time()-debut)\n","  # If good init exit loop\n","  print(duree)\n","  if duree > 10000:\n","    break\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["  File \"HW3_Task2/agent/models.py\", line 418\n","    ),\n","    ^\n","SyntaxError: invalid syntax\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-85cc76d8810b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0mdebut\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0;31m# Try an init\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m   \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'python3 HW3_Task2/agent/models.py --pretrain --path=HW3_Task2/agent/model_cb_19_19_55_45_49_7.pt --savepath=HW3_Task2/agent/model_20_05_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m   \u001b[0;31m#!python3 HW3_Task2/agent/testenv.py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0mduree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdebut\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'also_return_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_system_commands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_system_compat\u001b[0;34m(shell, cmd, also_return_output)\u001b[0m\n\u001b[1;32m    436\u001b[0m   \u001b[0;31m# stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m   result = _run_command(\n\u001b[0;32m--> 438\u001b[0;31m       shell.var_expand(cmd, depth=2), clear_streamed_output=False)\n\u001b[0m\u001b[1;32m    439\u001b[0m   \u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_exit_code'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_INTERRUPTED_SIGNALS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_run_command\u001b[0;34m(cmd, clear_streamed_output)\u001b[0m\n\u001b[1;32m    193\u001b[0m       \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild_pty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0m_monitor_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_pty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_stdin_widget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0mepoll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_display_stdin_widget\u001b[0;34m(delay_millis)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m   \u001b[0mhide_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'cell_remove_stdin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m   \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocking_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mhide_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_header\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    169\u001b[0m   \u001b[0;31m# unique.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m   \u001b[0mrequest_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msend_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     if (reply.get('type') == 'colab_reply' and\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"atNXnxM_NcTN","colab_type":"code","outputId":"4c778343-f990-499a-c52b-8a0b4fcdaa79","executionInfo":{"status":"ok","timestamp":1587371416367,"user_tz":-480,"elapsed":24465,"user":{"displayName":"N","photoUrl":"","userId":"08619141603155082384"}},"colab":{"base_uri":"https://localhost:8080/","height":825}},"source":["import sys\n","IN_COLAB = \"google.colab\" in sys.modules\n","\n","if IN_COLAB:\n","    !apt install python-opengl\n","    !apt install ffmpeg\n","    !apt install xvfb\n","    !pip install pyvirtualdisplay\n","    from pyvirtualdisplay import Display\n","    \n","    # Start virtual display\n","    dis = Display(visible=0, size=(400, 400))\n","    dis.start()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","Suggested packages:\n","  libgle3\n","The following NEW packages will be installed:\n","  python-opengl\n","0 upgraded, 1 newly installed, 0 to remove and 25 not upgraded.\n","Need to get 496 kB of archives.\n","After this operation, 5,416 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python-opengl all 3.1.0+dfsg-1 [496 kB]\n","Fetched 496 kB in 1s (677 kB/s)\n","Selecting previously unselected package python-opengl.\n","(Reading database ... 144568 files and directories currently installed.)\n","Preparing to unpack .../python-opengl_3.1.0+dfsg-1_all.deb ...\n","Unpacking python-opengl (3.1.0+dfsg-1) ...\n","Setting up python-opengl (3.1.0+dfsg-1) ...\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","ffmpeg is already the newest version (7:3.4.6-0ubuntu0.18.04.1).\n","0 upgraded, 0 newly installed, 0 to remove and 25 not upgraded.\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","The following NEW packages will be installed:\n","  xvfb\n","0 upgraded, 1 newly installed, 0 to remove and 25 not upgraded.\n","Need to get 784 kB of archives.\n","After this operation, 2,266 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.4 [784 kB]\n","Fetched 784 kB in 1s (991 kB/s)\n","Selecting previously unselected package xvfb.\n","(Reading database ... 146923 files and directories currently installed.)\n","Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.4_amd64.deb ...\n","Unpacking xvfb (2:1.19.6-1ubuntu4.4) ...\n","Setting up xvfb (2:1.19.6-1ubuntu4.4) ...\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n","Collecting pyvirtualdisplay\n","  Downloading https://files.pythonhosted.org/packages/69/ec/8221a07850d69fa3c57c02e526edd23d18c7c05d58ed103e3b19172757c1/PyVirtualDisplay-0.2.5-py2.py3-none-any.whl\n","Collecting EasyProcess\n","  Downloading https://files.pythonhosted.org/packages/32/8f/88d636f1da22a3c573259e44cfefb46a117d3f9432e2c98b1ab4a21372ad/EasyProcess-0.2.10-py2.py3-none-any.whl\n","Installing collected packages: EasyProcess, pyvirtualdisplay\n","Successfully installed EasyProcess-0.2.10 pyvirtualdisplay-0.2.5\n"],"name":"stdout"},{"output_type":"stream","text":["xdpyinfo was not found, X start can not be checked! Please install xdpyinfo!\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"oEhO5nj6eZz8","colab_type":"code","outputId":"cf0d45c5-c89a-45d4-ceed-0bef6375c547","executionInfo":{"status":"ok","timestamp":1587371421280,"user_tz":-480,"elapsed":19941,"user":{"displayName":"N","photoUrl":"","userId":"08619141603155082384"}},"colab":{"base_uri":"https://localhost:8080/","height":210}},"source":["import math\n","import os\n","import random\n","from collections import deque\n","from typing import Deque, Dict, List, Tuple\n","\n","import gym\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from IPython.display import clear_output\n","from torch.nn.utils import clip_grad_norm_\n","\n","# download segment tree module\n","if IN_COLAB:\n","    !wget https://raw.githubusercontent.com/curt-park/rainbow-is-all-you-need/master/segment_tree.py\n","\n","from segment_tree import MinSegmentTree, SumSegmentTree"],"execution_count":0,"outputs":[{"output_type":"stream","text":["--2020-04-20 08:30:20--  https://raw.githubusercontent.com/curt-park/rainbow-is-all-you-need/master/segment_tree.py\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 4283 (4.2K) [text/plain]\n","Saving to: ‘segment_tree.py’\n","\n","\rsegment_tree.py       0%[                    ]       0  --.-KB/s               \rsegment_tree.py     100%[===================>]   4.18K  --.-KB/s    in 0s      \n","\n","2020-04-20 08:30:20 (53.1 MB/s) - ‘segment_tree.py’ saved [4283/4283]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qS_dXtwjSEFH","colab_type":"code","colab":{}},"source":["class ReplayBuffer:\n","    \"\"\"A simple numpy replay buffer.\"\"\"\n","\n","    def __init__(\n","        self, \n","        obs_dim: int, \n","        size: int, \n","        batch_size: int = 32, \n","        n_step: int = 1, \n","        gamma: float = 0.99\n","    ):\n","        self.obs_buf = np.zeros([size, *obs_dim], dtype=np.float32)\n","        self.next_obs_buf = np.zeros([size, *obs_dim], dtype=np.float32)\n","        self.acts_buf = np.zeros([size], dtype=np.float32)\n","        self.rews_buf = np.zeros([size], dtype=np.float32)\n","        self.done_buf = np.zeros(size, dtype=np.float32)\n","        self.max_size, self.batch_size = size, batch_size\n","        self.ptr, self.size, = 0, 0\n","        \n","        # for N-step Learning\n","        self.n_step_buffer = deque(maxlen=n_step)\n","        self.n_step = n_step\n","        self.gamma = gamma\n","\n","    def store(\n","        self, \n","        obs: np.ndarray, \n","        act: np.ndarray, \n","        rew: float, \n","        next_obs: np.ndarray, \n","        done: bool,\n","    ) -> Tuple[np.ndarray, np.ndarray, float, np.ndarray, bool]:\n","        transition = (obs, act, rew, next_obs, done)\n","        self.n_step_buffer.append(transition)\n","\n","        # single step transition is not ready\n","        if len(self.n_step_buffer) < self.n_step:\n","            return ()\n","        \n","        # make a n-step transition\n","        rew, next_obs, done = self._get_n_step_info(\n","            self.n_step_buffer, self.gamma\n","        )\n","        obs, act = self.n_step_buffer[0][:2]\n","        \n","        self.obs_buf[self.ptr] = obs\n","        self.next_obs_buf[self.ptr] = next_obs\n","        self.acts_buf[self.ptr] = act\n","        self.rews_buf[self.ptr] = rew\n","        self.done_buf[self.ptr] = done\n","        self.ptr = (self.ptr + 1) % self.max_size\n","        self.size = min(self.size + 1, self.max_size)\n","        \n","        return self.n_step_buffer[0]\n","\n","    def sample_batch(self) -> Dict[str, np.ndarray]:\n","        idxs = np.random.choice(self.size, size=self.batch_size, replace=False)\n","\n","        return dict(\n","            obs=self.obs_buf[idxs],\n","            next_obs=self.next_obs_buf[idxs],\n","            acts=self.acts_buf[idxs],\n","            rews=self.rews_buf[idxs],\n","            done=self.done_buf[idxs],\n","            # for N-step Learning\n","            indices=indices,\n","        )\n","    \n","    def sample_batch_from_idxs(\n","        self, idxs: np.ndarray\n","    ) -> Dict[str, np.ndarray]:\n","        # for N-step Learning\n","        return dict(\n","            obs=self.obs_buf[idxs],\n","            next_obs=self.next_obs_buf[idxs],\n","            acts=self.acts_buf[idxs],\n","            rews=self.rews_buf[idxs],\n","            done=self.done_buf[idxs],\n","        )\n","    \n","    def _get_n_step_info(\n","        self, n_step_buffer: Deque, gamma: float\n","    ) -> Tuple[np.int64, np.ndarray, bool]:\n","        \"\"\"Return n step rew, next_obs, and done.\"\"\"\n","        # info of the last transition\n","        rew, next_obs, done = n_step_buffer[-1][-3:]\n","\n","        for transition in reversed(list(n_step_buffer)[:-1]):\n","            r, n_o, d = transition[-3:]\n","\n","            rew = r + gamma * rew * (1 - d)\n","            next_obs, done = (n_o, d) if d else (next_obs, done)\n","\n","        return rew, next_obs, done\n","\n","    def __len__(self) -> int:\n","        return self.size\n","\n","class PrioritizedReplayBuffer(ReplayBuffer):\n","    \"\"\"Prioritized Replay buffer.\n","    \n","    Attributes:\n","        max_priority (float): max priority\n","        tree_ptr (int): next index of tree\n","        alpha (float): alpha parameter for prioritized replay buffer\n","        sum_tree (SumSegmentTree): sum tree for prior\n","        min_tree (MinSegmentTree): min tree for min prior to get max weight\n","        \n","    \"\"\"\n","    \n","    def __init__(\n","        self, \n","        obs_dim: int, \n","        size: int, \n","        batch_size: int = 32, \n","        alpha: float = 0.6,\n","        n_step: int = 1, \n","        gamma: float = 0.99,\n","    ):\n","        \"\"\"Initialization.\"\"\"\n","        assert alpha >= 0\n","        \n","        super(PrioritizedReplayBuffer, self).__init__(\n","            obs_dim, size, batch_size, n_step, gamma\n","        )\n","        self.max_priority, self.tree_ptr = 1.0, 0\n","        self.alpha = alpha\n","        \n","        # capacity must be positive and a power of 2.\n","        tree_capacity = 1\n","        while tree_capacity < self.max_size:\n","            tree_capacity *= 2\n","\n","        self.sum_tree = SumSegmentTree(tree_capacity)\n","        self.min_tree = MinSegmentTree(tree_capacity)\n","        \n","    def store(\n","        self, \n","        obs: np.ndarray, \n","        act: int, \n","        rew: float, \n","        next_obs: np.ndarray, \n","        done: bool,\n","    ) -> Tuple[np.ndarray, np.ndarray, float, np.ndarray, bool]:\n","        \"\"\"Store experience and priority.\"\"\"\n","        transition = super().store(obs, act, rew, next_obs, done)\n","        \n","        if transition:\n","            self.sum_tree[self.tree_ptr] = self.max_priority ** self.alpha\n","            self.min_tree[self.tree_ptr] = self.max_priority ** self.alpha\n","            self.tree_ptr = (self.tree_ptr + 1) % self.max_size\n","        \n","        return transition\n","\n","    def sample_batch(self, beta: float = 0.4) -> Dict[str, np.ndarray]:\n","        \"\"\"Sample a batch of experiences.\"\"\"\n","        assert len(self) >= self.batch_size\n","        assert beta > 0\n","        \n","        indices = self._sample_proportional()\n","        \n","        obs = self.obs_buf[indices]\n","        next_obs = self.next_obs_buf[indices]\n","        acts = self.acts_buf[indices]\n","        rews = self.rews_buf[indices]\n","        done = self.done_buf[indices]\n","        weights = np.array([self._calculate_weight(i, beta) for i in indices])\n","        \n","        return dict(\n","            obs=obs,\n","            next_obs=next_obs,\n","            acts=acts,\n","            rews=rews,\n","            done=done,\n","            weights=weights,\n","            indices=indices,\n","        )\n","        \n","    def update_priorities(self, indices: List[int], priorities: np.ndarray):\n","        \"\"\"Update priorities of sampled transitions.\"\"\"\n","        assert len(indices) == len(priorities)\n","\n","        for idx, priority in zip(indices, priorities):\n","            assert priority > 0\n","            assert 0 <= idx < len(self)\n","\n","            self.sum_tree[idx] = priority ** self.alpha\n","            self.min_tree[idx] = priority ** self.alpha\n","\n","            self.max_priority = max(self.max_priority, priority)\n","            \n","    def _sample_proportional(self) -> List[int]:\n","        \"\"\"Sample indices based on proportions.\"\"\"\n","        indices = []\n","        p_total = self.sum_tree.sum(0, len(self) - 1)\n","        segment = p_total / self.batch_size\n","        \n","        for i in range(self.batch_size):\n","            a = segment * i\n","            b = segment * (i + 1)\n","            upperbound = random.uniform(a, b)\n","            idx = self.sum_tree.retrieve(upperbound)\n","            indices.append(idx)\n","            \n","        return indices\n","    \n","    def _calculate_weight(self, idx: int, beta: float):\n","        \"\"\"Calculate the weight of the experience at idx.\"\"\"\n","        # get max weight\n","        p_min = self.min_tree.min() / self.sum_tree.sum()\n","        max_weight = (p_min * len(self)) ** (-beta)\n","        \n","        # calculate weights\n","        p_sample = self.sum_tree[idx] / self.sum_tree.sum()\n","        weight = (p_sample * len(self)) ** (-beta)\n","        weight = weight / max_weight\n","        \n","        return weight\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1RIkGEjdJw83","colab_type":"code","colab":{}},"source":["class NoisyLinear(nn.Module):\n","    \"\"\"Noisy linear module for NoisyNet.\n","    \n","    \n","        \n","    Attributes:\n","        in_features (int): input size of linear module\n","        out_features (int): output size of linear module\n","        std_init (float): initial std value\n","        weight_mu (nn.Parameter): mean value weight parameter\n","        weight_sigma (nn.Parameter): std value weight parameter\n","        bias_mu (nn.Parameter): mean value bias parameter\n","        bias_sigma (nn.Parameter): std value bias parameter\n","        \n","    \"\"\"\n","\n","    def __init__(\n","        self, \n","        in_features: int, \n","        out_features: int, \n","        std_init: float = 0.5,\n","    ):\n","        \"\"\"Initialization.\"\"\"\n","        super(NoisyLinear, self).__init__()\n","        \n","        self.in_features = in_features\n","        self.out_features = out_features\n","        self.std_init = std_init\n","\n","        self.weight_mu = nn.Parameter(torch.Tensor(out_features, in_features))\n","        self.weight_sigma = nn.Parameter(\n","            torch.Tensor(out_features, in_features)\n","        )\n","        self.register_buffer(\n","            \"weight_epsilon\", torch.Tensor(out_features, in_features)\n","        )\n","\n","        self.bias_mu = nn.Parameter(torch.Tensor(out_features))\n","        self.bias_sigma = nn.Parameter(torch.Tensor(out_features))\n","        self.register_buffer(\"bias_epsilon\", torch.Tensor(out_features))\n","\n","        self.reset_parameters()\n","        self.reset_noise()\n","\n","    def reset_parameters(self):\n","        \"\"\"Reset trainable network parameters (factorized gaussian noise).\"\"\"\n","        mu_range = 1 / math.sqrt(self.in_features)\n","        self.weight_mu.data.uniform_(-mu_range, mu_range)\n","        self.weight_sigma.data.fill_(\n","            self.std_init / math.sqrt(self.in_features)\n","        )\n","        self.bias_mu.data.uniform_(-mu_range, mu_range)\n","        self.bias_sigma.data.fill_(\n","            self.std_init / math.sqrt(self.out_features)\n","        )\n","\n","    def reset_noise(self):\n","        \"\"\"Make new noise.\"\"\"\n","        epsilon_in = self.scale_noise(self.in_features)\n","        epsilon_out = self.scale_noise(self.out_features)\n","\n","        # outer product\n","        self.weight_epsilon.copy_(epsilon_out.ger(epsilon_in))\n","        self.bias_epsilon.copy_(epsilon_out)\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        \"\"\"Forward method implementation.\n","        \n","        We don't use separate statements on train / eval mode.\n","        It doesn't show remarkable difference of performance.\n","        \"\"\"\n","        return F.linear(\n","            x,\n","            self.weight_mu + self.weight_sigma * self.weight_epsilon,\n","            self.bias_mu + self.bias_sigma * self.bias_epsilon,\n","        )\n","    \n","    @staticmethod\n","    def scale_noise(size: int) -> torch.Tensor:\n","        \"\"\"Set scale to make noise (factorized gaussian noise).\"\"\"\n","        x = torch.FloatTensor(np.random.normal(loc=0.0, scale=1.0, size=size))\n","\n","        return x.sign().mul(x.abs().sqrt())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ru96YQP2MC35","colab_type":"code","colab":{}},"source":["class Network(nn.Module):\n","    \n","    '''\n","    \n","    class DQN(BaseAgent):\n","    def construct(self):\n","        self.layers = nn.Sequential(\n","            nn.Linear(self.feature_size(), 256),\n","            nn.ReLU(),\n","            nn.Linear(256, self.num_actions)\n","        )\n","\n","    class ConvDQN(DQN):\n","    def construct(self):\n","        self.features = nn.Sequential(\n","            nn.Conv2d(self.input_shape[0], 32, kernel_size=2),\n","            nn.ReLU(),\n","            nn.Conv2d(32, 64, kernel_size=2),\n","            nn.ReLU(),\n","        )\n","        super().construct()\n","      '''\n","\n","    \n","    def __init__(\n","        self,\n","        in_dim: int, \n","        out_dim: int,\n","        atom_size: int, \n","        support: torch.Tensor\n","    ):\n","        \"\"\"Initialization.\"\"\"\n","        super(Network, self).__init__()\n","        self.in_dim = in_dim\n","        self.support = support\n","        self.out_dim = out_dim\n","        self.atom_size = atom_size\n","\n","        self.conv_features = nn.Sequential(\n","            nn.Conv2d(self.in_dim[0], 32, kernel_size=2),\n","            nn.ReLU(),\n","            nn.Conv2d(32, 64, kernel_size=2),\n","            nn.ReLU(),\n","        )\n","\n","        # set common feature layer\n","        self.feature_layer = nn.Sequential(\n","            nn.Linear(24576, 256), \n","            nn.ReLU(),\n","        )\n","        \n","        # set advantage layer\n","        self.advantage_hidden_layer = NoisyLinear(256, 256)\n","        self.advantage_layer = NoisyLinear(256, out_dim * atom_size)\n","\n","        # set value layer\n","        self.value_hidden_layer = NoisyLinear(256, 256)\n","        self.value_layer = NoisyLinear(256, atom_size)\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        \"\"\"Forward method implementation.\"\"\"\n","        dist = self.dist(x)\n","        q = torch.sum(dist * self.support, dim=2)\n","        \n","        return q\n","    \n","    def dist(self, x: torch.Tensor) -> torch.Tensor:\n","        \"\"\"Get distribution for atoms.\"\"\"\n","        \n","        x = self.conv_features(x)\n","        \n","        # Resize features \n","        x = x.view(x.size(0), -1)\n","        feature = self.feature_layer(x)\n","\n","\n","        adv_hid = F.relu(self.advantage_hidden_layer(feature))\n","        val_hid = F.relu(self.value_hidden_layer(feature))\n","        \n","        advantage = self.advantage_layer(adv_hid).view(\n","            -1, self.out_dim, self.atom_size\n","        )\n","        value = self.value_layer(val_hid).view(-1, 1, self.atom_size)\n","        q_atoms = value + advantage - advantage.mean(dim=1, keepdim=True)\n","        \n","        dist = F.softmax(q_atoms, dim=-1)\n","        dist = dist.clamp(min=1e-3)  # for avoiding nans\n","        \n","        return dist\n","    \n","    def reset_noise(self):\n","        \"\"\"Reset all noisy layers.\"\"\"\n","        self.advantage_hidden_layer.reset_noise()\n","        self.advantage_layer.reset_noise()\n","        self.value_hidden_layer.reset_noise()\n","        self.value_layer.reset_noise()\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"M8T7RYz5Zjzw","colab_type":"code","colab":{}},"source":["class DQNAgent:\n","    \"\"\"DQN Agent interacting with environment.\n","    \n","    Attribute:\n","        env (gym.Env): openAI Gym environment\n","        memory (PrioritizedReplayBuffer): replay memory to store transitions\n","        batch_size (int): batch size for sampling\n","        target_update (int): period for target model's hard update\n","        gamma (float): discount factor\n","        dqn (Network): model to train and select actions\n","        dqn_target (Network): target model to update\n","        optimizer (torch.optim): optimizer for training dqn\n","        transition (list): transition information including \n","                           state, action, reward, next_state, done\n","        v_min (float): min value of support\n","        v_max (float): max value of support\n","        atom_size (int): the unit number of support\n","        support (torch.Tensor): support for categorical dqn\n","        use_n_step (bool): whether to use n_step memory\n","        n_step (int): step number to calculate n-step td error\n","        memory_n (ReplayBuffer): n-step replay buffer\n","    \"\"\"\n","\n","    def __init__(\n","        self, \n","        env: gym.Env,\n","        memory_size: int,\n","        batch_size: int,\n","        target_update: int,\n","        gamma: float = 0.99,\n","        # PER parameters\n","        alpha: float = 0.2,\n","        beta: float = 0.6,\n","        prior_eps: float = 1e-6,\n","        # Categorical DQN parameters\n","        v_min: float = 0.0,\n","        v_max: float = 200.0,\n","        atom_size: int = 51,\n","        # N-step Learning\n","        n_step: int = 3,\n","    ):\n","        \"\"\"Initialization.\n","        \n","        Args:\n","            env (gym.Env): openAI Gym environment\n","            memory_size (int): length of memory\n","            batch_size (int): batch size for sampling\n","            target_update (int): period for target model's hard update\n","            lr (float): learning rate\n","            gamma (float): discount factor\n","            alpha (float): determines how much prioritization is used\n","            beta (float): determines how much importance sampling is used\n","            prior_eps (float): guarantees every transition can be sampled\n","            v_min (float): min value of support\n","            v_max (float): max value of support\n","            atom_size (int): the unit number of support\n","            n_step (int): step number to calculate n-step td error\n","        \"\"\"\n","        obs_dim = env.observation_space.shape\n","        action_dim = env.action_space.n\n","        \n","        self.env = env\n","        self.batch_size = batch_size\n","        self.target_update = target_update\n","        self.gamma = gamma\n","        # NoisyNet: All attributes related to epsilon are removed\n","        \n","        # device: cpu / gpu\n","        \n","        self.device = torch.device(\n","            \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","        )\n","        print(self.device)\n","        \n","        # PER\n","        # memory for 1-step Learning\n","        self.beta = beta\n","        self.prior_eps = prior_eps\n","        self.memory = PrioritizedReplayBuffer(\n","            obs_dim, memory_size, batch_size, alpha=alpha\n","        )\n","        \n","        # memory for N-step Learning\n","        self.use_n_step = True if n_step > 1 else False\n","        if self.use_n_step:\n","            self.n_step = n_step\n","            self.memory_n = ReplayBuffer(\n","                obs_dim, memory_size, batch_size, n_step=n_step, gamma=gamma\n","            )\n","            \n","        # Categorical DQN parameters\n","        self.v_min = v_min\n","        self.v_max = v_max\n","        self.atom_size = atom_size\n","        self.support = torch.linspace(\n","            self.v_min, self.v_max, self.atom_size\n","        ).to(self.device)\n","\n","        # networks: dqn, dqn_target\n","        self.dqn = Network(\n","            obs_dim, action_dim, self.atom_size, self.support\n","        ).to(self.device)\n","        self.dqn_target = Network(\n","            obs_dim, action_dim, self.atom_size, self.support\n","        ).to(self.device)\n","        self.dqn_target.load_state_dict(self.dqn.state_dict())\n","        self.dqn_target.eval()\n","        \n","        # optimizer\n","        self.optimizer = optim.Adam(self.dqn.parameters())\n","\n","        # transition to store in memory\n","        self.transition = list()\n","        \n","        # mode: train / test\n","        self.is_test = False\n","\n","    def select_action(self, state: np.ndarray) -> np.ndarray:\n","        \"\"\"Select an action from the input state.\"\"\"\n","        # NoisyNet: no epsilon greedy action selection\n","        if not isinstance(state, torch.FloatTensor):\n","            state_up = torch.from_numpy(state).float().unsqueeze(0).to(device)\n","        #state = state.unsqueeze(0).to(device)\n","        selected_action = self.dqn(\n","            state_up\n","        ).argmax()\n","        selected_action = selected_action.detach().cpu().numpy()\n","        \n","        if not self.is_test:\n","            self.transition = [state, selected_action]\n","        \n","        return selected_action\n","\n","    def step(self, action: np.ndarray) -> Tuple[np.ndarray, np.float64, bool]:\n","        \"\"\"Take an action and return the response of the env.\"\"\"\n","        next_state, reward, done, _ = self.env.step(action)\n","\n","        if not self.is_test:\n","            self.transition += [reward, next_state, done]\n","            \n","            # N-step transition\n","            if self.use_n_step:\n","                one_step_transition = self.memory_n.store(*self.transition)\n","            # 1-step transition\n","            else:\n","                one_step_transition = self.transition\n","\n","            # add a single step transition\n","            if one_step_transition:\n","                self.memory.store(*one_step_transition)\n","    \n","        return next_state, reward, done\n","\n","    def update_model(self) -> torch.Tensor:\n","        \"\"\"Update the model by gradient descent.\"\"\"\n","        # PER needs beta to calculate weights\n","        samples = self.memory.sample_batch(self.beta)\n","        weights = torch.FloatTensor(\n","            samples[\"weights\"].reshape(-1, 1)\n","        ).to(self.device)\n","        indices = samples[\"indices\"]\n","        \n","        # 1-step Learning loss\n","        elementwise_loss = self._compute_dqn_loss(samples, self.gamma)\n","        \n","        # PER: importance sampling before average\n","        loss = torch.mean(elementwise_loss * weights)\n","        \n","        # N-step Learning loss\n","        # we are gonna combine 1-step loss and n-step loss so as to\n","        # prevent high-variance. The original rainbow employs n-step loss only.\n","        if self.use_n_step:\n","            gamma = self.gamma ** self.n_step\n","            samples = self.memory_n.sample_batch_from_idxs(indices)\n","            elementwise_loss_n_loss = self._compute_dqn_loss(samples, gamma)\n","            elementwise_loss += elementwise_loss_n_loss\n","            \n","            # PER: importance sampling before average\n","            loss = torch.mean(elementwise_loss * weights)\n","\n","        self.optimizer.zero_grad()\n","        loss.backward()\n","        clip_grad_norm_(self.dqn.parameters(), 10.0)\n","        self.optimizer.step()\n","        \n","        # PER: update priorities\n","        loss_for_prior = elementwise_loss.detach().cpu().numpy()\n","        new_priorities = loss_for_prior + self.prior_eps\n","        self.memory.update_priorities(indices, new_priorities)\n","        \n","        # NoisyNet: reset noise\n","        self.dqn.reset_noise()\n","        self.dqn_target.reset_noise()\n","\n","        return loss.item()\n","        \n","    def train(self,config,num_frames_global: int, plotting_interval: int = 10):\n","        \"\"\"Train the agent.\"\"\"\n","        self.is_test = False\n","        \n","        update_cnt = 0\n","        losses = []\n","        scores = []\n","        score = 0\n","        t_max = 100\n","        nb_episode = 0 \n","        t_cur = 0\n","        restart_range = 20 \n","        num_frames = num_frames_global // 10\n","        for con_i in range(len(config)-1,len(config)):\n","          \n","          config_iter = config[con_i]\n","          print(config,config_iter)\n","          x_min = config_iter[0]\n","          x_max = config_iter[1]\n","          y_min = config_iter[2]\n","          y_max = config_iter[3]\n","\n","          print(\"New config prout : x min {} x max {} y min {} y max {}\".format(x_min,x_max,y_min,y_max))          \n","\n","          state = self.env.reset()\n","          \n","          for frame_idx in range(1, num_frames*10):\n","              t_cur += 1\n","              \n","              for time_step in range(0,1):\n","                action = self.select_action(state)\n","                \n","                next_state, reward, done = self.step(int(action))\n","\n","                state = next_state\n","                score += reward\n","                \n","                # NoisyNet: removed decrease of epsilon\n","                \n","                # PER: increase beta\n","                fraction = min(frame_idx / num_frames, 1.0)\n","                self.beta = self.beta + fraction * (1.0 - self.beta)\n","                \n","                #If episode do not end\n","                if t_cur > t_max:\n","                  done = True\n","                  #score = -1\n","                  t_cur = 0 \n","                  nb_episode +=1\n","                  if nb_episode % 5 ==0:\n","                    print(nb_episode)\n","                # if episode ends\n","                if done:\n","                    state = self.env.reset()\n","                    if not(score==10):\n","                      score = 0\n","                    scores.append(score)\n","                    \n","                    score = 0\n","                    t_cur = 0 \n","                    nb_episode +=1\n","                    \n","\n","                if nb_episode % 1000 == 0 and nb_episode >0 :\n","                  scores = []\n","                  for i in range(50):\n","                    scores.append(self.test())\n","                  scores_a = np.array(scores)\n","                  print(\"Test, Score moyen: {}, % de succes : {}\".format(np.mean(scores_a),np.mean(scores_a)/len(scores)))\n","\n","                if nb_episode % 1000 == 0 and nb_episode >0 :\n","                  try:\n","                    print(\"Nb episodes is: {}, last rewards {}, avg reward {}\".format(nb_episode,np.mean(scores[-100:]),np.mean(scores)))\n","                  except:pass\n","\n","                if nb_episode % 1000 == 0 and nb_episode >0 :\n","\n","                  i_1 = random.randint(x_min,x_max)\n","                  j_1 = random.randint(y_min,y_max)\n","                  self.env.close()\n","                  self.env = construct_task2_env(i_1,j_1)\n","                  state = self.env.reset()\n","                  print(\"New choice : i,j {},{}\".format(i_1,j_1))\n","\n","\n","\n","\n","\n","                # if training is ready\n","                if len(self.memory) >= self.batch_size:\n","                    loss = self.update_model()\n","                    losses.append(loss)\n","                    update_cnt += 1\n","                    \n","                    # if hard update is needed\n","                    if update_cnt % self.target_update == 0:\n","                        self._target_hard_update()\n","\n","              # plotting\n","              #if frame_idx % plotting_interval == 0:\n","              #    self._plot(frame_idx, scores, losses)\n","\n","          torch.save([self.dqn,self.dqn_target,self.dqn_target],\"HW3_Task2/rainbow_s/rainbow_{}_config_{}\".format(1,con_i))\n","\n","        self.env.close()\n","                \n","    def test(self):\n","        \"\"\"Test the agent.\"\"\"\n","        self.is_test = True\n","        \n","        state = self.env.reset()\n","        done = False\n","        score = 0\n","        \n","        while not done:\n","            #self.env.render()\n","            action = self.select_action(state)\n","            next_state, reward, done = self.step(int(action))\n","\n","            state = next_state\n","            score += reward\n","        \n","        #print(\"score: \", score)\n","        self.env.close()\n","        return(score)\n","\n","    def _compute_dqn_loss(self, samples: Dict[str, np.ndarray], gamma: float) -> torch.Tensor:\n","        \"\"\"Return categorical dqn loss.\"\"\"\n","        device = self.device  # for shortening the following lines\n","        state = torch.FloatTensor(samples[\"obs\"]).to(device)\n","        #state = state.unsqueeze(0).to(device)\n","        next_state = torch.FloatTensor(samples[\"next_obs\"]).to(device)\n","        action = torch.LongTensor(samples[\"acts\"]).to(device)\n","        reward = torch.FloatTensor(samples[\"rews\"].reshape(-1, 1)).to(device)\n","        done = torch.FloatTensor(samples[\"done\"].reshape(-1, 1)).to(device)\n","        \n","        # Categorical DQN algorithm\n","        delta_z = float(self.v_max - self.v_min) / (self.atom_size - 1)\n","\n","        with torch.no_grad():\n","            # Double DQN\n","            next_action = self.dqn(next_state).argmax(1)\n","            next_dist = self.dqn_target.dist(next_state)\n","            next_dist = next_dist[range(self.batch_size), next_action]\n","\n","            t_z = reward + (1 - done) * gamma * self.support\n","            t_z = t_z.clamp(min=self.v_min, max=self.v_max)\n","            b = (t_z - self.v_min) / delta_z\n","            l = b.floor().long()\n","            u = b.ceil().long()\n","\n","            offset = (\n","                torch.linspace(\n","                    0, (self.batch_size - 1) * self.atom_size, self.batch_size\n","                ).long()\n","                .unsqueeze(1)\n","                .expand(self.batch_size, self.atom_size)\n","                .to(self.device)\n","            )\n","\n","            proj_dist = torch.zeros(next_dist.size(), device=self.device)\n","            proj_dist.view(-1).index_add_(\n","                0, (l + offset).view(-1), (next_dist * (u.float() - b)).view(-1)\n","            )\n","            proj_dist.view(-1).index_add_(\n","                0, (u + offset).view(-1), (next_dist * (b - l.float())).view(-1)\n","            )\n","\n","        dist = self.dqn.dist(state)\n","        log_p = torch.log(dist[range(self.batch_size), action])\n","        elementwise_loss = -(proj_dist * log_p).sum(1)\n","\n","        return elementwise_loss\n","\n","    def _target_hard_update(self):\n","        \"\"\"Hard update: target <- local.\"\"\"\n","        self.dqn_target.load_state_dict(self.dqn.state_dict())\n","                \n","    def _plot(\n","        self, \n","        frame_idx: int, \n","        scores: List[float], \n","        losses: List[float],\n","    ):\n","        \"\"\"Plot the training progresses.\"\"\"\n","        clear_output(True)\n","        plt.figure(figsize=(20, 5))\n","        plt.subplot(131)\n","        plt.title('frame %s. score: %s' % (frame_idx, np.mean(scores[-10:])))\n","        plt.plot(scores)\n","        plt.subplot(132)\n","        plt.title('loss')\n","        plt.plot(losses)\n","        plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4jneM8b6Zklk","colab_type":"code","colab":{}},"source":["#from HW3_Task2.agent.env import construct_task2_env\n","import gym\n","from gym_grid_driving.envs.grid_driving import LaneSpec,Point\n","\n","def construct_task2_env(i,j):\n","    config = {'observation_type': 'tensor', 'agent_speed_range': [-3, -1], 'width': 50,'agent_pos_init':Point(i,j),\n","              'lanes': [LaneSpec(cars=7, speed_range=[-2, -1]), \n","                        LaneSpec(cars=8, speed_range=[-2, -1]), \n","                        LaneSpec(cars=6, speed_range=[-1, -1]), \n","                        LaneSpec(cars=6, speed_range=[-3, -1]), \n","                        LaneSpec(cars=7, speed_range=[-2, -1]), \n","                        LaneSpec(cars=8, speed_range=[-2, -1]), \n","                        LaneSpec(cars=6, speed_range=[-3, -2]), \n","                        LaneSpec(cars=7, speed_range=[-1, -1]), \n","                        LaneSpec(cars=6, speed_range=[-2, -1]), \n","                        LaneSpec(cars=8, speed_range=[-2, -2])]\n","            }\n","    return gym.make('GridDriving-v0', **config)\n","\n","\n","\n","env = construct_task2_env(5,2)\n","seed = 777\n","\n","def seed_torch(seed):\n","    torch.manual_seed(seed)\n","    if torch.backends.cudnn.enabled:\n","        torch.backends.cudnn.benchmark = False\n","        torch.backends.cudnn.deterministic = True\n","\n","np.random.seed(seed)\n","random.seed(seed)\n","seed_torch(seed)\n","#env.seed(seed)\n","\n","\n","# parameters\n","num_frames = 10000000\n","memory_size = 4000\n","batch_size = 32\n","target_update = 10\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","# train\n","config = [[3,5,0,2],[5,10,0,3],[10,15,0,4],[15,20,0,4],[20,25,0,4],[25,30,0,4],[30,35,0,4],[35,40,0,4],[40,45,0,4],[49,49,4,4]]\n","config=[[49,49,4,4]]\n","#agent2 = DQNAgent(env, memory_size, batch_size, target_update)\n","agent.train(config,num_frames)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yNMx3M-_Z9-V","colab_type":"code","outputId":"ce00f105-aab3-4f50-a031-147f7ac8338e","executionInfo":{"status":"ok","timestamp":1587384216342,"user_tz":-480,"elapsed":704,"user":{"displayName":"N","photoUrl":"","userId":"08619141603155082384"}},"colab":{"base_uri":"https://localhost:8080/","height":107}},"source":["import torch\n","torch.save([agent.dqn,agent.dqn_target,agent.dqn_target],\"HW3_Task2/rainbow_s/rainbow_global_config_{}\".format(0))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type Network. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type NoisyLinear. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"qeUkm9kD3tiy","colab_type":"code","colab":{}},"source":["a = [1,2,3,4,5,6]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jh4EonO6_ydU","colab_type":"code","outputId":"c4f10597-7971-40a8-c1e9-dac71797eb07","executionInfo":{"status":"ok","timestamp":1587391097408,"user_tz":-480,"elapsed":2666,"user":{"displayName":"N","photoUrl":"","userId":"08619141603155082384"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["scores = []\n","for i in range(0,10):\n","  scores.append(agent.test())\n","print(scores)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[0, 0, 0, 10, 0, 0, 0, 0, 10, 0]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zUjeU4wS_0d_","colab_type":"code","outputId":"3b28ebfb-81ac-4957-ce6c-602445bdcb1c","executionInfo":{"status":"ok","timestamp":1587410556365,"user_tz":-480,"elapsed":1062,"user":{"displayName":"N","photoUrl":"","userId":"08619141603155082384"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["#!python3 HW3_Task2/agent/models.py --test --path=HW3_Task2/agent/model_last_20_05_actionR_cb_8_16000.pt --savepath=HW3_Task2/agent/model_last_20_05_actionR\n","device = torch.device(\n","            \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","        )\n","print(device)\n","\n","from HW3_Task2.agent import models\n","from HW3_Task2.agent.models import ConvDQN\n","model_class, model_state_dict, input_shape, num_actions = torch.load(\"HW3_Task2/agent/model_last_20_05_actionR_cb_8_16000.pt\")\n","\n","model = eval(model_class)(input_shape, num_actions).to(device)\n","model.load_state_dict(model_state_dict) \n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["cuda\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{"tags":[]},"execution_count":128}]},{"cell_type":"code","metadata":{"id":"BwIvGzgI7h-G","colab_type":"code","outputId":"e76e30d8-6faf-4cd2-9f50-a84845129bb0","executionInfo":{"status":"ok","timestamp":1587409897345,"user_tz":-480,"elapsed":12666,"user":{"displayName":"N","photoUrl":"","userId":"08619141603155082384"}},"colab":{"base_uri":"https://localhost:8080/","height":105}},"source":["import torch\n","if True:\n","    import sys\n","    import time\n","    from HW3_Task2.agent.env import construct_task2_env\n","\n","    FAST_DOWNWARD_PATH = \"/fast_downward/\"\n","\n","    def test(agent, env, runs=1000, t_max=100):\n","        rewards = []\n","        for run in range(runs):\n","            state = env.reset()\n","            agent_init = {'fast_downward_path': FAST_DOWNWARD_PATH, 'agent_speed_range': (-3,-1), 'gamma' : 1}\n","            #agent.initialize(**agent_init)\n","            episode_rewards = 0.0\n","            for t in range(t_max):\n","                action = agent.act(state)\n","                next_state, reward, done, info = env.step(int(action))\n","                full_state = {\n","                    'state': state, 'action': action, 'reward': reward, 'next_state': next_state, \n","                    'done': done, 'info': info\n","                }\n","                #agent.update(**full_state)\n","                state = next_state\n","                episode_rewards += reward\n","                if done:\n","                    break\n","            rewards.append(episode_rewards)\n","        avg_rewards = sum(rewards)/len(rewards)\n","        print(\"{} run(s) avg rewards : {:.1f}\".format(runs, avg_rewards))\n","        return avg_rewards\n","\n","    def timed_test(task):\n","        start_time = time.time()\n","        rewards = []\n","        for tc in task['testcases']:\n","            agent = model\n","            print(\"[{}]\".format(tc['id']), end=' ')\n","            avg_rewards = test(model, tc['env'], tc['runs'], tc['t_max'])\n","            rewards.append(avg_rewards)\n","        point = sum(rewards)/len(rewards)\n","        elapsed_time = time.time() - start_time\n","\n","        print('Point:', point)\n","\n","        for t, remarks in [(0.4, 'fast'), (0.6, 'safe'), (0.8, 'dangerous'), (1.0, 'time limit exceeded')]:\n","            if elapsed_time < task['time_limit'] * t:\n","                print(\"Local runtime: {} seconds --- {}\".format(elapsed_time, remarks))\n","                print(\"WARNING: do note that this might not reflect the runtime on the server.\")\n","                break\n","\n","    def get_task():\n","        tcs = [('task_2_tmax50', 600), ('task_2_tmax40', 40)]\n","        return {\n","            'time_limit': 600,\n","            'testcases': [{ 'id': tc, 'env': construct_task2_env(), 'runs': 10, 't_max': t_max } for tc, t_max in tcs]\n","        }\n","\n","    task = get_task()\n","    timed_test(task)    "],"execution_count":0,"outputs":[{"output_type":"stream","text":["[task_2_tmax50] 10 run(s) avg rewards : 0.0\n","[task_2_tmax40] 10 run(s) avg rewards : 0.0\n","Point: 0.0\n","Local runtime: 11.745181798934937 seconds --- fast\n","WARNING: do note that this might not reflect the runtime on the server.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IXvdLNtA-ZN4","colab_type":"code","outputId":"cca6af28-fba3-455a-f4f4-026b4d9a849d","executionInfo":{"status":"ok","timestamp":1587410042062,"user_tz":-480,"elapsed":30814,"user":{"displayName":"N","photoUrl":"","userId":"08619141603155082384"}},"colab":{"base_uri":"https://localhost:8080/","height":105}},"source":["\n","\n","def mainTest(agent,runs=25):\n","\n","    import sys\n","    import time\n","    from HW3_Task2.agent.env import construct_task2_env\n","\n","    FAST_DOWNWARD_PATH = \"/fast_downward/\"\n","\n","    def test(agent, env, runs=1000, t_max=100):\n","        rewards = []\n","        for run in range(runs):\n","            state = env.reset()\n","            agent_init = {'fast_downward_path': FAST_DOWNWARD_PATH, 'agent_speed_range': (-3,-1), 'gamma' : 1}\n","            #agent.initialize(**agent_init)\n","            episode_rewards = 0.0\n","            for t in range(t_max):\n","                action = agent.act(state)   \n","                next_state, reward, done, info = env.step(action)\n","                full_state = {\n","                    'state': state, 'action': action, 'reward': reward, 'next_state': next_state, \n","                    'done': done, 'info': info\n","                }\n","                #agent.update(**full_state)\n","                state = next_state\n","                episode_rewards += reward\n","                if done:\n","                    break\n","            rewards.append(episode_rewards)\n","        avg_rewards = sum(rewards)/len(rewards)\n","        print(\"{} run(s) avg rewards : {:.1f}\".format(runs, avg_rewards))\n","        return avg_rewards\n","\n","    def timed_test(agent,task):\n","        start_time = time.time()\n","        rewards = []\n","        for tc in task['testcases']:\n","            #agent = create_agent(tc['id'])\n","            print(\"[{}]\".format(tc['id']), end=' ')\n","            avg_rewards = test(agent, tc['env'], tc['runs'], tc['t_max'])\n","            rewards.append(avg_rewards)\n","        point = sum(rewards)/len(rewards)\n","        elapsed_time = time.time() - start_time\n","\n","        print('Point:', point)\n","\n","        for t, remarks in [(0.4, 'fast'), (0.6, 'safe'), (0.8, 'dangerous'), (1.0, 'time limit exceeded')]:\n","            if elapsed_time < task['time_limit'] * t:\n","                print(\"Local runtime: {} seconds --- {}\".format(elapsed_time, remarks))\n","                print(\"WARNING: do note that this might not reflect the runtime on the server.\")\n","                break\n","\n","    def get_task(runs=300):\n","        tcs = [('task_2_tmax50', 50), ('task_2_tmax40', 40)]\n","        return {\n","            'time_limit': 600,\n","            'testcases': [{ 'id': tc, 'env': construct_task2_env(), 'runs': runs, 't_max': t_max } for tc, t_max in tcs]\n","        }\n","\n","    task = get_task(runs=runs)\n","    timed_test(agent,task)\n","\n","mainTest(model,25)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[task_2_tmax50] 25 run(s) avg rewards : 0.0\n","[task_2_tmax40] 25 run(s) avg rewards : 0.0\n","Point: 0.0\n","Local runtime: 29.552878379821777 seconds --- fast\n","WARNING: do note that this might not reflect the runtime on the server.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wboCU-yW_JbL","colab_type":"code","outputId":"219006ea-366c-49d5-ab54-b87173b9bdd6","executionInfo":{"status":"ok","timestamp":1587410262972,"user_tz":-480,"elapsed":878,"user":{"displayName":"N","photoUrl":"","userId":"08619141603155082384"}},"colab":{"base_uri":"https://localhost:8080/","height":245}},"source":["print(model)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["ConvDQN(\n","  (features): Sequential(\n","    (0): Conv2d(4, 32, kernel_size=(2, 2), stride=(1, 1))\n","    (1): ReLU()\n","    (2): Conv2d(32, 64, kernel_size=(2, 2), stride=(1, 1))\n","    (3): ReLU()\n","  )\n","  (layers): Sequential(\n","    (0): Linear(in_features=24576, out_features=256, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=256, out_features=5, bias=True)\n","  )\n",")\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YwkxlNEUA_ki","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}