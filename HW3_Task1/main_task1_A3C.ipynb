{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"main_task1_A3C.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"sCXRy7hMe_eN","colab_type":"text"},"source":["## 1. Read Me \n","**Run first cell :** install dependencies \n","\n","**Run second cell :** install files and local environment on the VM, choose which task you want to run by setting task_1 or task_2 value to true\n","\n","**Run third cell :** try diferent init of the agent. When one is found -> the agent is trained  "]},{"cell_type":"code","metadata":{"id":"SfaeFp3u_6Yh","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"b515ff26-9508-4bd9-8f6f-850602291736"},"source":["from HW3_Task2.agent import models \n","from HW3_Task2.agent import agent \n","from HW3_Task2.agent import env as env_builder\n","import importlib \n","importlib.reload(models)\n","importlib.reload(agent)\n","importlib.reload(env_builder)\n","try:\n","  importlib.reload(models)\n","  importlib.reload(agent)\n","  importlib.reload(env)\n","except:pass\n","Config = collections.namedtuple('Config', ('x_debut', 'y_debut', 'max_episode', 'max_epsilon', 'epsilon_decay', 'test_interval','save_interval','batch_size','buffer_limit','methode','gamma_nstep','nstep'))\n","config_l=[]\n","last_y = 2\n","for i in range(0,9):\n","  x_debut = 10+i*5\n","  \n","\n","  for j in range(0,last_y):\n","    time = i + j \n","    \n","    y_debut = j\n","    max_episode = 250*(i+1)+250*(j)\n","\n","    # max_epsilon decrease with time \n","    max_epsilon = 1.0 / (time+1)\n","    epsilon_decay = max_episode // 8\n","    test_interval = min(500,max_episode//2)\n","    save_interval = 1000\n","    batch_size = 64\n","    buffer_limit = min(10000,2000*(time+1))\n","    methode = 'Mixed Monte Carlo + DQN'\n","    gamma_nstep  = 0.5\n","    nstep = 3\n","    config_cur = Config(x_debut,y_debut,max_episode,max_epsilon,epsilon_decay,test_interval,save_interval,batch_size,buffer_limit,methode,gamma_nstep,nstep)\n","    \n","    env = env_builder.construct_task2_env_ij(x_debut,y_debut)\n","    \n","    if i == 0 and j == 0:\n","      model = models.train(agent.ConvDQN, env=env,pretrain=False,model_p= None,savepath='HW3_Task2/saves/m_',config=config_cur)\n","    else:\n","      model = models.train(agent.ConvDQN, env=env,pretrain=True,model_p= model,savepath='HW3_Task2/saves/m_',config=config_cur)\n","\n","\n","  last_y += 1\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["device is: cuda\n","config : \n"," x debut 10, y_debut 0, max ep 250, max epsilon 1.0 \n"," Epsilon_decay 50, test_interval 125\n","Save_interval 1000, batch_size 64, buffer_limit 2000 \n"," Methode Mixed Monte Carlo + DQN, gamma_nstep 0.5, nstep 3\n","Start learning from : x_debut 10 y_debut 0\n","[Episode 50]\t rewards globals : 3.922 \tavg rewards : 3.922,\tavg loss: : 8.929349,\tbuffer size : 321,\tepsilon : 43.1%, \t r <=40 0.0, \t r > 40 39.21568627450981\n","[Episode 100]\t rewards globals : 4.554 \tavg rewards : 5.294,\tavg loss: : 0.585497,\tbuffer size : 547,\tepsilon : 22.2%, \t r <=40 0.0, \t r > 40 52.94117647058824\n","[task_2_tmax50] 100 run(s) avg rewards : 5.9\n","[task_2_tmax40] 100 run(s) avg rewards : 5.9\n","Point: 5.9\n","[Episode 150]\t rewards globals : 5.430 \tavg rewards : 7.059,\tavg loss: : 0.381447,\tbuffer size : 785,\tepsilon : 14.5%, \t r <=40 0.0, \t r > 40 70.58823529411765\n","[Episode 200]\t rewards globals : 5.871 \tavg rewards : 7.255,\tavg loss: : 0.287427,\tbuffer size : 1004,\tepsilon : 11.6%, \t r <=40 0.0, \t r > 40 72.54901960784314\n","error while saving\n","device is: cuda\n","config : \n"," x debut 10, y_debut 1, max ep 500, max epsilon 0.5 \n"," Epsilon_decay 100, test_interval 250\n","Save_interval 1000, batch_size 64, buffer_limit 4000 \n"," Methode Mixed Monte Carlo + DQN, gamma_nstep 0.5, nstep 3\n","Start learning from : x_debut 10 y_debut 1\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n","  out=out, **kwargs)\n","/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n","  ret = ret.dtype.type(ret / rcount)\n"],"name":"stderr"},{"output_type":"stream","text":["[Episode 50]\t rewards globals : 1.765 \tavg rewards : 1.765,\tavg loss: : nan,\tbuffer size : 193,\tepsilon : 34.3%, \t r <=40 0.0, \t r > 40 17.647058823529413\n","[Episode 100]\t rewards globals : 2.277 \tavg rewards : 2.745,\tavg loss: : 2.869111,\tbuffer size : 362,\tepsilon : 24.7%, \t r <=40 0.0, \t r > 40 27.450980392156865\n","[Episode 150]\t rewards globals : 2.517 \tavg rewards : 3.137,\tavg loss: : 0.535175,\tbuffer size : 671,\tepsilon : 18.9%, \t r <=40 0.0, \t r > 40 31.372549019607842\n","[Episode 200]\t rewards globals : 2.488 \tavg rewards : 2.549,\tavg loss: : 0.341157,\tbuffer size : 975,\tepsilon : 15.4%, \t r <=40 0.0, \t r > 40 25.49019607843137\n","[task_2_tmax50] 100 run(s) avg rewards : 5.4\n","[task_2_tmax40] 100 run(s) avg rewards : 5.7\n","Point: 5.550000000000001\n","[Episode 250]\t rewards globals : 2.629 \tavg rewards : 3.333,\tavg loss: : 0.278715,\tbuffer size : 1264,\tepsilon : 13.3%, \t r <=40 0.0, \t r > 40 33.33333333333333\n","[Episode 300]\t rewards globals : 2.890 \tavg rewards : 4.314,\tavg loss: : 0.233028,\tbuffer size : 1540,\tepsilon : 12.0%, \t r <=40 0.0, \t r > 40 43.13725490196079\n","[Episode 350]\t rewards globals : 2.934 \tavg rewards : 3.333,\tavg loss: : 0.208997,\tbuffer size : 1844,\tepsilon : 11.2%, \t r <=40 0.0, \t r > 40 33.33333333333333\n","[Episode 400]\t rewards globals : 3.217 \tavg rewards : 5.294,\tavg loss: : 0.190245,\tbuffer size : 2117,\tepsilon : 10.7%, \t r <=40 0.0, \t r > 40 52.94117647058824\n","[Episode 450]\t rewards globals : 3.415 \tavg rewards : 5.098,\tavg loss: : 0.183175,\tbuffer size : 2395,\tepsilon : 10.4%, \t r <=40 0.0, \t r > 40 50.98039215686274\n","error while saving\n","device is: cuda\n","config : \n"," x debut 15, y_debut 0, max ep 500, max epsilon 0.5 \n"," Epsilon_decay 100, test_interval 250\n","Save_interval 1000, batch_size 64, buffer_limit 4000 \n"," Methode Mixed Monte Carlo + DQN, gamma_nstep 0.5, nstep 3\n","Start learning from : x_debut 15 y_debut 0\n","[Episode 50]\t rewards globals : 4.118 \tavg rewards : 4.118,\tavg loss: : 2.453754,\tbuffer size : 334,\tepsilon : 34.3%, \t r <=40 0.0, \t r > 40 41.17647058823529\n","[Episode 100]\t rewards globals : 5.050 \tavg rewards : 6.078,\tavg loss: : 0.192483,\tbuffer size : 701,\tepsilon : 24.7%, \t r <=40 0.0, \t r > 40 60.78431372549019\n","[Episode 150]\t rewards globals : 5.298 \tavg rewards : 5.882,\tavg loss: : 0.127369,\tbuffer size : 1035,\tepsilon : 18.9%, \t r <=40 0.0, \t r > 40 58.82352941176471\n","[Episode 200]\t rewards globals : 5.821 \tavg rewards : 7.255,\tavg loss: : 0.104110,\tbuffer size : 1414,\tepsilon : 15.4%, \t r <=40 0.0, \t r > 40 72.54901960784314\n","[task_2_tmax50] 100 run(s) avg rewards : 8.5\n","[task_2_tmax40] 100 run(s) avg rewards : 7.8\n","Point: 8.15\n","[Episode 250]\t rewards globals : 6.135 \tavg rewards : 7.255,\tavg loss: : 0.095632,\tbuffer size : 1756,\tepsilon : 13.3%, \t r <=40 0.0, \t r > 40 72.54901960784314\n","[Episode 300]\t rewards globals : 6.412 \tavg rewards : 7.647,\tavg loss: : 0.087027,\tbuffer size : 2108,\tepsilon : 12.0%, \t r <=40 0.0, \t r > 40 76.47058823529412\n","[Episode 350]\t rewards globals : 6.581 \tavg rewards : 7.647,\tavg loss: : 0.083904,\tbuffer size : 2498,\tepsilon : 11.2%, \t r <=40 0.0, \t r > 40 76.47058823529412\n","[Episode 400]\t rewards globals : 6.783 \tavg rewards : 8.235,\tavg loss: : 0.076274,\tbuffer size : 2884,\tepsilon : 10.7%, \t r <=40 0.0, \t r > 40 82.35294117647058\n","[Episode 450]\t rewards globals : 6.984 \tavg rewards : 8.627,\tavg loss: : 0.076828,\tbuffer size : 3278,\tepsilon : 10.4%, \t r <=40 0.0, \t r > 40 86.27450980392157\n","error while saving\n","device is: cuda\n","config : \n"," x debut 15, y_debut 1, max ep 750, max epsilon 0.3333333333333333 \n"," Epsilon_decay 150, test_interval 375\n","Save_interval 1000, batch_size 64, buffer_limit 6000 \n"," Methode Mixed Monte Carlo + DQN, gamma_nstep 0.5, nstep 3\n","Start learning from : x_debut 15 y_debut 1\n","[Episode 50]\t rewards globals : 5.294 \tavg rewards : 5.294,\tavg loss: : 0.791152,\tbuffer size : 378,\tepsilon : 26.7%, \t r <=40 0.0, \t r > 40 52.94117647058824\n","[Episode 100]\t rewards globals : 5.842 \tavg rewards : 6.471,\tavg loss: : 0.166619,\tbuffer size : 750,\tepsilon : 22.0%, \t r <=40 0.0, \t r > 40 64.70588235294117\n","[Episode 150]\t rewards globals : 5.563 \tavg rewards : 5.098,\tavg loss: : 0.128376,\tbuffer size : 1096,\tepsilon : 18.6%, \t r <=40 0.0, \t r > 40 50.98039215686274\n","[Episode 200]\t rewards globals : 5.771 \tavg rewards : 6.471,\tavg loss: : 0.102739,\tbuffer size : 1477,\tepsilon : 16.2%, \t r <=40 0.0, \t r > 40 64.70588235294117\n","[Episode 250]\t rewards globals : 5.976 \tavg rewards : 6.863,\tavg loss: : 0.098867,\tbuffer size : 1858,\tepsilon : 14.4%, \t r <=40 0.0, \t r > 40 68.62745098039215\n","[Episode 300]\t rewards globals : 6.246 \tavg rewards : 7.451,\tavg loss: : 0.088112,\tbuffer size : 2259,\tepsilon : 13.2%, \t r <=40 0.0, \t r > 40 74.50980392156863\n","[Episode 350]\t rewards globals : 6.439 \tavg rewards : 7.451,\tavg loss: : 0.080315,\tbuffer size : 2630,\tepsilon : 12.3%, \t r <=40 0.0, \t r > 40 74.50980392156863\n","[task_2_tmax50] 100 run(s) avg rewards : 8.3\n","[task_2_tmax40] 100 run(s) avg rewards : 9.3\n","Point: 8.8\n","Re do test: double check\n","[task_2_tmax50] 100 run(s) avg rewards : 0.5\n","[task_2_tmax40] 100 run(s) avg rewards : 0.4\n","Point: 0.45\n","[Episode 400]\t rewards globals : 6.559 \tavg rewards : 7.451,\tavg loss: : 0.074503,\tbuffer size : 3023,\tepsilon : 11.6%, \t r <=40 0.0, \t r > 40 74.50980392156863\n","[Episode 450]\t rewards globals : 6.585 \tavg rewards : 6.863,\tavg loss: : 0.072789,\tbuffer size : 3397,\tepsilon : 11.2%, \t r <=40 0.0, \t r > 40 68.62745098039215\n","[Episode 500]\t rewards globals : 6.587 \tavg rewards : 6.667,\tavg loss: : 0.070734,\tbuffer size : 3757,\tepsilon : 10.8%, \t r <=40 0.0, \t r > 40 66.66666666666666\n","[Episode 550]\t rewards globals : 6.588 \tavg rewards : 6.667,\tavg loss: : 0.069020,\tbuffer size : 4115,\tepsilon : 10.6%, \t r <=40 0.0, \t r > 40 66.66666666666666\n","[Episode 600]\t rewards globals : 6.755 \tavg rewards : 8.627,\tavg loss: : 0.067376,\tbuffer size : 4508,\tepsilon : 10.4%, \t r <=40 0.0, \t r > 40 86.27450980392157\n","[Episode 650]\t rewards globals : 6.882 \tavg rewards : 8.431,\tavg loss: : 0.066726,\tbuffer size : 4891,\tepsilon : 10.3%, \t r <=40 0.0, \t r > 40 84.31372549019608\n","[Episode 700]\t rewards globals : 6.961 \tavg rewards : 7.843,\tavg loss: : 0.065442,\tbuffer size : 5280,\tepsilon : 10.2%, \t r <=40 0.0, \t r > 40 78.43137254901961\n","error while saving\n","device is: cuda\n","config : \n"," x debut 15, y_debut 2, max ep 1000, max epsilon 0.25 \n"," Epsilon_decay 200, test_interval 500\n","Save_interval 1000, batch_size 64, buffer_limit 8000 \n"," Methode Mixed Monte Carlo + DQN, gamma_nstep 0.5, nstep 3\n","Start learning from : x_debut 15 y_debut 2\n","[Episode 50]\t rewards globals : 4.706 \tavg rewards : 4.706,\tavg loss: : 0.824093,\tbuffer size : 375,\tepsilon : 21.7%, \t r <=40 0.0, \t r > 40 47.05882352941176\n","[Episode 100]\t rewards globals : 4.851 \tavg rewards : 4.902,\tavg loss: : 0.135864,\tbuffer size : 763,\tepsilon : 19.1%, \t r <=40 0.0, \t r > 40 49.01960784313725\n","[Episode 150]\t rewards globals : 5.364 \tavg rewards : 6.275,\tavg loss: : 0.088238,\tbuffer size : 1137,\tepsilon : 17.1%, \t r <=40 0.0, \t r > 40 62.745098039215684\n","[Episode 200]\t rewards globals : 5.622 \tavg rewards : 6.471,\tavg loss: : 0.073835,\tbuffer size : 1531,\tepsilon : 15.5%, \t r <=40 0.0, \t r > 40 64.70588235294117\n","[Episode 250]\t rewards globals : 5.976 \tavg rewards : 7.451,\tavg loss: : 0.072050,\tbuffer size : 1931,\tepsilon : 14.3%, \t r <=40 0.0, \t r > 40 74.50980392156863\n","[Episode 300]\t rewards globals : 5.980 \tavg rewards : 6.078,\tavg loss: : 0.067382,\tbuffer size : 2328,\tepsilon : 13.3%, \t r <=40 0.0, \t r > 40 60.78431372549019\n","[Episode 350]\t rewards globals : 6.125 \tavg rewards : 7.059,\tavg loss: : 0.063859,\tbuffer size : 2699,\tepsilon : 12.6%, \t r <=40 0.0, \t r > 40 70.58823529411765\n","[Episode 400]\t rewards globals : 6.185 \tavg rewards : 6.471,\tavg loss: : 0.060523,\tbuffer size : 3068,\tepsilon : 12.0%, \t r <=40 0.0, \t r > 40 64.70588235294117\n","[Episode 450]\t rewards globals : 6.341 \tavg rewards : 7.451,\tavg loss: : 0.061201,\tbuffer size : 3480,\tepsilon : 11.6%, \t r <=40 0.0, \t r > 40 74.50980392156863\n","[task_2_tmax50] 100 run(s) avg rewards : 8.3\n","[task_2_tmax40] 100 run(s) avg rewards : 8.0\n","Point: 8.15\n","[Episode 500]\t rewards globals : 6.387 \tavg rewards : 6.863,\tavg loss: : 0.061046,\tbuffer size : 3885,\tepsilon : 11.2%, \t r <=40 0.0, \t r > 40 68.62745098039215\n","[Episode 550]\t rewards globals : 6.334 \tavg rewards : 5.882,\tavg loss: : 0.060240,\tbuffer size : 4256,\tepsilon : 11.0%, \t r <=40 0.0, \t r > 40 58.82352941176471\n","[Episode 600]\t rewards globals : 6.323 \tavg rewards : 6.275,\tavg loss: : 0.060289,\tbuffer size : 4620,\tepsilon : 10.7%, \t r <=40 0.0, \t r > 40 62.745098039215684\n","[Episode 650]\t rewards globals : 6.359 \tavg rewards : 6.863,\tavg loss: : 0.060733,\tbuffer size : 5028,\tepsilon : 10.6%, \t r <=40 0.0, \t r > 40 68.62745098039215\n","[Episode 700]\t rewards globals : 6.391 \tavg rewards : 6.863,\tavg loss: : 0.059525,\tbuffer size : 5406,\tepsilon : 10.5%, \t r <=40 0.0, \t r > 40 68.62745098039215\n","[Episode 750]\t rewards globals : 6.365 \tavg rewards : 6.078,\tavg loss: : 0.059445,\tbuffer size : 5776,\tepsilon : 10.4%, \t r <=40 0.0, \t r > 40 60.78431372549019\n","[Episode 800]\t rewards globals : 6.417 \tavg rewards : 7.059,\tavg loss: : 0.059155,\tbuffer size : 6155,\tepsilon : 10.3%, \t r <=40 0.0, \t r > 40 70.58823529411765\n","[Episode 850]\t rewards globals : 6.463 \tavg rewards : 7.255,\tavg loss: : 0.059259,\tbuffer size : 6519,\tepsilon : 10.2%, \t r <=40 0.0, \t r > 40 72.54901960784314\n","[Episode 900]\t rewards globals : 6.493 \tavg rewards : 6.863,\tavg loss: : 0.058189,\tbuffer size : 6891,\tepsilon : 10.2%, \t r <=40 0.0, \t r > 40 68.62745098039215\n","[Episode 950]\t rewards globals : 6.509 \tavg rewards : 6.863,\tavg loss: : 0.058354,\tbuffer size : 7311,\tepsilon : 10.1%, \t r <=40 0.0, \t r > 40 68.62745098039215\n","error while saving\n","device is: cuda\n","config : \n"," x debut 20, y_debut 0, max ep 750, max epsilon 0.3333333333333333 \n"," Epsilon_decay 150, test_interval 375\n","Save_interval 1000, batch_size 64, buffer_limit 6000 \n"," Methode Mixed Monte Carlo + DQN, gamma_nstep 0.5, nstep 3\n","Start learning from : x_debut 20 y_debut 0\n","[Episode 50]\t rewards globals : 4.902 \tavg rewards : 4.902,\tavg loss: : 1.702820,\tbuffer size : 346,\tepsilon : 26.7%, \t r <=40 0.0, \t r > 40 49.01960784313725\n","[Episode 100]\t rewards globals : 4.950 \tavg rewards : 5.098,\tavg loss: : 0.233108,\tbuffer size : 767,\tepsilon : 22.0%, \t r <=40 0.0, \t r > 40 50.98039215686274\n","[Episode 150]\t rewards globals : 5.232 \tavg rewards : 5.882,\tavg loss: : 0.150171,\tbuffer size : 1210,\tepsilon : 18.6%, \t r <=40 0.0, \t r > 40 58.82352941176471\n","[Episode 200]\t rewards globals : 5.771 \tavg rewards : 7.255,\tavg loss: : 0.116237,\tbuffer size : 1706,\tepsilon : 16.2%, \t r <=40 0.0, \t r > 40 72.54901960784314\n","[Episode 250]\t rewards globals : 5.896 \tavg rewards : 6.471,\tavg loss: : 0.101683,\tbuffer size : 2171,\tepsilon : 14.4%, \t r <=40 0.0, \t r > 40 64.70588235294117\n","[Episode 300]\t rewards globals : 6.213 \tavg rewards : 7.647,\tavg loss: : 0.090100,\tbuffer size : 2667,\tepsilon : 13.2%, \t r <=40 0.0, \t r > 40 76.47058823529412\n","[Episode 350]\t rewards globals : 6.382 \tavg rewards : 7.451,\tavg loss: : 0.081093,\tbuffer size : 3120,\tepsilon : 12.3%, \t r <=40 0.0, \t r > 40 74.50980392156863\n","[task_2_tmax50] 100 run(s) avg rewards : 9.5\n","[task_2_tmax40] 100 run(s) avg rewards : 9.9\n","Point: 9.7\n","Re do test: double check\n","[task_2_tmax50] 100 run(s) avg rewards : 0.5\n","[task_2_tmax40] 100 run(s) avg rewards : 0.7\n","Point: 0.6\n","[Episode 400]\t rewards globals : 6.434 \tavg rewards : 6.863,\tavg loss: : 0.075574,\tbuffer size : 3624,\tepsilon : 11.6%, \t r <=40 0.0, \t r > 40 68.62745098039215\n","[Episode 450]\t rewards globals : 6.563 \tavg rewards : 7.451,\tavg loss: : 0.073784,\tbuffer size : 4132,\tepsilon : 11.2%, \t r <=40 0.0, \t r > 40 74.50980392156863\n","[Episode 500]\t rewards globals : 6.786 \tavg rewards : 8.824,\tavg loss: : 0.068770,\tbuffer size : 4630,\tepsilon : 10.8%, \t r <=40 0.0, \t r > 40 88.23529411764706\n","[Episode 550]\t rewards globals : 6.860 \tavg rewards : 7.647,\tavg loss: : 0.065379,\tbuffer size : 5126,\tepsilon : 10.6%, \t r <=40 0.0, \t r > 40 76.47058823529412\n","[Episode 600]\t rewards globals : 7.055 \tavg rewards : 9.216,\tavg loss: : 0.062087,\tbuffer size : 5677,\tepsilon : 10.4%, \t r <=40 0.0, \t r > 40 92.15686274509804\n","[Episode 650]\t rewards globals : 7.097 \tavg rewards : 7.647,\tavg loss: : 0.062562,\tbuffer size : 6000,\tepsilon : 10.3%, \t r <=40 0.0, \t r > 40 76.47058823529412\n","[Episode 700]\t rewards globals : 7.204 \tavg rewards : 8.431,\tavg loss: : 0.060392,\tbuffer size : 6000,\tepsilon : 10.2%, \t r <=40 0.0, \t r > 40 84.31372549019608\n","error while saving\n","device is: cuda\n","config : \n"," x debut 20, y_debut 1, max ep 1000, max epsilon 0.25 \n"," Epsilon_decay 200, test_interval 500\n","Save_interval 1000, batch_size 64, buffer_limit 8000 \n"," Methode Mixed Monte Carlo + DQN, gamma_nstep 0.5, nstep 3\n","Start learning from : x_debut 20 y_debut 1\n","[Episode 50]\t rewards globals : 5.490 \tavg rewards : 5.490,\tavg loss: : 0.330979,\tbuffer size : 433,\tepsilon : 21.7%, \t r <=40 0.0, \t r > 40 54.90196078431373\n","[Episode 100]\t rewards globals : 5.644 \tavg rewards : 5.882,\tavg loss: : 0.107322,\tbuffer size : 875,\tepsilon : 19.1%, \t r <=40 0.0, \t r > 40 58.82352941176471\n","[Episode 150]\t rewards globals : 5.762 \tavg rewards : 5.882,\tavg loss: : 0.088899,\tbuffer size : 1324,\tepsilon : 17.1%, \t r <=40 0.0, \t r > 40 58.82352941176471\n","[Episode 200]\t rewards globals : 6.020 \tavg rewards : 6.863,\tavg loss: : 0.074949,\tbuffer size : 1814,\tepsilon : 15.5%, \t r <=40 0.0, \t r > 40 68.62745098039215\n","[Episode 250]\t rewards globals : 6.255 \tavg rewards : 7.255,\tavg loss: : 0.071729,\tbuffer size : 2292,\tepsilon : 14.3%, \t r <=40 0.0, \t r > 40 72.54901960784314\n","[Episode 300]\t rewards globals : 6.445 \tavg rewards : 7.255,\tavg loss: : 0.066484,\tbuffer size : 2769,\tepsilon : 13.3%, \t r <=40 0.0, \t r > 40 72.54901960784314\n","[Episode 350]\t rewards globals : 6.524 \tavg rewards : 6.863,\tavg loss: : 0.062249,\tbuffer size : 3200,\tepsilon : 12.6%, \t r <=40 0.0, \t r > 40 68.62745098039215\n","[Episode 400]\t rewards globals : 6.733 \tavg rewards : 8.039,\tavg loss: : 0.058464,\tbuffer size : 3686,\tepsilon : 12.0%, \t r <=40 0.0, \t r > 40 80.3921568627451\n","[Episode 450]\t rewards globals : 6.807 \tavg rewards : 7.255,\tavg loss: : 0.058326,\tbuffer size : 4130,\tepsilon : 11.6%, \t r <=40 0.0, \t r > 40 72.54901960784314\n","[task_2_tmax50] 100 run(s) avg rewards : 8.8\n","[task_2_tmax40] 100 run(s) avg rewards : 8.9\n","Point: 8.850000000000001\n","Re do test: double check\n","[task_2_tmax50] 100 run(s) avg rewards : 0.4\n","[task_2_tmax40] 100 run(s) avg rewards : 0.2\n","Point: 0.30000000000000004\n","[Episode 500]\t rewards globals : 6.766 \tavg rewards : 6.471,\tavg loss: : 0.056229,\tbuffer size : 4543,\tepsilon : 11.2%, \t r <=40 0.0, \t r > 40 64.70588235294117\n","[Episode 550]\t rewards globals : 6.751 \tavg rewards : 6.667,\tavg loss: : 0.055841,\tbuffer size : 5016,\tepsilon : 11.0%, \t r <=40 0.0, \t r > 40 66.66666666666666\n","[Episode 600]\t rewards globals : 6.739 \tavg rewards : 6.667,\tavg loss: : 0.055546,\tbuffer size : 5452,\tepsilon : 10.7%, \t r <=40 0.0, \t r > 40 66.66666666666666\n","[Episode 650]\t rewards globals : 6.790 \tavg rewards : 7.451,\tavg loss: : 0.056215,\tbuffer size : 5932,\tepsilon : 10.6%, \t r <=40 0.0, \t r > 40 74.50980392156863\n","[Episode 700]\t rewards globals : 6.819 \tavg rewards : 7.255,\tavg loss: : 0.055521,\tbuffer size : 6409,\tepsilon : 10.5%, \t r <=40 0.0, \t r > 40 72.54901960784314\n","[Episode 750]\t rewards globals : 6.924 \tavg rewards : 8.235,\tavg loss: : 0.054528,\tbuffer size : 6893,\tepsilon : 10.4%, \t r <=40 0.0, \t r > 40 82.35294117647058\n","[Episode 800]\t rewards globals : 7.004 \tavg rewards : 8.235,\tavg loss: : 0.052852,\tbuffer size : 7359,\tepsilon : 10.3%, \t r <=40 0.0, \t r > 40 82.35294117647058\n","[Episode 850]\t rewards globals : 7.027 \tavg rewards : 7.451,\tavg loss: : 0.053353,\tbuffer size : 7831,\tepsilon : 10.2%, \t r <=40 0.0, \t r > 40 74.50980392156863\n","[Episode 900]\t rewards globals : 7.070 \tavg rewards : 7.843,\tavg loss: : 0.052539,\tbuffer size : 8000,\tepsilon : 10.2%, \t r <=40 0.0, \t r > 40 78.43137254901961\n","[Episode 950]\t rewards globals : 7.066 \tavg rewards : 7.059,\tavg loss: : 0.052101,\tbuffer size : 8000,\tepsilon : 10.1%, \t r <=40 0.0, \t r > 40 70.58823529411765\n","error while saving\n","device is: cuda\n","config : \n"," x debut 20, y_debut 2, max ep 1250, max epsilon 0.2 \n"," Epsilon_decay 250, test_interval 500\n","Save_interval 1000, batch_size 64, buffer_limit 10000 \n"," Methode Mixed Monte Carlo + DQN, gamma_nstep 0.5, nstep 3\n","Start learning from : x_debut 20 y_debut 2\n","[Episode 50]\t rewards globals : 4.510 \tavg rewards : 4.510,\tavg loss: : 0.260586,\tbuffer size : 412,\tepsilon : 18.2%, \t r <=40 0.0, \t r > 40 45.09803921568628\n","[Episode 100]\t rewards globals : 4.851 \tavg rewards : 5.098,\tavg loss: : 0.094032,\tbuffer size : 804,\tepsilon : 16.7%, \t r <=40 0.0, \t r > 40 50.98039215686274\n","[Episode 150]\t rewards globals : 5.430 \tavg rewards : 6.471,\tavg loss: : 0.074565,\tbuffer size : 1286,\tepsilon : 15.5%, \t r <=40 0.0, \t r > 40 64.70588235294117\n","[Episode 200]\t rewards globals : 5.274 \tavg rewards : 4.706,\tavg loss: : 0.065326,\tbuffer size : 1743,\tepsilon : 14.5%, \t r <=40 0.0, \t r > 40 47.05882352941176\n","[Episode 250]\t rewards globals : 5.777 \tavg rewards : 7.843,\tavg loss: : 0.062918,\tbuffer size : 2229,\tepsilon : 13.7%, \t r <=40 0.0, \t r > 40 78.43137254901961\n","[Episode 300]\t rewards globals : 5.781 \tavg rewards : 5.882,\tavg loss: : 0.058906,\tbuffer size : 2685,\tepsilon : 13.0%, \t r <=40 0.0, \t r > 40 58.82352941176471\n","[Episode 350]\t rewards globals : 5.755 \tavg rewards : 5.490,\tavg loss: : 0.057279,\tbuffer size : 3119,\tepsilon : 12.5%, \t r <=40 0.0, \t r > 40 54.90196078431373\n","[Episode 400]\t rewards globals : 5.786 \tavg rewards : 5.882,\tavg loss: : 0.057044,\tbuffer size : 3561,\tepsilon : 12.0%, \t r <=40 0.0, \t r > 40 58.82352941176471\n","[Episode 450]\t rewards globals : 5.876 \tavg rewards : 6.471,\tavg loss: : 0.058925,\tbuffer size : 4035,\tepsilon : 11.7%, \t r <=40 0.0, \t r > 40 64.70588235294117\n","[task_2_tmax50] 100 run(s) avg rewards : 8.1\n","[task_2_tmax40] 100 run(s) avg rewards : 8.5\n","Point: 8.3\n","[Episode 500]\t rewards globals : 5.888 \tavg rewards : 5.882,\tavg loss: : 0.057302,\tbuffer size : 4488,\tepsilon : 11.4%, \t r <=40 0.0, \t r > 40 58.82352941176471\n","[Episode 550]\t rewards globals : 5.862 \tavg rewards : 5.490,\tavg loss: : 0.056829,\tbuffer size : 4958,\tepsilon : 11.1%, \t r <=40 0.0, \t r > 40 54.90196078431373\n","[Episode 600]\t rewards globals : 6.023 \tavg rewards : 7.843,\tavg loss: : 0.055480,\tbuffer size : 5449,\tepsilon : 10.9%, \t r <=40 0.0, \t r > 40 78.43137254901961\n","[Episode 650]\t rewards globals : 5.975 \tavg rewards : 5.490,\tavg loss: : 0.056142,\tbuffer size : 5869,\tepsilon : 10.7%, \t r <=40 0.0, \t r > 40 54.90196078431373\n","[Episode 700]\t rewards globals : 6.049 \tavg rewards : 7.059,\tavg loss: : 0.055393,\tbuffer size : 6376,\tepsilon : 10.6%, \t r <=40 0.0, \t r > 40 70.58823529411765\n","[Episode 750]\t rewards globals : 6.005 \tavg rewards : 5.294,\tavg loss: : 0.055452,\tbuffer size : 6823,\tepsilon : 10.5%, \t r <=40 0.0, \t r > 40 52.94117647058824\n","[Episode 800]\t rewards globals : 6.130 \tavg rewards : 8.039,\tavg loss: : 0.054057,\tbuffer size : 7300,\tepsilon : 10.4%, \t r <=40 0.0, \t r > 40 80.3921568627451\n","[Episode 850]\t rewards globals : 6.181 \tavg rewards : 7.059,\tavg loss: : 0.055135,\tbuffer size : 7779,\tepsilon : 10.3%, \t r <=40 0.0, \t r > 40 70.58823529411765\n","[Episode 900]\t rewards globals : 6.238 \tavg rewards : 7.255,\tavg loss: : 0.054599,\tbuffer size : 8275,\tepsilon : 10.3%, \t r <=40 0.0, \t r > 40 72.54901960784314\n","[Episode 950]\t rewards globals : 6.257 \tavg rewards : 6.667,\tavg loss: : 0.053952,\tbuffer size : 8746,\tepsilon : 10.2%, \t r <=40 0.0, \t r > 40 66.66666666666666\n","[task_2_tmax50] 100 run(s) avg rewards : 8.5\n","[task_2_tmax40] 100 run(s) avg rewards : 8.8\n","Point: 8.65\n","Re do test: double check\n","[task_2_tmax50] 100 run(s) avg rewards : 0.5\n","[task_2_tmax40] 100 run(s) avg rewards : 0.3\n","Point: 0.4\n","Error while saving\n","[Episode 1000]\t rewards globals : 6.284 \tavg rewards : 6.667,\tavg loss: : 0.053427,\tbuffer size : 9229,\tepsilon : 10.2%, \t r <=40 0.0, \t r > 40 66.66666666666666\n","[Episode 1050]\t rewards globals : 6.327 \tavg rewards : 7.255,\tavg loss: : 0.053445,\tbuffer size : 9682,\tepsilon : 10.1%, \t r <=40 0.0, \t r > 40 72.54901960784314\n","[Episode 1100]\t rewards globals : 6.331 \tavg rewards : 6.471,\tavg loss: : 0.053262,\tbuffer size : 10000,\tepsilon : 10.1%, \t r <=40 0.0, \t r > 40 64.70588235294117\n","[Episode 1150]\t rewards globals : 6.325 \tavg rewards : 6.275,\tavg loss: : 0.052944,\tbuffer size : 10000,\tepsilon : 10.1%, \t r <=40 0.0, \t r > 40 62.745098039215684\n","[Episode 1200]\t rewards globals : 6.361 \tavg rewards : 7.255,\tavg loss: : 0.052407,\tbuffer size : 10000,\tepsilon : 10.1%, \t r <=40 0.0, \t r > 40 72.54901960784314\n","error while saving\n","device is: cuda\n","config : \n"," x debut 20, y_debut 3, max ep 1500, max epsilon 0.16666666666666666 \n"," Epsilon_decay 300, test_interval 500\n","Save_interval 1000, batch_size 64, buffer_limit 10000 \n"," Methode Mixed Monte Carlo + DQN, gamma_nstep 0.5, nstep 3\n","Start learning from : x_debut 20 y_debut 3\n","[Episode 50]\t rewards globals : 3.137 \tavg rewards : 3.137,\tavg loss: : 0.710308,\tbuffer size : 404,\tepsilon : 15.6%, \t r <=40 0.0, \t r > 40 31.372549019607842\n","[Episode 100]\t rewards globals : 3.069 \tavg rewards : 2.941,\tavg loss: : 0.225728,\tbuffer size : 851,\tepsilon : 14.8%, \t r <=40 0.0, \t r > 40 29.411764705882355\n","[Episode 150]\t rewards globals : 2.781 \tavg rewards : 2.157,\tavg loss: : 0.161200,\tbuffer size : 1249,\tepsilon : 14.0%, \t r <=40 0.0, \t r > 40 21.568627450980394\n","[Episode 200]\t rewards globals : 2.886 \tavg rewards : 3.137,\tavg loss: : 0.128737,\tbuffer size : 1693,\tepsilon : 13.4%, \t r <=40 0.0, \t r > 40 31.372549019607842\n","[Episode 250]\t rewards globals : 3.147 \tavg rewards : 4.118,\tavg loss: : 0.117252,\tbuffer size : 2184,\tepsilon : 12.9%, \t r <=40 0.0, \t r > 40 41.17647058823529\n","[Episode 300]\t rewards globals : 3.156 \tavg rewards : 3.137,\tavg loss: : 0.104925,\tbuffer size : 2674,\tepsilon : 12.5%, \t r <=40 0.0, \t r > 40 31.372549019607842\n","[Episode 350]\t rewards globals : 3.419 \tavg rewards : 4.902,\tavg loss: : 0.095781,\tbuffer size : 3192,\tepsilon : 12.1%, \t r <=40 0.0, \t r > 40 49.01960784313725\n","[Episode 400]\t rewards globals : 3.691 \tavg rewards : 5.490,\tavg loss: : 0.089680,\tbuffer size : 3729,\tepsilon : 11.8%, \t r <=40 0.0, \t r > 40 54.90196078431373\n","[Episode 450]\t rewards globals : 3.880 \tavg rewards : 5.294,\tavg loss: : 0.087536,\tbuffer size : 4251,\tepsilon : 11.5%, \t r <=40 0.0, \t r > 40 52.94117647058824\n","[task_2_tmax50] 100 run(s) avg rewards : 7.0\n","[task_2_tmax40] 100 run(s) avg rewards : 7.1\n","Point: 7.05\n","[Episode 500]\t rewards globals : 4.072 \tavg rewards : 5.882,\tavg loss: : 0.083131,\tbuffer size : 4744,\tepsilon : 11.3%, \t r <=40 0.0, \t r > 40 58.82352941176471\n","[Episode 550]\t rewards globals : 4.211 \tavg rewards : 5.686,\tavg loss: : 0.079622,\tbuffer size : 5240,\tepsilon : 11.1%, \t r <=40 0.0, \t r > 40 56.86274509803921\n","[Episode 600]\t rewards globals : 4.343 \tavg rewards : 5.882,\tavg loss: : 0.076795,\tbuffer size : 5740,\tepsilon : 10.9%, \t r <=40 0.0, \t r > 40 58.82352941176471\n","[Episode 650]\t rewards globals : 4.424 \tavg rewards : 5.490,\tavg loss: : 0.075948,\tbuffer size : 6240,\tepsilon : 10.8%, \t r <=40 0.0, \t r > 40 54.90196078431373\n","[Episode 700]\t rewards globals : 4.422 \tavg rewards : 4.510,\tavg loss: : 0.074576,\tbuffer size : 6741,\tepsilon : 10.6%, \t r <=40 0.0, \t r > 40 45.09803921568628\n","[Episode 750]\t rewards globals : 4.501 \tavg rewards : 5.490,\tavg loss: : 0.073577,\tbuffer size : 7225,\tepsilon : 10.5%, \t r <=40 0.0, \t r > 40 54.90196078431373\n","[Episode 800]\t rewards globals : 4.694 \tavg rewards : 7.451,\tavg loss: : 0.071354,\tbuffer size : 7709,\tepsilon : 10.5%, \t r <=40 0.0, \t r > 40 74.50980392156863\n","[Episode 850]\t rewards globals : 4.759 \tavg rewards : 5.686,\tavg loss: : 0.071773,\tbuffer size : 8190,\tepsilon : 10.4%, \t r <=40 0.0, \t r > 40 56.86274509803921\n","[Episode 900]\t rewards globals : 4.784 \tavg rewards : 5.098,\tavg loss: : 0.070801,\tbuffer size : 8651,\tepsilon : 10.3%, \t r <=40 0.0, \t r > 40 50.98039215686274\n","[Episode 950]\t rewards globals : 4.890 \tavg rewards : 6.667,\tavg loss: : 0.069758,\tbuffer size : 9161,\tepsilon : 10.3%, \t r <=40 0.0, \t r > 40 66.66666666666666\n","[task_2_tmax50] 100 run(s) avg rewards : 7.1\n","[task_2_tmax40] 100 run(s) avg rewards : 7.4\n","Point: 7.25\n","Error while saving\n","[Episode 1000]\t rewards globals : 4.845 \tavg rewards : 4.118,\tavg loss: : 0.068674,\tbuffer size : 9615,\tepsilon : 10.2%, \t r <=40 0.0, \t r > 40 41.17647058823529\n","[Episode 1050]\t rewards globals : 4.862 \tavg rewards : 5.098,\tavg loss: : 0.068779,\tbuffer size : 10000,\tepsilon : 10.2%, \t r <=40 0.0, \t r > 40 50.98039215686274\n","[Episode 1100]\t rewards globals : 4.896 \tavg rewards : 5.490,\tavg loss: : 0.067800,\tbuffer size : 10000,\tepsilon : 10.2%, \t r <=40 0.0, \t r > 40 54.90196078431373\n","[Episode 1150]\t rewards globals : 4.970 \tavg rewards : 6.667,\tavg loss: : 0.067287,\tbuffer size : 10000,\tepsilon : 10.1%, \t r <=40 0.0, \t r > 40 66.66666666666666\n","[Episode 1200]\t rewards globals : 4.979 \tavg rewards : 5.098,\tavg loss: : 0.066819,\tbuffer size : 10000,\tepsilon : 10.1%, \t r <=40 0.0, \t r > 40 50.98039215686274\n","[Episode 1250]\t rewards globals : 5.004 \tavg rewards : 5.686,\tavg loss: : 0.066721,\tbuffer size : 10000,\tepsilon : 10.1%, \t r <=40 0.0, \t r > 40 56.86274509803921\n","[Episode 1300]\t rewards globals : 5.058 \tavg rewards : 6.275,\tavg loss: : 0.065964,\tbuffer size : 10000,\tepsilon : 10.1%, \t r <=40 0.0, \t r > 40 62.745098039215684\n","[Episode 1350]\t rewards globals : 5.078 \tavg rewards : 5.686,\tavg loss: : 0.065313,\tbuffer size : 10000,\tepsilon : 10.1%, \t r <=40 0.0, \t r > 40 56.86274509803921\n","[Episode 1400]\t rewards globals : 5.089 \tavg rewards : 5.294,\tavg loss: : 0.064551,\tbuffer size : 10000,\tepsilon : 10.1%, \t r <=40 0.0, \t r > 40 52.94117647058824\n","[Episode 1450]\t rewards globals : 5.093 \tavg rewards : 5.294,\tavg loss: : 0.064997,\tbuffer size : 10000,\tepsilon : 10.1%, \t r <=40 0.0, \t r > 40 52.94117647058824\n","error while saving\n","device is: cuda\n","config : \n"," x debut 25, y_debut 0, max ep 1000, max epsilon 0.25 \n"," Epsilon_decay 200, test_interval 500\n","Save_interval 1000, batch_size 64, buffer_limit 8000 \n"," Methode Mixed Monte Carlo + DQN, gamma_nstep 0.5, nstep 3\n","Start learning from : x_debut 25 y_debut 0\n","[Episode 50]\t rewards globals : 5.098 \tavg rewards : 5.098,\tavg loss: : 0.312075,\tbuffer size : 460,\tepsilon : 21.7%, \t r <=40 0.0, \t r > 40 50.98039215686274\n","[Episode 100]\t rewards globals : 5.644 \tavg rewards : 6.275,\tavg loss: : 0.132452,\tbuffer size : 1015,\tepsilon : 19.1%, \t r <=40 0.0, \t r > 40 62.745098039215684\n","[Episode 150]\t rewards globals : 5.695 \tavg rewards : 5.882,\tavg loss: : 0.095899,\tbuffer size : 1577,\tepsilon : 17.1%, \t r <=40 0.0, \t r > 40 58.82352941176471\n","[Episode 200]\t rewards globals : 6.020 \tavg rewards : 7.059,\tavg loss: : 0.086572,\tbuffer size : 2180,\tepsilon : 15.5%, \t r <=40 0.0, \t r > 40 70.58823529411765\n","[Episode 250]\t rewards globals : 5.976 \tavg rewards : 5.882,\tavg loss: : 0.079440,\tbuffer size : 2779,\tepsilon : 14.3%, \t r <=40 0.0, \t r > 40 58.82352941176471\n","[Episode 300]\t rewards globals : 6.113 \tavg rewards : 6.863,\tavg loss: : 0.071677,\tbuffer size : 3382,\tepsilon : 13.3%, \t r <=40 0.0, \t r > 40 68.62745098039215\n","[Episode 350]\t rewards globals : 6.211 \tavg rewards : 6.863,\tavg loss: : 0.066839,\tbuffer size : 4019,\tepsilon : 12.6%, \t r <=40 0.0, \t r > 40 68.62745098039215\n","[Episode 400]\t rewards globals : 6.309 \tavg rewards : 7.059,\tavg loss: : 0.063944,\tbuffer size : 4623,\tepsilon : 12.0%, \t r <=40 0.0, \t r > 40 70.58823529411765\n","[Episode 450]\t rewards globals : 6.497 \tavg rewards : 7.843,\tavg loss: : 0.063732,\tbuffer size : 5246,\tepsilon : 11.6%, \t r <=40 0.0, \t r > 40 78.43137254901961\n","[task_2_tmax50] 100 run(s) avg rewards : 8.9\n","[task_2_tmax40] 100 run(s) avg rewards : 9.0\n","Point: 8.95\n","Re do test: double check\n","[task_2_tmax50] 100 run(s) avg rewards : 0.1\n","[task_2_tmax40] 100 run(s) avg rewards : 0.1\n","Point: 0.1\n","[Episode 500]\t rewards globals : 6.567 \tavg rewards : 7.255,\tavg loss: : 0.060708,\tbuffer size : 5868,\tepsilon : 11.2%, \t r <=40 0.0, \t r > 40 72.54901960784314\n","[Episode 550]\t rewards globals : 6.624 \tavg rewards : 7.059,\tavg loss: : 0.057940,\tbuffer size : 6465,\tepsilon : 11.0%, \t r <=40 0.0, \t r > 40 70.58823529411765\n","[Episode 600]\t rewards globals : 6.722 \tavg rewards : 7.843,\tavg loss: : 0.055833,\tbuffer size : 7012,\tepsilon : 10.7%, \t r <=40 0.0, \t r > 40 78.43137254901961\n","[Episode 650]\t rewards globals : 6.805 \tavg rewards : 7.647,\tavg loss: : 0.055901,\tbuffer size : 7664,\tepsilon : 10.6%, \t r <=40 0.0, \t r > 40 76.47058823529412\n","[Episode 700]\t rewards globals : 6.819 \tavg rewards : 7.059,\tavg loss: : 0.055465,\tbuffer size : 8000,\tepsilon : 10.5%, \t r <=40 0.0, \t r > 40 70.58823529411765\n","[Episode 750]\t rewards globals : 6.818 \tavg rewards : 6.667,\tavg loss: : 0.054273,\tbuffer size : 8000,\tepsilon : 10.4%, \t r <=40 0.0, \t r > 40 66.66666666666666\n","[Episode 800]\t rewards globals : 6.841 \tavg rewards : 7.059,\tavg loss: : 0.052954,\tbuffer size : 8000,\tepsilon : 10.3%, \t r <=40 0.0, \t r > 40 70.58823529411765\n","[Episode 850]\t rewards globals : 6.898 \tavg rewards : 7.843,\tavg loss: : 0.052930,\tbuffer size : 8000,\tepsilon : 10.2%, \t r <=40 0.0, \t r > 40 78.43137254901961\n","[Episode 900]\t rewards globals : 6.915 \tavg rewards : 7.255,\tavg loss: : 0.052800,\tbuffer size : 8000,\tepsilon : 10.2%, \t r <=40 0.0, \t r > 40 72.54901960784314\n","[Episode 950]\t rewards globals : 6.951 \tavg rewards : 7.647,\tavg loss: : 0.051624,\tbuffer size : 8000,\tepsilon : 10.1%, \t r <=40 0.0, \t r > 40 76.47058823529412\n","error while saving\n","device is: cuda\n","config : \n"," x debut 25, y_debut 1, max ep 1250, max epsilon 0.2 \n"," Epsilon_decay 250, test_interval 500\n","Save_interval 1000, batch_size 64, buffer_limit 10000 \n"," Methode Mixed Monte Carlo + DQN, gamma_nstep 0.5, nstep 3\n","Start learning from : x_debut 25 y_debut 1\n","[Episode 50]\t rewards globals : 3.922 \tavg rewards : 3.922,\tavg loss: : 0.265790,\tbuffer size : 539,\tepsilon : 18.2%, \t r <=40 0.0, \t r > 40 39.21568627450981\n","[Episode 100]\t rewards globals : 4.554 \tavg rewards : 5.294,\tavg loss: : 0.118909,\tbuffer size : 1087,\tepsilon : 16.7%, \t r <=40 0.0, \t r > 40 52.94117647058824\n","[Episode 150]\t rewards globals : 4.570 \tavg rewards : 4.706,\tavg loss: : 0.093219,\tbuffer size : 1603,\tepsilon : 15.5%, \t r <=40 0.0, \t r > 40 47.05882352941176\n","[Episode 200]\t rewards globals : 5.274 \tavg rewards : 7.255,\tavg loss: : 0.080683,\tbuffer size : 2203,\tepsilon : 14.5%, \t r <=40 0.0, \t r > 40 72.54901960784314\n","[Episode 250]\t rewards globals : 5.418 \tavg rewards : 6.078,\tavg loss: : 0.078531,\tbuffer size : 2768,\tepsilon : 13.7%, \t r <=40 0.0, \t r > 40 60.78431372549019\n","[Episode 300]\t rewards globals : 5.581 \tavg rewards : 6.275,\tavg loss: : 0.071419,\tbuffer size : 3305,\tepsilon : 13.0%, \t r <=40 0.0, \t r > 40 62.745098039215684\n","[Episode 350]\t rewards globals : 5.670 \tavg rewards : 6.078,\tavg loss: : 0.066769,\tbuffer size : 3871,\tepsilon : 12.5%, \t r <=40 0.0, \t r > 40 60.78431372549019\n","[Episode 400]\t rewards globals : 5.786 \tavg rewards : 6.667,\tavg loss: : 0.063192,\tbuffer size : 4446,\tepsilon : 12.0%, \t r <=40 0.0, \t r > 40 66.66666666666666\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8Rbvg4vCI9nE","colab_type":"code","outputId":"f686509e-b254-4230-d10d-2323cdc6a904","executionInfo":{"status":"ok","timestamp":1587495769338,"user_tz":-480,"elapsed":12272,"user":{"displayName":"N","photoUrl":"","userId":"08619141603155082384"}},"colab":{"base_uri":"https://localhost:8080/","height":0}},"source":["# Dependencies\n","\n","!pip install torch numpy git+https://github.com/cs4246/gym-grid-driving.git\n","!pip install -U -q PyDrive"],"execution_count":166,"outputs":[{"output_type":"stream","text":["Collecting git+https://github.com/cs4246/gym-grid-driving.git\n","  Cloning https://github.com/cs4246/gym-grid-driving.git to /tmp/pip-req-build-8wfcm8ic\n","  Running command git clone -q https://github.com/cs4246/gym-grid-driving.git /tmp/pip-req-build-8wfcm8ic\n","Requirement already satisfied (use --upgrade to upgrade): gym-grid-driving==0.0.1 from git+https://github.com/cs4246/gym-grid-driving.git in /usr/local/lib/python3.6/dist-packages\n","Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.18.2)\n","Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (from gym-grid-driving==0.0.1) (0.17.1)\n","Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym->gym-grid-driving==0.0.1) (1.3.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym->gym-grid-driving==0.0.1) (1.4.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym->gym-grid-driving==0.0.1) (1.12.0)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym->gym-grid-driving==0.0.1) (1.5.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym->gym-grid-driving==0.0.1) (0.16.0)\n","Building wheels for collected packages: gym-grid-driving\n","  Building wheel for gym-grid-driving (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gym-grid-driving: filename=gym_grid_driving-0.0.1-cp36-none-any.whl size=8623 sha256=95bc4735b96e0412ec85006dfded498c8ed8489f88ecabd2e6ff5e09ab35deca\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-855szeeo/wheels/e1/30/f2/157c0938ab9bfe9c10c29c9fcab8392f587c9d141f215b67ca\n","Successfully built gym-grid-driving\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AgYsRl_buv89","colab_type":"code","outputId":"0dbd227d-d167-47a0-eae3-1a0e5d0c8806","executionInfo":{"status":"ok","timestamp":1587275018716,"user_tz":-480,"elapsed":6124,"user":{"displayName":"N","photoUrl":"","userId":"08619141603155082384"}},"colab":{"base_uri":"https://localhost:8080/","height":105}},"source":["!git clone https://github.com/pytorch/examples.git"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Cloning into 'examples'...\n","remote: Enumerating objects: 2333, done.\u001b[K\n","remote: Total 2333 (delta 0), reused 0 (delta 0), pack-reused 2333\u001b[K\n","Receiving objects: 100% (2333/2333), 39.02 MiB | 19.27 MiB/s, done.\n","Resolving deltas: 100% (1172/1172), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cjrdeWB-JA1x","colab_type":"code","outputId":"8f37c046-74cf-4585-bc74-9944152e1904","executionInfo":{"status":"ok","timestamp":1587481411009,"user_tz":-480,"elapsed":23669,"user":{"displayName":"N","photoUrl":"","userId":"08619141603155082384"}},"colab":{"base_uri":"https://localhost:8080/","height":437}},"source":["# Authentification / Initialization of workspace \n","\n","import os\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)\n","\n","# Create repo folders \n","\n","task_1 = False\n","# If False task_2 workplace will be downloaded\n","\n","if task_1:\n","  number_task = 1\n","else:\n","  number_task = 2\n","\n","root = '/content'\n","local_download = os.path.join(root,'HW3_Task{}'.format(number_task))\n","\n","if not(os.path.exists(local_download)): \n","  os.mkdir(local_download)\n","\n","local_download_agent = os.path.join(local_download,'agent')\n","\n","if not(os.path.exists(local_download_agent)): \n","  os.mkdir(local_download_agent)\n","\n","  # Download files \n","\n","\n","def download_list(file_list,path_name):\n","    \n","  error_l = [];\n","    \n","  for f in file_list:\n","    # 3. Create & download by id.\n","  \n","    print('file found : title: %s, id: %s' % (f['title'], f['id']))\n","    try:\n","      #print('title: %s, id: %s' % (f['title'], f['id']))\n","      fname = os.path.join(path_name, f['title'])\n","      \n","      # Download only .py files\n","      if fname[-3:] == \".py\":\n","        print('downloading to {}'.format(fname))\n","        f_ = drive.CreateFile({'id': f['id']})\n","        f_.GetContentFile(fname)\n","      \n","    except:\n","      print(\"there is an error\")          \n","      error_l.append(fname)\n","\n","\n","if task_1:\n","  # Initial folder\n","  local_download_path = local_download_agent\n","\n","  # Agent files \n","  file_list = drive.ListFile(\n","        {'q': \"'1ktZR8KIIWEi8Cre92SFomSEchWleHtSu' in parents\"}).GetList()\n","\n","  download_list(file_list,local_download_path)\n","\n","\n","  # Initial files\n","  local_download_path = local_download\n","\n","  file_list = drive.ListFile(\n","        {'q': \"'1x4sJIKHA6NZ5y78AnOJeCZP8abI4gLLO' in parents\"}).GetList()\n","\n","  download_list(file_list,local_download_path)\n","\n","else:\n","\n","  # Initial folder\n","  local_download_path = local_download_agent\n","\n","  # Agent files \n","  file_list = drive.ListFile(\n","        {'q': \"'1PUrHkG6ki4JsiXtKOtvc2vAuR6IJMnCa' in parents\"}).GetList()\n","\n","  download_list(file_list,local_download_path)\n","\n","\n","  # Initial files\n","  local_download_path = local_download\n","\n","  file_list = drive.ListFile(\n","        {'q': \"'17wsroYnRmoTt-OIzDJRrUNvVFmohBVFR' in parents\"}).GetList()\n","\n","  download_list(file_list,local_download_path)\n","\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["file found : title: state_change.py, id: 1NUFRvldgt6dRkjycttkz11dmY56cRIEz\n","downloading to /content/HW3_Task2/agent/state_change.py\n","file found : title: models.py, id: 1BW6dbcbWvmC7JbqE78269K1IXIPwtlEi\n","downloading to /content/HW3_Task2/agent/models.py\n","file found : title: __pycache__, id: 1OfJrkmLCYqUS8vVjJnxgeZCvQEnQSQ4J\n","file found : title: env.py, id: 1bYr6NdzVOXe77n-gUac3VdGA8tnpS8HJ\n","downloading to /content/HW3_Task2/agent/env.py\n","file found : title: __init__.py, id: 1WO1O3RwqCpk7XdN07UUZbF6YTaj5mGGg\n","downloading to /content/HW3_Task2/agent/__init__.py\n","file found : title: model_last_20_05_actionR_cb_.textClipping, id: 1aESmsKWtCbxwgAAqKV9wP1eE4xDOCBFp\n","file found : title: model_last_20_05_actionR_cb_8_5000.pt, id: 1FdLXoWHZQz8l5hS76ygQJXe2HWa-wLJI\n","file found : title: model.pt, id: 1_hr_fysoSC6AXix1Qjqq0bBYk0KiSIcq\n","file found : title: .ipynb_checkpoints, id: 14rimDklDDBO4_OqcqfbiybhzNMxQQFSm\n","file found : title: ZZZ.zip, id: 1CVWs9pxeQ6FieIRMszIIfNK1my32N3Wl\n","file found : title: ZZZ, id: 10kHEkPFSjAINnDqi2s5KF7PxQN9bImk3\n","file found : title: try2 2, id: 1Ka1yYoo2aGsxPITkqKfBYB4AAHln0aMZ\n","file found : title: try2.zip, id: 1o5hCQ_s6UkmcbRNTkR8crcNcftp1IhZ7\n","file found : title: try2 2.zip, id: 1R8lmVqVxgvfuQ150WK4ZckNmfjvWmvW4\n","file found : title: try2, id: 1T1A0dN0enM4yy8DmA6A7HJOFyGQS0IlX\n","file found : title: A0212190W_Task2.zip, id: 1XJ9tRmPvxju3IjD76WkmINyUlYTIJvVm\n","file found : title: agent, id: 1PUrHkG6ki4JsiXtKOtvc2vAuR6IJMnCa\n","file found : title: setup.py, id: 1GrxqesqYqNJit3lNQABi9n3Wisa5zuir\n","downloading to /content/HW3_Task2/setup.py\n","file found : title: MANIFEST.in, id: 16_Iy5dYGYMUP2T9hl-OZdODIhV7dbgCf\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"D--g2rvTwbSs","colab_type":"code","colab":{}},"source":["!python HW3_Task2/rl/actor_critic.py --train --path=HW3_Task2/rl/ac_cb_6_19_13_33_0.pt"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zbI0Tym5iGWW","colab_type":"code","outputId":"1cb40226-a7f7-4e97-c69e-42037b387ee2","executionInfo":{"status":"ok","timestamp":1587460392193,"user_tz":-480,"elapsed":7146086,"user":{"displayName":"N","photoUrl":"","userId":"08619141603155082384"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# TASK 2\n","\n","import sys\n","import time\n","\n","import torch\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n","\n","date_save = time.strftime(\"%d %H %M\")\n","model_path = os.path.join('HW3_Task2','agent', 'model_{}_{}.pt'.format(date_save,2))\n","save_path = os.path.join('HW3_Task2','agent', 'model_{}_{}.pt'.format(date_save,3))\n","\n","duree = 0 \n","nb_bad_init = 0 \n","# Try for 10 diferent inits\n","\n","for i in range(0,1):\n","  debut = time.time()\n","  # Try an init \n","  #!python3 HW3_Task2/agent/models.py --test --path=HW3_Task2/agent/model_last1_20_05_cb_8_1000.pt --savepath=HW3_Task2/agent/model_last1_20_05\n","  #!python3 HW3_Task2/agent/models.py --pretrain --path=HW3_Task2/agent/model_last_20_05_cb_8.pt --savepath=HW3_Task2/agent/model_last_20_05\n","  !python3 HW3_Task2/agent/models.py --train --path=HW3_Task2/agent/model_last_20_05_cb_8.pt --savepath=HW3_Task2/agent/model_last_21_05\n","  #!python3 HW3_Task2/agent/testenv.py\n","  duree = (time.time()-debut)\n","  # If good init exit loop\n","  print(duree)\n","  if duree > 10000:\n","    break\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["cuda\n","pretrain mode\n","model loaded from HW3_Task2/agent/model_last_20_05_cb_8.pt\n","ConvDQN(\n","  (features): Sequential(\n","    (0): Conv2d(4, 32, kernel_size=(2, 2), stride=(1, 1))\n","    (1): ReLU()\n","    (2): Conv2d(32, 64, kernel_size=(2, 2), stride=(1, 1))\n","    (3): ReLU()\n","  )\n","  (layers): Sequential(\n","    (0): Linear(in_features=24576, out_features=256, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=256, out_features=5, bias=True)\n","  )\n",")\n","range change : x_min 49 x_max 49\n","environment change : i_1 49 j_1 9\n","[Episode 100]\t rewards globals : 0.891 \tavg rewards : 0.891,\tavg loss: : 0.329282,\tbuffer size : 2353,\tepsilon : 14.0%, \t r <=40 0.0, \t r > 40 8.91089108910891\n","[Episode 200]\t rewards globals : 1.940 \tavg rewards : 2.970,\tavg loss: : 0.324356,\tbuffer size : 4000,\tepsilon : 5.8%, \t r <=40 0.0, \t r > 40 29.7029702970297\n","[Episode 300]\t rewards globals : 2.558 \tavg rewards : 3.762,\tavg loss: : 0.329656,\tbuffer size : 4000,\tepsilon : 2.8%, \t r <=40 0.0, \t r > 40 37.62376237623762\n","[Episode 400]\t rewards globals : 3.242 \tavg rewards : 5.347,\tavg loss: : 0.324906,\tbuffer size : 4000,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 53.46534653465347\n","environment change : i_1 49 j_1 9\n","Test mode, runs: 100, i: 49, j: 9, final: True\n","[task_2_tmax50] 100 run(s) avg rewards : 6.9\n","[task_2_tmax40] 100 run(s) avg rewards : 0.0\n","Point: 3.45\n","Local runtime: 88.78776741027832 seconds --- fast\n","WARNING: do note that this might not reflect the runtime on the server.\n","[Episode 500]\t rewards globals : 3.832 \tavg rewards : 6.139,\tavg loss: : 0.324121,\tbuffer size : 4000,\tepsilon : 36.2%, \t r <=40 0.0, \t r > 40 61.386138613861384\n","[Episode 600]\t rewards globals : 3.378 \tavg rewards : 1.089,\tavg loss: : 0.324359,\tbuffer size : 4000,\tepsilon : 14.0%, \t r <=40 0.0, \t r > 40 10.891089108910892\n","[Episode 700]\t rewards globals : 3.338 \tavg rewards : 3.069,\tavg loss: : 0.317103,\tbuffer size : 4000,\tepsilon : 5.8%, \t r <=40 0.0, \t r > 40 30.693069306930692\n","[Episode 800]\t rewards globals : 3.446 \tavg rewards : 4.257,\tavg loss: : 0.312053,\tbuffer size : 4000,\tepsilon : 2.8%, \t r <=40 0.0, \t r > 40 42.57425742574257\n","[Episode 900]\t rewards globals : 3.818 \tavg rewards : 6.733,\tavg loss: : 0.306261,\tbuffer size : 4000,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 67.32673267326733\n","environment change : i_1 49 j_1 9\n","Test mode, runs: 100, i: 49, j: 9, final: True\n","[task_2_tmax50] 100 run(s) avg rewards : 7.7\n","[task_2_tmax40] 100 run(s) avg rewards : 0.0\n","Point: 3.85\n","Local runtime: 87.98091053962708 seconds --- fast\n","WARNING: do note that this might not reflect the runtime on the server.\n","[Episode 1000]\t rewards globals : 4.146 \tavg rewards : 7.129,\tavg loss: : 0.300492,\tbuffer size : 4000,\tepsilon : 36.2%, \t r <=40 0.0, \t r > 40 71.28712871287128\n","[Episode 1100]\t rewards globals : 3.878 \tavg rewards : 1.188,\tavg loss: : 0.295383,\tbuffer size : 4000,\tepsilon : 14.0%, \t r <=40 0.0, \t r > 40 11.881188118811881\n","[Episode 1200]\t rewards globals : 3.797 \tavg rewards : 2.871,\tavg loss: : 0.294049,\tbuffer size : 4000,\tepsilon : 5.8%, \t r <=40 0.0, \t r > 40 28.71287128712871\n","[Episode 1300]\t rewards globals : 3.928 \tavg rewards : 5.446,\tavg loss: : 0.294026,\tbuffer size : 4000,\tepsilon : 2.8%, \t r <=40 0.0, \t r > 40 54.45544554455446\n","[Episode 1400]\t rewards globals : 4.083 \tavg rewards : 6.040,\tavg loss: : 0.293010,\tbuffer size : 4000,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 60.396039603960396\n","environment change : i_1 49 j_1 9\n","Test mode, runs: 100, i: 49, j: 9, final: True\n","[task_2_tmax50] 100 run(s) avg rewards : 6.5\n","[task_2_tmax40] 100 run(s) avg rewards : 0.2\n","Point: 3.35\n","Local runtime: 85.66874504089355 seconds --- fast\n","WARNING: do note that this might not reflect the runtime on the server.\n","[Episode 1500]\t rewards globals : 4.184 \tavg rewards : 5.644,\tavg loss: : 0.293054,\tbuffer size : 4000,\tepsilon : 36.2%, \t r <=40 0.0, \t r > 40 56.43564356435643\n","[Episode 1600]\t rewards globals : 4.072 \tavg rewards : 2.376,\tavg loss: : 0.291943,\tbuffer size : 4000,\tepsilon : 14.0%, \t r <=40 0.0, \t r > 40 23.762376237623762\n","[Episode 1700]\t rewards globals : 4.039 \tavg rewards : 3.465,\tavg loss: : 0.289170,\tbuffer size : 4000,\tepsilon : 5.8%, \t r <=40 0.0, \t r > 40 34.65346534653465\n","[Episode 1800]\t rewards globals : 4.098 \tavg rewards : 5.149,\tavg loss: : 0.288775,\tbuffer size : 4000,\tepsilon : 2.8%, \t r <=40 0.0, \t r > 40 51.48514851485149\n","[Episode 1900]\t rewards globals : 4.203 \tavg rewards : 6.139,\tavg loss: : 0.285435,\tbuffer size : 4000,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 61.386138613861384\n","environment change : i_1 49 j_1 9\n","Test mode, runs: 100, i: 49, j: 9, final: True\n","[task_2_tmax50] 100 run(s) avg rewards : 7.2\n","[task_2_tmax40] 100 run(s) avg rewards : 0.2\n","Point: 3.7\n","Local runtime: 87.72738099098206 seconds --- fast\n","WARNING: do note that this might not reflect the runtime on the server.\n","[Episode 2000]\t rewards globals : 4.293 \tavg rewards : 6.040,\tavg loss: : 0.282827,\tbuffer size : 4000,\tepsilon : 36.2%, \t r <=40 0.0, \t r > 40 60.396039603960396\n","[Episode 2100]\t rewards globals : 4.136 \tavg rewards : 0.990,\tavg loss: : 0.281210,\tbuffer size : 4000,\tepsilon : 14.0%, \t r <=40 0.0, \t r > 40 9.900990099009901\n","[Episode 2200]\t rewards globals : 4.094 \tavg rewards : 3.267,\tavg loss: : 0.280792,\tbuffer size : 4000,\tepsilon : 5.8%, \t r <=40 0.0, \t r > 40 32.67326732673268\n","[Episode 2300]\t rewards globals : 4.155 \tavg rewards : 5.545,\tavg loss: : 0.280727,\tbuffer size : 4000,\tepsilon : 2.8%, \t r <=40 0.0, \t r > 40 55.44554455445545\n","[Episode 2400]\t rewards globals : 4.257 \tavg rewards : 6.634,\tavg loss: : 0.281182,\tbuffer size : 4000,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 66.33663366336634\n","environment change : i_1 49 j_1 9\n","Test mode, runs: 100, i: 49, j: 9, final: True\n","[task_2_tmax50] 100 run(s) avg rewards : 6.8\n","[task_2_tmax40] 100 run(s) avg rewards : 0.4\n","Point: 3.6\n","Local runtime: 84.96619939804077 seconds --- fast\n","WARNING: do note that this might not reflect the runtime on the server.\n","[Episode 2500]\t rewards globals : 4.354 \tavg rewards : 6.634,\tavg loss: : 0.280225,\tbuffer size : 4000,\tepsilon : 36.2%, \t r <=40 0.0, \t r > 40 66.33663366336634\n","[Episode 2600]\t rewards globals : 4.252 \tavg rewards : 1.683,\tavg loss: : 0.278948,\tbuffer size : 4000,\tepsilon : 14.0%, \t r <=40 0.0, \t r > 40 16.831683168316832\n","[Episode 2700]\t rewards globals : 4.239 \tavg rewards : 3.861,\tavg loss: : 0.278128,\tbuffer size : 4000,\tepsilon : 5.8%, \t r <=40 0.0, \t r > 40 38.613861386138616\n","[Episode 2800]\t rewards globals : 4.302 \tavg rewards : 6.040,\tavg loss: : 0.277898,\tbuffer size : 4000,\tepsilon : 2.8%, \t r <=40 0.0, \t r > 40 60.396039603960396\n","[Episode 2900]\t rewards globals : 4.354 \tavg rewards : 5.743,\tavg loss: : 0.278218,\tbuffer size : 4000,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 57.42574257425742\n","environment change : i_1 49 j_1 9\n","Test mode, runs: 100, i: 49, j: 9, final: True\n","[task_2_tmax50] 100 run(s) avg rewards : 7.2\n","[task_2_tmax40] 100 run(s) avg rewards : 1.1\n","Point: 4.15\n","Local runtime: 83.96934413909912 seconds --- fast\n","WARNING: do note that this might not reflect the runtime on the server.\n","[Episode 3000]\t rewards globals : 4.409 \tavg rewards : 5.941,\tavg loss: : 0.278409,\tbuffer size : 4000,\tepsilon : 36.2%, \t r <=40 0.0, \t r > 40 59.4059405940594\n","[Episode 3100]\t rewards globals : 4.318 \tavg rewards : 1.584,\tavg loss: : 0.277450,\tbuffer size : 4000,\tepsilon : 14.0%, \t r <=40 0.0, \t r > 40 15.841584158415841\n","[Episode 3200]\t rewards globals : 4.283 \tavg rewards : 3.168,\tavg loss: : 0.276213,\tbuffer size : 4000,\tepsilon : 5.8%, \t r <=40 0.0, \t r > 40 31.683168316831683\n","[Episode 3300]\t rewards globals : 4.268 \tavg rewards : 3.762,\tavg loss: : 0.275639,\tbuffer size : 4000,\tepsilon : 2.8%, \t r <=40 0.0, \t r > 40 37.62376237623762\n","[Episode 3400]\t rewards globals : 4.296 \tavg rewards : 5.149,\tavg loss: : 0.275082,\tbuffer size : 4000,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 51.48514851485149\n","environment change : i_1 49 j_1 9\n","Test mode, runs: 100, i: 49, j: 9, final: True\n","[task_2_tmax50] 100 run(s) avg rewards : 6.6\n","[task_2_tmax40] 100 run(s) avg rewards : 1.5\n","Point: 4.05\n","Local runtime: 82.06571173667908 seconds --- fast\n","WARNING: do note that this might not reflect the runtime on the server.\n","[Episode 3500]\t rewards globals : 4.347 \tavg rewards : 6.040,\tavg loss: : 0.273648,\tbuffer size : 4000,\tepsilon : 36.2%, \t r <=40 0.0, \t r > 40 60.396039603960396\n","[Episode 3600]\t rewards globals : 4.274 \tavg rewards : 1.683,\tavg loss: : 0.272580,\tbuffer size : 4000,\tepsilon : 14.0%, \t r <=40 0.0, \t r > 40 16.831683168316832\n","[Episode 3700]\t rewards globals : 4.229 \tavg rewards : 2.574,\tavg loss: : 0.271710,\tbuffer size : 4000,\tepsilon : 5.8%, \t r <=40 0.0, \t r > 40 25.742574257425744\n","[Episode 3800]\t rewards globals : 4.262 \tavg rewards : 5.446,\tavg loss: : 0.272302,\tbuffer size : 4000,\tepsilon : 2.8%, \t r <=40 0.0, \t r > 40 54.45544554455446\n","[Episode 3900]\t rewards globals : 4.301 \tavg rewards : 5.842,\tavg loss: : 0.273048,\tbuffer size : 4000,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 58.415841584158414\n","environment change : i_1 49 j_1 9\n","Test mode, runs: 100, i: 49, j: 9, final: True\n","[task_2_tmax50] 100 run(s) avg rewards : 6.7\n","[task_2_tmax40] 100 run(s) avg rewards : 0.2\n","Point: 3.45\n","Local runtime: 83.58305191993713 seconds --- fast\n","WARNING: do note that this might not reflect the runtime on the server.\n","[Episode 4000]\t rewards globals : 4.341 \tavg rewards : 5.842,\tavg loss: : 0.273182,\tbuffer size : 4000,\tepsilon : 36.2%, \t r <=40 0.0, \t r > 40 58.415841584158414\n","[Episode 4100]\t rewards globals : 4.277 \tavg rewards : 1.683,\tavg loss: : 0.272366,\tbuffer size : 4000,\tepsilon : 14.0%, \t r <=40 0.0, \t r > 40 16.831683168316832\n","[Episode 4200]\t rewards globals : 4.268 \tavg rewards : 3.960,\tavg loss: : 0.272865,\tbuffer size : 4000,\tepsilon : 5.8%, \t r <=40 0.0, \t r > 40 39.603960396039604\n","[Episode 4300]\t rewards globals : 4.262 \tavg rewards : 4.059,\tavg loss: : 0.274690,\tbuffer size : 4000,\tepsilon : 2.8%, \t r <=40 0.0, \t r > 40 40.5940594059406\n","[Episode 4400]\t rewards globals : 4.279 \tavg rewards : 5.050,\tavg loss: : 0.275230,\tbuffer size : 4000,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 50.495049504950494\n","environment change : i_1 49 j_1 9\n","Test mode, runs: 100, i: 49, j: 9, final: True\n","[task_2_tmax50] 100 run(s) avg rewards : 4.9\n","[task_2_tmax40] 100 run(s) avg rewards : 0.9\n","Point: 2.9000000000000004\n","Local runtime: 84.36832237243652 seconds --- fast\n","WARNING: do note that this might not reflect the runtime on the server.\n","[Episode 4500]\t rewards globals : 4.281 \tavg rewards : 4.455,\tavg loss: : 0.275755,\tbuffer size : 4000,\tepsilon : 36.2%, \t r <=40 0.0, \t r > 40 44.554455445544555\n","[Episode 4600]\t rewards globals : 4.203 \tavg rewards : 0.693,\tavg loss: : 0.275692,\tbuffer size : 4000,\tepsilon : 14.0%, \t r <=40 0.0, \t r > 40 6.9306930693069315\n","[Episode 4700]\t rewards globals : 4.186 \tavg rewards : 3.366,\tavg loss: : 0.275810,\tbuffer size : 4000,\tepsilon : 5.8%, \t r <=40 0.0, \t r > 40 33.663366336633665\n","[Episode 4800]\t rewards globals : 4.210 \tavg rewards : 5.248,\tavg loss: : 0.275936,\tbuffer size : 4000,\tepsilon : 2.8%, \t r <=40 0.0, \t r > 40 52.475247524752476\n","[Episode 4900]\t rewards globals : 4.242 \tavg rewards : 5.743,\tavg loss: : 0.275784,\tbuffer size : 4000,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 57.42574257425742\n","environment change : i_1 49 j_1 9\n","Test mode, runs: 100, i: 49, j: 9, final: True\n","[task_2_tmax50] 100 run(s) avg rewards : 6.2\n","[task_2_tmax40] 100 run(s) avg rewards : 0.1\n","Point: 3.15\n","Local runtime: 86.25348472595215 seconds --- fast\n","WARNING: do note that this might not reflect the runtime on the server.\n","[Episode 5000]\t rewards globals : 4.287 \tavg rewards : 6.535,\tavg loss: : 0.276578,\tbuffer size : 4000,\tepsilon : 36.2%, \t r <=40 0.0, \t r > 40 65.34653465346535\n","[Episode 5100]\t rewards globals : 4.233 \tavg rewards : 1.485,\tavg loss: : 0.275974,\tbuffer size : 4000,\tepsilon : 14.0%, \t r <=40 0.0, \t r > 40 14.85148514851485\n","[Episode 5200]\t rewards globals : 4.207 \tavg rewards : 2.871,\tavg loss: : 0.276054,\tbuffer size : 4000,\tepsilon : 5.8%, \t r <=40 0.0, \t r > 40 28.71287128712871\n","[Episode 5300]\t rewards globals : 4.218 \tavg rewards : 4.752,\tavg loss: : 0.276114,\tbuffer size : 4000,\tepsilon : 2.8%, \t r <=40 0.0, \t r > 40 47.524752475247524\n","[Episode 5400]\t rewards globals : 4.253 \tavg rewards : 6.139,\tavg loss: : 0.275596,\tbuffer size : 4000,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 61.386138613861384\n","environment change : i_1 49 j_1 9\n","Test mode, runs: 100, i: 49, j: 9, final: True\n","[task_2_tmax50] 100 run(s) avg rewards : 6.0\n","[task_2_tmax40] 100 run(s) avg rewards : 0.6\n","Point: 3.3\n","Local runtime: 83.65603947639465 seconds --- fast\n","WARNING: do note that this might not reflect the runtime on the server.\n","[Episode 5500]\t rewards globals : 4.272 \tavg rewards : 5.248,\tavg loss: : 0.275582,\tbuffer size : 4000,\tepsilon : 36.2%, \t r <=40 0.0, \t r > 40 52.475247524752476\n","[Episode 5600]\t rewards globals : 4.212 \tavg rewards : 0.891,\tavg loss: : 0.275493,\tbuffer size : 4000,\tepsilon : 14.0%, \t r <=40 0.0, \t r > 40 8.91089108910891\n","[Episode 5700]\t rewards globals : 4.194 \tavg rewards : 3.168,\tavg loss: : 0.274765,\tbuffer size : 4000,\tepsilon : 5.8%, \t r <=40 0.0, \t r > 40 31.683168316831683\n","[Episode 5800]\t rewards globals : 4.203 \tavg rewards : 4.653,\tavg loss: : 0.274827,\tbuffer size : 4000,\tepsilon : 2.8%, \t r <=40 0.0, \t r > 40 46.53465346534654\n","[Episode 5900]\t rewards globals : 4.220 \tavg rewards : 5.149,\tavg loss: : 0.275149,\tbuffer size : 4000,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 51.48514851485149\n","environment change : i_1 49 j_1 9\n","Test mode, runs: 100, i: 49, j: 9, final: True\n","[task_2_tmax50] 100 run(s) avg rewards : 6.0\n","[task_2_tmax40] 100 run(s) avg rewards : 0.4\n","Point: 3.2\n","Local runtime: 84.13158869743347 seconds --- fast\n","WARNING: do note that this might not reflect the runtime on the server.\n","[Episode 6000]\t rewards globals : 4.243 \tavg rewards : 5.545,\tavg loss: : 0.275479,\tbuffer size : 4000,\tepsilon : 36.2%, \t r <=40 0.0, \t r > 40 55.44554455445545\n","[Episode 6100]\t rewards globals : 4.191 \tavg rewards : 1.089,\tavg loss: : 0.275238,\tbuffer size : 4000,\tepsilon : 14.0%, \t r <=40 0.0, \t r > 40 10.891089108910892\n","[Episode 6200]\t rewards globals : 4.183 \tavg rewards : 3.663,\tavg loss: : 0.275315,\tbuffer size : 4000,\tepsilon : 5.8%, \t r <=40 0.0, \t r > 40 36.633663366336634\n","[Episode 6300]\t rewards globals : 4.196 \tavg rewards : 5.050,\tavg loss: : 0.275490,\tbuffer size : 4000,\tepsilon : 2.8%, \t r <=40 0.0, \t r > 40 50.495049504950494\n","[Episode 6400]\t rewards globals : 4.234 \tavg rewards : 6.535,\tavg loss: : 0.275496,\tbuffer size : 4000,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 65.34653465346535\n","environment change : i_1 49 j_1 9\n","Test mode, runs: 100, i: 49, j: 9, final: True\n","[task_2_tmax50] 100 run(s) avg rewards : 6.6\n","[task_2_tmax40] 100 run(s) avg rewards : 0.6\n","Point: 3.5999999999999996\n","Local runtime: 82.97839570045471 seconds --- fast\n","WARNING: do note that this might not reflect the runtime on the server.\n","[Episode 6500]\t rewards globals : 4.270 \tavg rewards : 6.535,\tavg loss: : 0.275266,\tbuffer size : 4000,\tepsilon : 36.2%, \t r <=40 0.0, \t r > 40 65.34653465346535\n","[Episode 6600]\t rewards globals : 4.222 \tavg rewards : 1.089,\tavg loss: : 0.274939,\tbuffer size : 4000,\tepsilon : 14.0%, \t r <=40 0.0, \t r > 40 10.891089108910892\n","[Episode 6700]\t rewards globals : 4.202 \tavg rewards : 2.871,\tavg loss: : 0.274680,\tbuffer size : 4000,\tepsilon : 5.8%, \t r <=40 0.0, \t r > 40 28.71287128712871\n","[Episode 6800]\t rewards globals : 4.216 \tavg rewards : 5.149,\tavg loss: : 0.274371,\tbuffer size : 4000,\tepsilon : 2.8%, \t r <=40 0.0, \t r > 40 51.48514851485149\n","[Episode 6900]\t rewards globals : 4.233 \tavg rewards : 5.347,\tavg loss: : 0.274006,\tbuffer size : 4000,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 53.46534653465347\n","environment change : i_1 49 j_1 9\n","Test mode, runs: 100, i: 49, j: 9, final: True\n","[task_2_tmax50] 100 run(s) avg rewards : 6.7\n","[task_2_tmax40] 100 run(s) avg rewards : 0.1\n","Point: 3.4\n","Local runtime: 82.28992581367493 seconds --- fast\n","WARNING: do note that this might not reflect the runtime on the server.\n","[Episode 7000]\t rewards globals : 4.275 \tavg rewards : 7.228,\tavg loss: : 0.273623,\tbuffer size : 4000,\tepsilon : 36.2%, \t r <=40 0.0, \t r > 40 72.27722772277228\n","[Episode 7100]\t rewards globals : 4.239 \tavg rewards : 1.683,\tavg loss: : 0.272916,\tbuffer size : 4000,\tepsilon : 14.0%, \t r <=40 0.0, \t r > 40 16.831683168316832\n","[Episode 7200]\t rewards globals : 4.237 \tavg rewards : 4.059,\tavg loss: : 0.272411,\tbuffer size : 4000,\tepsilon : 5.8%, \t r <=40 0.0, \t r > 40 40.5940594059406\n","[Episode 7300]\t rewards globals : 4.239 \tavg rewards : 4.455,\tavg loss: : 0.272108,\tbuffer size : 4000,\tepsilon : 2.8%, \t r <=40 0.0, \t r > 40 44.554455445544555\n","[Episode 7400]\t rewards globals : 4.263 \tavg rewards : 6.040,\tavg loss: : 0.271580,\tbuffer size : 4000,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 60.396039603960396\n","environment change : i_1 49 j_1 9\n","Test mode, runs: 100, i: 49, j: 9, final: True\n","[task_2_tmax50] 100 run(s) avg rewards : 7.4\n","[task_2_tmax40] 100 run(s) avg rewards : 0.0\n","Point: 3.7\n","Local runtime: 82.24107336997986 seconds --- fast\n","WARNING: do note that this might not reflect the runtime on the server.\n","[Episode 7500]\t rewards globals : 4.274 \tavg rewards : 5.050,\tavg loss: : 0.270960,\tbuffer size : 4000,\tepsilon : 36.2%, \t r <=40 0.0, \t r > 40 50.495049504950494\n","[Episode 7600]\t rewards globals : 4.240 \tavg rewards : 1.683,\tavg loss: : 0.270529,\tbuffer size : 4000,\tepsilon : 14.0%, \t r <=40 0.0, \t r > 40 16.831683168316832\n","[Episode 7700]\t rewards globals : 4.229 \tavg rewards : 3.465,\tavg loss: : 0.270893,\tbuffer size : 4000,\tepsilon : 5.8%, \t r <=40 0.0, \t r > 40 34.65346534653465\n","[Episode 7800]\t rewards globals : 4.233 \tavg rewards : 4.554,\tavg loss: : 0.270835,\tbuffer size : 4000,\tepsilon : 2.8%, \t r <=40 0.0, \t r > 40 45.54455445544555\n","[Episode 7900]\t rewards globals : 4.245 \tavg rewards : 5.248,\tavg loss: : 0.270766,\tbuffer size : 4000,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 52.475247524752476\n","environment change : i_1 49 j_1 9\n","Test mode, runs: 100, i: 49, j: 9, final: True\n","[task_2_tmax50] 100 run(s) avg rewards : 6.7\n","[task_2_tmax40] 100 run(s) avg rewards : 0.2\n","Point: 3.45\n","Local runtime: 80.37525463104248 seconds --- fast\n","WARNING: do note that this might not reflect the runtime on the server.\n","[Episode 8000]\t rewards globals : 4.261 \tavg rewards : 5.446,\tavg loss: : 0.270511,\tbuffer size : 4000,\tepsilon : 36.2%, \t r <=40 0.0, \t r > 40 54.45544554455446\n","[Episode 8100]\t rewards globals : 4.220 \tavg rewards : 0.990,\tavg loss: : 0.270137,\tbuffer size : 4000,\tepsilon : 14.0%, \t r <=40 0.0, \t r > 40 9.900990099009901\n","[Episode 8200]\t rewards globals : 4.206 \tavg rewards : 2.970,\tavg loss: : 0.270219,\tbuffer size : 4000,\tepsilon : 5.8%, \t r <=40 0.0, \t r > 40 29.7029702970297\n","[Episode 8300]\t rewards globals : 4.221 \tavg rewards : 5.545,\tavg loss: : 0.270200,\tbuffer size : 4000,\tepsilon : 2.8%, \t r <=40 0.0, \t r > 40 55.44554455445545\n","[Episode 8400]\t rewards globals : 4.234 \tavg rewards : 5.347,\tavg loss: : 0.270303,\tbuffer size : 4000,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 53.46534653465347\n","environment change : i_1 49 j_1 9\n","Test mode, runs: 100, i: 49, j: 9, final: True\n","[task_2_tmax50] 100 run(s) avg rewards : 6.0\n","[task_2_tmax40] 100 run(s) avg rewards : 0.3\n","Point: 3.15\n","Local runtime: 83.57546806335449 seconds --- fast\n","WARNING: do note that this might not reflect the runtime on the server.\n","[Episode 8500]\t rewards globals : 4.249 \tavg rewards : 5.446,\tavg loss: : 0.270337,\tbuffer size : 4000,\tepsilon : 36.2%, \t r <=40 0.0, \t r > 40 54.45544554455446\n","[Episode 8600]\t rewards globals : 4.212 \tavg rewards : 1.089,\tavg loss: : 0.270068,\tbuffer size : 4000,\tepsilon : 14.0%, \t r <=40 0.0, \t r > 40 10.891089108910892\n","[Episode 8700]\t rewards globals : 4.201 \tavg rewards : 3.267,\tavg loss: : 0.269936,\tbuffer size : 4000,\tepsilon : 5.8%, \t r <=40 0.0, \t r > 40 32.67326732673268\n","[Episode 8800]\t rewards globals : 4.218 \tavg rewards : 5.644,\tavg loss: : 0.270031,\tbuffer size : 4000,\tepsilon : 2.8%, \t r <=40 0.0, \t r > 40 56.43564356435643\n","[Episode 8900]\t rewards globals : 4.241 \tavg rewards : 6.337,\tavg loss: : 0.270154,\tbuffer size : 4000,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 63.366336633663366\n","environment change : i_1 49 j_1 9\n","Test mode, runs: 100, i: 49, j: 9, final: True\n","[task_2_tmax50] 100 run(s) avg rewards : 6.3\n","[task_2_tmax40] 100 run(s) avg rewards : 2.7\n","Point: 4.5\n","Local runtime: 76.41131448745728 seconds --- fast\n","WARNING: do note that this might not reflect the runtime on the server.\n","[Episode 9000]\t rewards globals : 4.252 \tavg rewards : 5.248,\tavg loss: : 0.270530,\tbuffer size : 4000,\tepsilon : 36.2%, \t r <=40 0.0, \t r > 40 52.475247524752476\n","[Episode 9100]\t rewards globals : 4.225 \tavg rewards : 1.782,\tavg loss: : 0.270595,\tbuffer size : 4000,\tepsilon : 14.0%, \t r <=40 0.0, \t r > 40 17.82178217821782\n","[Episode 9200]\t rewards globals : 4.209 \tavg rewards : 2.772,\tavg loss: : 0.270626,\tbuffer size : 4000,\tepsilon : 5.8%, \t r <=40 0.0, \t r > 40 27.722772277227726\n","[Episode 9300]\t rewards globals : 4.222 \tavg rewards : 5.347,\tavg loss: : 0.270841,\tbuffer size : 4000,\tepsilon : 2.8%, \t r <=40 0.0, \t r > 40 53.46534653465347\n","[Episode 9400]\t rewards globals : 4.239 \tavg rewards : 5.743,\tavg loss: : 0.271145,\tbuffer size : 4000,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 57.42574257425742\n","environment change : i_1 49 j_1 9\n","Test mode, runs: 100, i: 49, j: 9, final: True\n","[task_2_tmax50] 100 run(s) avg rewards : 7.1\n","[task_2_tmax40] 100 run(s) avg rewards : 0.8\n","Point: 3.9499999999999997\n","Local runtime: 79.76133918762207 seconds --- fast\n","WARNING: do note that this might not reflect the runtime on the server.\n","[Episode 9500]\t rewards globals : 4.260 \tavg rewards : 6.238,\tavg loss: : 0.271149,\tbuffer size : 4000,\tepsilon : 36.2%, \t r <=40 0.0, \t r > 40 62.37623762376238\n","[Episode 9600]\t rewards globals : 4.231 \tavg rewards : 1.485,\tavg loss: : 0.270835,\tbuffer size : 4000,\tepsilon : 14.0%, \t r <=40 0.0, \t r > 40 14.85148514851485\n","[Episode 9700]\t rewards globals : 4.228 \tavg rewards : 3.960,\tavg loss: : 0.270812,\tbuffer size : 4000,\tepsilon : 5.8%, \t r <=40 0.0, \t r > 40 39.603960396039604\n","[Episode 9800]\t rewards globals : 4.240 \tavg rewards : 5.347,\tavg loss: : 0.271096,\tbuffer size : 4000,\tepsilon : 2.8%, \t r <=40 0.0, \t r > 40 53.46534653465347\n","[Episode 9900]\t rewards globals : 4.262 \tavg rewards : 6.436,\tavg loss: : 0.271356,\tbuffer size : 4000,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 64.35643564356435\n","7144.447502613068\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kM9mre8d4VL-","colab_type":"code","outputId":"89801504-863c-47d2-9e74-173e6eb3df5e","executionInfo":{"status":"ok","timestamp":1587475213159,"user_tz":-480,"elapsed":2275214,"user":{"displayName":"N","photoUrl":"","userId":"08619141603155082384"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# TASK 2\n","\n","import sys\n","import time\n","\n","import torch\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n","\n","date_save = time.strftime(\"%d %H %M\")\n","model_path = os.path.join('HW3_Task2','agent', 'model_{}_{}.pt'.format(date_save,2))\n","save_path = os.path.join('HW3_Task2','agent', 'model_{}_{}.pt'.format(date_save,3))\n","\n","duree = 0 \n","nb_bad_init = 0 \n","# Try for 10 diferent inits\n","\n","for i in range(0,1):\n","  debut = time.time()\n","  # Try an init \n","  #!python3 HW3_Task2/agent/models.py --test --path=HW3_Task2/agent/model_last1_20_05_cb_8_1000.pt --savepath=HW3_Task2/agent/model_last1_20_05\n","  #!python3 HW3_Task2/agent/models.py --pretrain --path=HW3_Task2/agent/model_last_20_05_cb_8.pt --savepath=HW3_Task2/agent/model_last_20_05\n","  !python3 HW3_Task2/agent/models.py --pretrain --path=HW3_Task2/agent/model_last_21_05_cb_final_8.pt --savepath=HW3_Task2/agent/model_snif_21_05\n","  #!python3 HW3_Task2/agent/testenv.py\n","  duree = (time.time()-debut)\n","  # If good init exit loop\n","  print(duree)\n","  if duree > 10000:\n","    break\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["cuda\n","pretrain mode\n","model loaded from HW3_Task2/agent/model_last_21_05_cb_final_8.pt\n","ConvDQN(\n","  (features): Sequential(\n","    (0): Conv2d(4, 32, kernel_size=(2, 2), stride=(1, 1))\n","    (1): ReLU()\n","    (2): Conv2d(32, 64, kernel_size=(2, 2), stride=(1, 1))\n","    (3): ReLU()\n","  )\n","  (layers): Sequential(\n","    (0): Linear(in_features=24576, out_features=256, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=256, out_features=5, bias=True)\n","  )\n",")\n","range change : x_min 49 x_max 49\n","environment change : i_1 49 j_1 9\n","[Episode 50]\t rewards globals : 0.598 \tavg rewards : 0.598,\tavg loss: : 0.256007,\tfirst time zero 0.7058823529411765,\tbuffer size : 1187,\tepsilon : 28.3%, \t r <=40 0.0, \t r > 40 5.88235294117647\n","[Episode 100]\t rewards globals : 0.599 \tavg rewards : 0.588,\tavg loss: : 0.109032,\tfirst time zero 0.3564356435643564,\tbuffer size : 2129,\tepsilon : 22.3%, \t r <=40 0.0, \t r > 40 5.88235294117647\n","[Episode 150]\t rewards globals : 0.960 \tavg rewards : 1.657,\tavg loss: : 0.113348,\tfirst time zero 0.6821192052980133,\tbuffer size : 3461,\tepsilon : 17.6%, \t r <=40 0.0, \t r > 40 11.76470588235294\n","[Episode 200]\t rewards globals : 1.179 \tavg rewards : 1.804,\tavg loss: : 0.102202,\tfirst time zero 0.8557213930348259,\tbuffer size : 4790,\tepsilon : 13.9%, \t r <=40 0.0, \t r > 40 13.725490196078432\n","[Episode 250]\t rewards globals : 1.394 \tavg rewards : 2.412,\tavg loss: : 0.116100,\tfirst time zero 0.9840637450199203,\tbuffer size : 6261,\tepsilon : 11.0%, \t r <=40 0.0, \t r > 40 19.607843137254903\n","[Episode 300]\t rewards globals : 1.641 \tavg rewards : 2.824,\tavg loss: : 0.114048,\tfirst time zero 1.4551495016611296,\tbuffer size : 7838,\tepsilon : 8.8%, \t r <=40 0.0, \t r > 40 21.568627450980394\n","[Episode 350]\t rewards globals : 1.947 \tavg rewards : 3.912,\tavg loss: : 0.126491,\tfirst time zero 2.034188034188034,\tbuffer size : 9432,\tepsilon : 7.1%, \t r <=40 0.0, \t r > 40 27.450980392156865\n","[Episode 400]\t rewards globals : 2.349 \tavg rewards : 5.284,\tavg loss: : 0.127224,\tfirst time zero 2.4713216957605986,\tbuffer size : 10000,\tepsilon : 5.7%, \t r <=40 0.0, \t r > 40 37.254901960784316\n","[Episode 450]\t rewards globals : 2.579 \tavg rewards : 4.529,\tavg loss: : 0.137102,\tfirst time zero 2.8758314855875833,\tbuffer size : 10000,\tepsilon : 4.7%, \t r <=40 0.0, \t r > 40 29.411764705882355\n","Test mode, runs: 100, i: 49, j: 9, final: True\n","[task_2_tmax50] 100 run(s) avg rewards : 5.9\n","[task_2_tmax40] 100 run(s) avg rewards : 3.2\n","Point: 4.550000000000001\n","Local runtime: 74.69968676567078 seconds --- fast\n","WARNING: do note that this might not reflect the runtime on the server.\n","[Episode 500]\t rewards globals : 2.750 \tavg rewards : 4.402,\tavg loss: : 0.138225,\tfirst time zero 2.9081836327345307,\tbuffer size : 10000,\tepsilon : 3.9%, \t r <=40 0.0, \t r > 40 39.21568627450981\n","[Episode 550]\t rewards globals : 3.134 \tavg rewards : 7.059,\tavg loss: : 0.147388,\tfirst time zero 3.2123411978221417,\tbuffer size : 10000,\tepsilon : 3.2%, \t r <=40 0.0, \t r > 40 58.82352941176471\n","[Episode 600]\t rewards globals : 3.359 \tavg rewards : 5.912,\tavg loss: : 0.147040,\tfirst time zero 3.6622296173044924,\tbuffer size : 10000,\tepsilon : 2.7%, \t r <=40 0.0, \t r > 40 45.09803921568628\n","[Episode 650]\t rewards globals : 3.594 \tavg rewards : 6.294,\tavg loss: : 0.154384,\tfirst time zero 4.339477726574501,\tbuffer size : 10000,\tepsilon : 2.4%, \t r <=40 0.0, \t r > 40 39.21568627450981\n","[Episode 700]\t rewards globals : 3.821 \tavg rewards : 6.647,\tavg loss: : 0.154144,\tfirst time zero 4.807417974322396,\tbuffer size : 10000,\tepsilon : 2.1%, \t r <=40 0.0, \t r > 40 39.21568627450981\n","[Episode 750]\t rewards globals : 3.889 \tavg rewards : 4.951,\tavg loss: : 0.161009,\tfirst time zero 4.958721704394141,\tbuffer size : 10000,\tepsilon : 1.8%, \t r <=40 0.0, \t r > 40 33.33333333333333\n","[Episode 800]\t rewards globals : 3.928 \tavg rewards : 4.608,\tavg loss: : 0.161204,\tfirst time zero 5.228464419475656,\tbuffer size : 10000,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 25.49019607843137\n","[Episode 850]\t rewards globals : 4.073 \tavg rewards : 6.480,\tavg loss: : 0.166485,\tfirst time zero 5.643948296122209,\tbuffer size : 10000,\tepsilon : 1.5%, \t r <=40 0.0, \t r > 40 37.254901960784316\n","[Episode 900]\t rewards globals : 4.227 \tavg rewards : 6.902,\tavg loss: : 0.166099,\tfirst time zero 6.064372918978912,\tbuffer size : 10000,\tepsilon : 1.4%, \t r <=40 0.0, \t r > 40 47.05882352941176\n","[Episode 950]\t rewards globals : 4.341 \tavg rewards : 6.265,\tavg loss: : 0.170307,\tfirst time zero 6.3249211356466875,\tbuffer size : 10000,\tepsilon : 1.3%, \t r <=40 0.0, \t r > 40 39.21568627450981\n","Test mode, runs: 100, i: 49, j: 9, final: True\n","[task_2_tmax50] 100 run(s) avg rewards : 7.3\n","[task_2_tmax40] 100 run(s) avg rewards : 0.1\n","Point: 3.6999999999999997\n","Local runtime: 79.1206738948822 seconds --- fast\n","WARNING: do note that this might not reflect the runtime on the server.\n","[Episode 1000]\t rewards globals : 4.477 \tavg rewards : 7.118,\tavg loss: : 0.169548,\tfirst time zero 6.772227772227772,\tbuffer size : 10000,\tepsilon : 1.2%, \t r <=40 0.0, \t r > 40 45.09803921568628\n","[Episode 1050]\t rewards globals : 4.529 \tavg rewards : 5.706,\tavg loss: : 0.174438,\tfirst time zero 6.87535680304472,\tbuffer size : 10000,\tepsilon : 1.2%, \t r <=40 0.0, \t r > 40 29.411764705882355\n","[Episode 1100]\t rewards globals : 4.612 \tavg rewards : 6.422,\tavg loss: : 0.174220,\tfirst time zero 7.136239782016348,\tbuffer size : 10000,\tepsilon : 1.1%, \t r <=40 0.0, \t r > 40 33.33333333333333\n","[Episode 1150]\t rewards globals : 4.716 \tavg rewards : 7.098,\tavg loss: : 0.177923,\tfirst time zero 7.420503909643788,\tbuffer size : 10000,\tepsilon : 1.1%, \t r <=40 0.0, \t r > 40 43.13725490196079\n","[Episode 1200]\t rewards globals : 4.726 \tavg rewards : 5.069,\tavg loss: : 0.177564,\tfirst time zero 7.502081598667777,\tbuffer size : 10000,\tepsilon : 1.1%, \t r <=40 0.0, \t r > 40 27.450980392156865\n","[Episode 1250]\t rewards globals : 4.736 \tavg rewards : 5.118,\tavg loss: : 0.181282,\tfirst time zero 7.394084732214228,\tbuffer size : 10000,\tepsilon : 1.1%, \t r <=40 0.0, \t r > 40 35.294117647058826\n","[Episode 1300]\t rewards globals : 4.786 \tavg rewards : 6.098,\tavg loss: : 0.181036,\tfirst time zero 7.283627978478094,\tbuffer size : 10000,\tepsilon : 1.1%, \t r <=40 0.0, \t r > 40 47.05882352941176\n","[Episode 1350]\t rewards globals : 4.837 \tavg rewards : 6.304,\tavg loss: : 0.184070,\tfirst time zero 7.467061435973353,\tbuffer size : 10000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 37.254901960784316\n","[Episode 1400]\t rewards globals : 4.889 \tavg rewards : 6.382,\tavg loss: : 0.183338,\tfirst time zero 7.650963597430406,\tbuffer size : 10000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 33.33333333333333\n","[Episode 1450]\t rewards globals : 4.910 \tavg rewards : 5.588,\tavg loss: : 0.186032,\tfirst time zero 7.835975189524466,\tbuffer size : 10000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 25.49019607843137\n","Test mode, runs: 100, i: 49, j: 9, final: True\n","[task_2_tmax50] 100 run(s) avg rewards : 6.5\n","[task_2_tmax40] 100 run(s) avg rewards : 0.4\n","Point: 3.45\n","Local runtime: 81.2177209854126 seconds --- fast\n","WARNING: do note that this might not reflect the runtime on the server.\n","[Episode 1500]\t rewards globals : 4.973 \tavg rewards : 6.686,\tavg loss: : 0.184894,\tfirst time zero 8.135243171219187,\tbuffer size : 10000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 29.411764705882355\n","[Episode 1550]\t rewards globals : 5.028 \tavg rewards : 6.784,\tavg loss: : 0.186866,\tfirst time zero 8.274016763378466,\tbuffer size : 10000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 37.254901960784316\n","[Episode 1600]\t rewards globals : 5.068 \tavg rewards : 6.373,\tavg loss: : 0.185897,\tfirst time zero 8.443472829481575,\tbuffer size : 10000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 37.254901960784316\n","[Episode 1650]\t rewards globals : 5.119 \tavg rewards : 6.627,\tavg loss: : 0.187189,\tfirst time zero 8.603876438522107,\tbuffer size : 10000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 35.294117647058826\n","[Episode 1700]\t rewards globals : 5.191 \tavg rewards : 7.647,\tavg loss: : 0.185976,\tfirst time zero 8.753674309229865,\tbuffer size : 10000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 41.17647058823529\n","[Episode 1750]\t rewards globals : 5.206 \tavg rewards : 5.804,\tavg loss: : 0.187058,\tfirst time zero 8.918332381496288,\tbuffer size : 10000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 31.372549019607842\n","[Episode 1800]\t rewards globals : 5.243 \tavg rewards : 6.412,\tavg loss: : 0.185985,\tfirst time zero 8.99833425874514,\tbuffer size : 10000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 31.372549019607842\n","[Episode 1850]\t rewards globals : 5.307 \tavg rewards : 7.471,\tavg loss: : 0.187595,\tfirst time zero 9.180983252296055,\tbuffer size : 10000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 41.17647058823529\n","[Episode 1900]\t rewards globals : 5.344 \tavg rewards : 6.755,\tavg loss: : 0.186942,\tfirst time zero 9.19726459758022,\tbuffer size : 10000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 50.98039215686274\n","[Episode 1950]\t rewards globals : 5.397 \tavg rewards : 7.294,\tavg loss: : 0.189402,\tfirst time zero 9.302921578677601,\tbuffer size : 10000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 35.294117647058826\n","2273.953361272812\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"oEhO5nj6eZz8","colab_type":"code","outputId":"599ff076-81de-4613-8555-a6cce530cec2","executionInfo":{"status":"ok","timestamp":1587201073879,"user_tz":-480,"elapsed":296419,"user":{"displayName":"N","photoUrl":"","userId":"08619141603155082384"}},"colab":{"base_uri":"https://localhost:8080/","height":105}},"source":["# Test for TASK 2:\n","!python3 HW3_Task2/agent/__init__.py\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[task_2_tmax50] 300 run(s) avg rewards : 0.0\n","[task_2_tmax40] 300 run(s) avg rewards : 0.0\n","Point: 0.0\n","Local runtime: 291.5432770252228 seconds --- safe\n","WARNING: do note that this might not reflect the runtime on the server.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1RIkGEjdJw83","colab_type":"code","outputId":"5a1e1651-31fd-4784-91fa-0d672a113f88","executionInfo":{"status":"error","timestamp":1587229992901,"user_tz":-480,"elapsed":3492,"user":{"displayName":"N","photoUrl":"","userId":"08619141603155082384"}},"colab":{"base_uri":"https://localhost:8080/","height":401}},"source":["# TASK 1\n","\n","import time\n","duree = 0 \n","nb_bad_init = 0 \n","# Try for 10 diferent inits\n","\n","for i in range(0,10):\n","  debut = time.time()\n","  # Try an init \n","  !python3 HW3_Task1/agent/dqn.py --train\n","  duree = (time.time()-debut)\n","  # If good init exit loop\n","  if duree > 100:\n","    break"],"execution_count":0,"outputs":[{"output_type":"stream","text":["python3: can't open file 'HW3_Task1/agent/dqn.py': [Errno 2] No such file or directory\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-80-c525fe8926e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mdebut\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;31m# Try an init\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m   \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'python3 HW3_Task1/agent/dqn.py --train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m   \u001b[0mduree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdebut\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0;31m# If good init exit loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'also_return_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_system_commands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_system_compat\u001b[0;34m(shell, cmd, also_return_output)\u001b[0m\n\u001b[1;32m    436\u001b[0m   \u001b[0;31m# stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m   result = _run_command(\n\u001b[0;32m--> 438\u001b[0;31m       shell.var_expand(cmd, depth=2), clear_streamed_output=False)\n\u001b[0m\u001b[1;32m    439\u001b[0m   \u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_exit_code'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_INTERRUPTED_SIGNALS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_run_command\u001b[0;34m(cmd, clear_streamed_output)\u001b[0m\n\u001b[1;32m    193\u001b[0m       \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild_pty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0m_monitor_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_pty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_stdin_widget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0mepoll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_display_stdin_widget\u001b[0;34m(delay_millis)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m   \u001b[0mhide_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'cell_remove_stdin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m   \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocking_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mhide_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_header\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    169\u001b[0m   \u001b[0;31m# unique.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m   \u001b[0mrequest_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msend_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     if (reply.get('type') == 'colab_reply' and\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"ru96YQP2MC35","colab_type":"code","outputId":"e103876d-5253-4da0-e47c-68f762738964","executionInfo":{"status":"error","timestamp":1587493696392,"user_tz":-480,"elapsed":772,"user":{"displayName":"N","photoUrl":"","userId":"08619141603155082384"}},"colab":{"base_uri":"https://localhost:8080/","height":377}},"source":["from HW3_Task2.agent.env import construct_task2_env_0\n","import numpy as np\n","env = construct_task2_env_0()\n","\n","state = env.reset()\n","env.render()\n","for i in range(0,10):\n","  env.step(1)\n","  env.render()\n","def pos_from_state(state):\n","  for i in range(0,10):\n","    column = (np.where(state[1,i,:] == 1.))\n","    if len(column[0]) > 0:\n","      column = column[0][0]\n","      row = i \n","      return(column,row)\n","\n","def state_from_pos(pos_x,pos_y,state):\n","  #Crop to keep a window [49,4]\n","  if pos_y > 8:\n","    y_max = 9\n","    y_min = 5\n","  elif pos_y < 2:\n","    y_max = 4\n","    y_min = 0\n","  else:\n","    y_max = pos_y + 2\n","    y_min = pos_y - 2\n","\n","  return(state[:,y_min:(y_max+1),:])\n","\n","pos_y,pos_x = pos_from_state(state)\n","print(pos_y,pos_x)\n","next_state = state_from_pos(pos_x,pos_y,state)\n","\n","print(next_state)\n","\n"],"execution_count":120,"outputs":[{"output_type":"error","ename":"ImportError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-120-3dd1b1723730>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mHW3_Task2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconstruct_task2_env_0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstruct_task2_env_0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'construct_task2_env_0'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}]},{"cell_type":"code","metadata":{"id":"2bjWQhZdswyC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":360},"outputId":"197b0003-8a5b-43cb-cf60-8a0e9490b4cf","executionInfo":{"status":"error","timestamp":1587486926646,"user_tz":-480,"elapsed":748,"user":{"displayName":"N","photoUrl":"","userId":"08619141603155082384"}}},"source":["from HW3_Task2.agent.loss_src import monteCarloUpdate\n","import collections\n","from HW3_Task2.agent.buffer_src import ReplayBuffer\n","Transition = collections.namedtuple('Transition', ('state', 'action', 'reward', 'next_state', 'done'))\n","\n","memory = ReplayBuffer(buffer_limit=10)\n","print(memory.current_number)\n","memory.push(Transition(1,1,0,0,0))\n","memory.push(Transition(1,1,2,0,0))\n","memory.push(Transition(1,1,3,0,0))\n","\n","monteCarloUpdate((memory,3,0.8))\n","print(memory)\n"],"execution_count":18,"outputs":[{"output_type":"stream","text":["0\n"],"name":"stdout"},{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-18-afe2e0f603d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTransition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mmonteCarloUpdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/HW3_Task2/agent/loss_src.py\u001b[0m in \u001b[0;36mmonteCarloUpdate\u001b[0;34m(memory, nb_steps, discount_factor)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmonteCarloUpdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnb_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdiscount_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m   \u001b[0mcur_memory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_number\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'current_number'"]}]},{"cell_type":"code","metadata":{"id":"mac08SvWkxM6","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":523},"outputId":"b5e2a0ed-d3b0-40ee-88c5-8b8ac0b0d2e2","executionInfo":{"status":"error","timestamp":1587493702772,"user_tz":-480,"elapsed":1221,"user":{"displayName":"N","photoUrl":"","userId":"08619141603155082384"}}},"source":[""],"execution_count":121,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-121-fb4421c2810b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mHW3_Task2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mHW3_Task2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimporlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/HW3_Task2/agent/models.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m#from agent import ConvDQN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbuffer_src\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConvDQN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/HW3_Task2/agent/agent.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mloss_src\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'loss_src'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}]},{"cell_type":"code","metadata":{"id":"sr0jIYNEmV_w","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":490},"outputId":"0c7fad96-e949-4e6f-ad60-d9039c448757","executionInfo":{"status":"ok","timestamp":1587494419234,"user_tz":-480,"elapsed":7138,"user":{"displayName":"N","photoUrl":"","userId":"08619141603155082384"}}},"source":["!apt-get install zip\n","!zip -r HW3_Task2.zip HW3_Task2\n","from google.colab import files\n","files.download('HW3_Task2.zip')"],"execution_count":146,"outputs":[{"output_type":"stream","text":["Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","zip is already the newest version (3.0-11build1).\n","0 upgraded, 0 newly installed, 0 to remove and 25 not upgraded.\n","updating: HW3_Task2/ (stored 0%)\n","updating: HW3_Task2/setup.py (deflated 27%)\n","updating: HW3_Task2/agent/ (stored 0%)\n","updating: HW3_Task2/agent/.ipynb_checkpoints/ (stored 0%)\n","updating: HW3_Task2/agent/env.py (deflated 83%)\n","updating: HW3_Task2/agent/buffer_src.py (deflated 67%)\n","updating: HW3_Task2/agent/loss.py (deflated 61%)\n","updating: HW3_Task2/agent/__init__.py (deflated 61%)\n","updating: HW3_Task2/agent/loss_src.py (deflated 61%)\n","updating: HW3_Task2/agent/models.py (deflated 70%)\n","updating: HW3_Task2/agent/__pycache__/ (stored 0%)\n","updating: HW3_Task2/agent/__pycache__/loss_src.cpython-36.pyc (deflated 40%)\n","updating: HW3_Task2/agent/__pycache__/buffer_src.cpython-36.pyc (deflated 50%)\n","updating: HW3_Task2/agent/__pycache__/__init__.cpython-36.pyc (deflated 41%)\n","updating: HW3_Task2/agent/state_change.py (deflated 56%)\n","updating: HW3_Task2/agent/agent.py (deflated 68%)\n","  adding: HW3_Task2/.ipynb_checkpoints/ (stored 0%)\n","  adding: HW3_Task2/saves/ (stored 0%)\n","  adding: HW3_Task2/untitled (stored 0%)\n","  adding: HW3_Task2/agent/__pycache__/env.cpython-36.pyc (deflated 52%)\n","  adding: HW3_Task2/agent/__pycache__/agent.cpython-36.pyc (deflated 52%)\n","  adding: HW3_Task2/agent/__pycache__/models.cpython-36.pyc (deflated 44%)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bjesHYzy3DnR","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}