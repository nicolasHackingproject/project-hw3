{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"main_task1.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"sCXRy7hMe_eN","colab_type":"text"},"source":["## 1. Read Me \n","**Run first cell :** install dependencies \n","\n","**Run second cell :** install files and local environment on the VM, choose which task you want to run by setting task_1 or task_2 value to true\n","\n","**Run third cell :** try diferent init of the agent. When one is found -> the agent is trained  "]},{"cell_type":"code","metadata":{"id":"8Rbvg4vCI9nE","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":352},"outputId":"dbbadc6c-caa0-4c49-ae01-d907284da0fb","executionInfo":{"status":"ok","timestamp":1587496901826,"user_tz":-480,"elapsed":10909,"user":{"displayName":"N","photoUrl":"","userId":"08619141603155082384"}}},"source":["# Dependencies\n","\n","!pip install torch numpy git+https://github.com/cs4246/gym-grid-driving.git\n","!pip install -U -q PyDrive"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting git+https://github.com/cs4246/gym-grid-driving.git\n","  Cloning https://github.com/cs4246/gym-grid-driving.git to /tmp/pip-req-build-3isjc500\n","  Running command git clone -q https://github.com/cs4246/gym-grid-driving.git /tmp/pip-req-build-3isjc500\n","Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.18.2)\n","Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (from gym-grid-driving==0.0.1) (0.17.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym->gym-grid-driving==0.0.1) (1.4.1)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym->gym-grid-driving==0.0.1) (1.5.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym->gym-grid-driving==0.0.1) (1.12.0)\n","Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym->gym-grid-driving==0.0.1) (1.3.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym->gym-grid-driving==0.0.1) (0.16.0)\n","Building wheels for collected packages: gym-grid-driving\n","  Building wheel for gym-grid-driving (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gym-grid-driving: filename=gym_grid_driving-0.0.1-cp36-none-any.whl size=8623 sha256=e301538d685af0a3da25e24728113d2619303390ad2f4b301b30eedcb344167c\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-dfzky4s5/wheels/e1/30/f2/157c0938ab9bfe9c10c29c9fcab8392f587c9d141f215b67ca\n","Successfully built gym-grid-driving\n","Installing collected packages: gym-grid-driving\n","Successfully installed gym-grid-driving-0.0.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dYGLrHZSMtsp","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":507},"outputId":"8234b639-edaf-4e0a-f722-d03661cf5e6b","executionInfo":{"status":"ok","timestamp":1587499558672,"user_tz":-480,"elapsed":34992,"user":{"displayName":"N","photoUrl":"","userId":"08619141603155082384"}}},"source":["!apt-get install zip\n","!zip -r HW3_Task2.zip HW3_Task2\n","from google.colab import files\n","files.download('HW3_Task2.zip')"],"execution_count":15,"outputs":[{"output_type":"stream","text":["Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","zip is already the newest version (3.0-11build1).\n","0 upgraded, 0 newly installed, 0 to remove and 25 not upgraded.\n","  adding: HW3_Task2/ (stored 0%)\n","  adding: HW3_Task2/untitled (stored 0%)\n","  adding: HW3_Task2/saves/ (stored 0%)\n","  adding: HW3_Task2/saves/m__cb_15_1.pt (deflated 7%)\n","  adding: HW3_Task2/.ipynb_checkpoints/ (stored 0%)\n","  adding: HW3_Task2/agent/ (stored 0%)\n","  adding: HW3_Task2/agent/state_change.py (deflated 56%)\n","  adding: HW3_Task2/agent/__init__.py (deflated 61%)\n","  adding: HW3_Task2/agent/buffer_src.py (deflated 67%)\n","  adding: HW3_Task2/agent/agent.py (deflated 68%)\n","  adding: HW3_Task2/agent/__pycache__/ (stored 0%)\n","  adding: HW3_Task2/agent/__pycache__/__init__.cpython-36.pyc (deflated 41%)\n","  adding: HW3_Task2/agent/__pycache__/models.cpython-36.pyc (deflated 44%)\n","  adding: HW3_Task2/agent/__pycache__/loss_src.cpython-36.pyc (deflated 39%)\n","  adding: HW3_Task2/agent/__pycache__/buffer_src.cpython-36.pyc (deflated 50%)\n","  adding: HW3_Task2/agent/__pycache__/agent.cpython-36.pyc (deflated 51%)\n","  adding: HW3_Task2/agent/__pycache__/env.cpython-36.pyc (deflated 52%)\n","  adding: HW3_Task2/agent/loss.py (deflated 61%)\n","  adding: HW3_Task2/agent/.ipynb_checkpoints/ (stored 0%)\n","  adding: HW3_Task2/agent/env.py (deflated 83%)\n","  adding: HW3_Task2/agent/models.py (deflated 70%)\n","  adding: HW3_Task2/agent/loss_src.py (deflated 62%)\n","  adding: HW3_Task2/setup.py (deflated 27%)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_0lVUcYwSoSv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":140},"outputId":"04a32ea6-0cee-4971-e569-877a725eb8a2","executionInfo":{"status":"ok","timestamp":1587499092565,"user_tz":-480,"elapsed":6383,"user":{"displayName":"N","photoUrl":"","userId":"08619141603155082384"}}},"source":["from HW3_Task2.agent import models \n","from HW3_Task2.agent import agent \n","from HW3_Task2.agent import env as env_builder\n","import importlib\n","import torch\n","importlib.reload(models)\n","\n","#Init \n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"device is:\",device)\n","\n","model = models.get_model(modelpath='HW3_Task2/saves/m__cb_15_1.pt',device=device)\n","models.mainTest(model,i1=15,j1=1)"],"execution_count":13,"outputs":[{"output_type":"stream","text":["device is: cuda\n","device is: cuda\n","device is: cuda\n","[task_2_tmax50] 25 run(s) avg rewards : 9.2\n","[task_2_tmax40] 25 run(s) avg rewards : 9.2\n","Point: 9.2\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["9.2"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"I2x3en4hNUuR","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"2f8aedb7-8682-4124-9e84-b624400640a1"},"source":["from HW3_Task2.agent import models \n","from HW3_Task2.agent import agent \n","from HW3_Task2.agent import env as env_builder\n","import importlib \n","import collections\n","importlib.reload(models)\n","importlib.reload(agent)\n","importlib.reload(env_builder)\n","try:\n","  importlib.reload(models)\n","  importlib.reload(agent)\n","  importlib.reload(env)\n","except:pass\n","Config = collections.namedtuple('Config', ('x_debut', 'y_debut', 'max_episode', 'max_epsilon', 'epsilon_decay', 'test_interval','save_interval','batch_size','buffer_limit','methode','gamma_nstep','nstep'))\n","config_l=[]\n","last_y = 2\n","for i in range(0,9):\n","  x_debut = 10+i*5\n","  \n","\n","  for j in range(0,last_y):\n","    time = i + j \n","    \n","    y_debut = j\n","    max_episode = 250*(i+1)+400*(j)\n","    max_episode = min(max_episode,10000)\n","    # max_epsilon decrease with time \n","    max_epsilon = max(1.0 / (time+1),0.3)\n","    epsilon_decay = max_episode // 4\n","    test_interval = min(500,max_episode//2)\n","    save_interval = 1000\n","    batch_size = 64\n","    buffer_limit = min(40000,4000*(time+1))\n","    methode = 'Mixed Monte Carlo + Atari DQN'\n","    gamma_nstep  = 0.5\n","    nstep = 3\n","    config_cur = Config(x_debut,y_debut,max_episode,max_epsilon,epsilon_decay,test_interval,save_interval,batch_size,buffer_limit,methode,gamma_nstep,nstep)\n","    \n","    env = env_builder.construct_task2_env_ij(x_debut,y_debut)\n","    \n","    if i == 0 and j == 0 :\n","      model = models.train(agent.AtariDQN, env=env,pretrain=False,model_p= None,savepath='HW3_Task2/saves/m_',config=config_cur)\n","    else:\n","      model = models.train(agent.AtariDQN, env=env,pretrain=True,model_p= model,savepath='HW3_Task2/saves/m_',config=config_cur)\n","\n","\n","  last_y += 1\n","\n","!zip -r HW3_Task2_Atari.zip HW3_Task2\n","from google.colab import files\n","files.download('HW3_Task2_Atari.zip')\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["device is: cuda\n","config : \n"," x debut 10, y_debut 0, max ep 250, max epsilon 1.0 \n"," Epsilon_decay 62, test_interval 125\n","Save_interval 1000, batch_size 64, buffer_limit 4000 \n"," Methode Mixed Monte Carlo + Atari DQN, gamma_nstep 0.5, nstep 3\n","Start learning from : x_debut 10 y_debut 0\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n","  out=out, **kwargs)\n","/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n","  ret = ret.dtype.type(ret / rcount)\n"],"name":"stderr"},{"output_type":"stream","text":["[Episode 50]\t rewards globals : 3.333 \tavg rewards : 3.333,\tavg loss: : nan,\tbuffer size : 294,\tepsilon : 45.2%, \t r <=40 0.0, \t r > 40 33.33333333333333\n","[Episode 100]\t rewards globals : 3.762 \tavg rewards : 4.118,\tavg loss: : 1.690692,\tbuffer size : 541,\tepsilon : 20.7%, \t r <=40 0.0, \t r > 40 41.17647058823529\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 7.1\n","[task_2_tmax40] 100 run(s) avg rewards : 7.0\n","Point: 7.05\n","[Episode 150]\t rewards globals : 4.570 \tavg rewards : 6.078,\tavg loss: : 0.973128,\tbuffer size : 786,\tepsilon : 9.8%, \t r <=40 0.0, \t r > 40 60.78431372549019\n","[Episode 200]\t rewards globals : 5.025 \tavg rewards : 6.471,\tavg loss: : 0.746701,\tbuffer size : 1019,\tepsilon : 4.9%, \t r <=40 0.0, \t r > 40 64.70588235294117\n","error while saving final\n","device is: cuda\n","config : \n"," x debut 10, y_debut 1, max ep 650, max epsilon 0.5 \n"," Epsilon_decay 162, test_interval 325\n","Save_interval 1000, batch_size 64, buffer_limit 8000 \n"," Methode Mixed Monte Carlo + Atari DQN, gamma_nstep 0.5, nstep 3\n","Start learning from : x_debut 10 y_debut 1\n","[Episode 50]\t rewards globals : 3.922 \tavg rewards : 3.922,\tavg loss: : nan,\tbuffer size : 220,\tepsilon : 37.0%, \t r <=40 0.0, \t r > 40 39.21568627450981\n","[Episode 100]\t rewards globals : 4.059 \tavg rewards : 4.118,\tavg loss: : 1.085496,\tbuffer size : 454,\tepsilon : 27.4%, \t r <=40 0.0, \t r > 40 41.17647058823529\n","[Episode 150]\t rewards globals : 3.709 \tavg rewards : 2.941,\tavg loss: : 0.551766,\tbuffer size : 730,\tepsilon : 20.4%, \t r <=40 0.0, \t r > 40 29.411764705882355\n","[Episode 200]\t rewards globals : 3.731 \tavg rewards : 3.922,\tavg loss: : 0.414949,\tbuffer size : 988,\tepsilon : 15.3%, \t r <=40 0.0, \t r > 40 39.21568627450981\n","[Episode 250]\t rewards globals : 3.785 \tavg rewards : 3.922,\tavg loss: : 0.360585,\tbuffer size : 1232,\tepsilon : 11.5%, \t r <=40 0.0, \t r > 40 39.21568627450981\n","[Episode 300]\t rewards globals : 3.887 \tavg rewards : 4.510,\tavg loss: : 0.320012,\tbuffer size : 1507,\tepsilon : 8.7%, \t r <=40 0.0, \t r > 40 45.09803921568628\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 5.6\n","[task_2_tmax40] 100 run(s) avg rewards : 5.3\n","Point: 5.449999999999999\n","[Episode 350]\t rewards globals : 3.960 \tavg rewards : 4.510,\tavg loss: : 0.286268,\tbuffer size : 1753,\tepsilon : 6.6%, \t r <=40 0.0, \t r > 40 45.09803921568628\n","[Episode 400]\t rewards globals : 4.040 \tavg rewards : 4.510,\tavg loss: : 0.266058,\tbuffer size : 2026,\tepsilon : 5.1%, \t r <=40 0.0, \t r > 40 45.09803921568628\n","[Episode 450]\t rewards globals : 4.102 \tavg rewards : 4.510,\tavg loss: : 0.258887,\tbuffer size : 2250,\tepsilon : 4.0%, \t r <=40 0.0, \t r > 40 45.09803921568628\n","[Episode 500]\t rewards globals : 4.271 \tavg rewards : 5.686,\tavg loss: : 0.242618,\tbuffer size : 2509,\tepsilon : 3.2%, \t r <=40 0.0, \t r > 40 56.86274509803921\n","[Episode 550]\t rewards globals : 4.428 \tavg rewards : 6.078,\tavg loss: : 0.228167,\tbuffer size : 2757,\tepsilon : 2.6%, \t r <=40 0.0, \t r > 40 60.78431372549019\n","[Episode 600]\t rewards globals : 4.493 \tavg rewards : 5.098,\tavg loss: : 0.219923,\tbuffer size : 3008,\tepsilon : 2.2%, \t r <=40 0.0, \t r > 40 50.98039215686274\n","error while saving final\n","device is: cuda\n","config : \n"," x debut 15, y_debut 0, max ep 500, max epsilon 0.5 \n"," Epsilon_decay 125, test_interval 250\n","Save_interval 1000, batch_size 64, buffer_limit 8000 \n"," Methode Mixed Monte Carlo + Atari DQN, gamma_nstep 0.5, nstep 3\n","Start learning from : x_debut 15 y_debut 0\n","[Episode 50]\t rewards globals : 3.922 \tavg rewards : 3.922,\tavg loss: : 1.116493,\tbuffer size : 373,\tepsilon : 33.8%, \t r <=40 0.0, \t r > 40 39.21568627450981\n","[Episode 100]\t rewards globals : 3.960 \tavg rewards : 3.922,\tavg loss: : 0.272027,\tbuffer size : 692,\tepsilon : 23.0%, \t r <=40 0.0, \t r > 40 39.21568627450981\n","[Episode 150]\t rewards globals : 4.503 \tavg rewards : 5.490,\tavg loss: : 0.203693,\tbuffer size : 1054,\tepsilon : 15.8%, \t r <=40 0.0, \t r > 40 54.90196078431373\n","[Episode 200]\t rewards globals : 4.577 \tavg rewards : 4.902,\tavg loss: : 0.169090,\tbuffer size : 1391,\tepsilon : 10.9%, \t r <=40 0.0, \t r > 40 49.01960784313725\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 7.4\n","[task_2_tmax40] 100 run(s) avg rewards : 7.2\n","Point: 7.300000000000001\n","[Episode 250]\t rewards globals : 4.940 \tavg rewards : 6.471,\tavg loss: : 0.164196,\tbuffer size : 1754,\tepsilon : 7.6%, \t r <=40 0.0, \t r > 40 64.70588235294117\n","[Episode 300]\t rewards globals : 5.316 \tavg rewards : 7.059,\tavg loss: : 0.152278,\tbuffer size : 2138,\tepsilon : 5.4%, \t r <=40 0.0, \t r > 40 70.58823529411765\n","[Episode 350]\t rewards globals : 5.755 \tavg rewards : 8.431,\tavg loss: : 0.140841,\tbuffer size : 2578,\tepsilon : 4.0%, \t r <=40 0.0, \t r > 40 84.31372549019608\n","[Episode 400]\t rewards globals : 5.910 \tavg rewards : 7.059,\tavg loss: : 0.133680,\tbuffer size : 2940,\tepsilon : 3.0%, \t r <=40 0.0, \t r > 40 70.58823529411765\n","[Episode 450]\t rewards globals : 6.142 \tavg rewards : 8.039,\tavg loss: : 0.130306,\tbuffer size : 3335,\tepsilon : 2.3%, \t r <=40 0.0, \t r > 40 80.3921568627451\n","error while saving final\n","device is: cuda\n","config : \n"," x debut 15, y_debut 1, max ep 900, max epsilon 0.3333333333333333 \n"," Epsilon_decay 225, test_interval 450\n","Save_interval 1000, batch_size 64, buffer_limit 12000 \n"," Methode Mixed Monte Carlo + Atari DQN, gamma_nstep 0.5, nstep 3\n","Start learning from : x_debut 15 y_debut 1\n","[Episode 50]\t rewards globals : 4.706 \tavg rewards : 4.706,\tavg loss: : 0.954041,\tbuffer size : 358,\tepsilon : 26.9%, \t r <=40 0.0, \t r > 40 47.05882352941176\n","[Episode 100]\t rewards globals : 4.950 \tavg rewards : 5.098,\tavg loss: : 0.188457,\tbuffer size : 724,\tepsilon : 21.7%, \t r <=40 0.0, \t r > 40 50.98039215686274\n","[Episode 150]\t rewards globals : 4.702 \tavg rewards : 4.118,\tavg loss: : 0.145327,\tbuffer size : 1065,\tepsilon : 17.6%, \t r <=40 0.0, \t r > 40 41.17647058823529\n","[Episode 200]\t rewards globals : 4.975 \tavg rewards : 5.686,\tavg loss: : 0.125232,\tbuffer size : 1428,\tepsilon : 14.3%, \t r <=40 0.0, \t r > 40 56.86274509803921\n","[Episode 250]\t rewards globals : 5.378 \tavg rewards : 6.863,\tavg loss: : 0.118226,\tbuffer size : 1799,\tepsilon : 11.6%, \t r <=40 0.0, \t r > 40 68.62745098039215\n","[Episode 300]\t rewards globals : 5.814 \tavg rewards : 7.843,\tavg loss: : 0.106510,\tbuffer size : 2172,\tepsilon : 9.5%, \t r <=40 0.0, \t r > 40 78.43137254901961\n","[Episode 350]\t rewards globals : 5.954 \tavg rewards : 6.863,\tavg loss: : 0.098000,\tbuffer size : 2527,\tepsilon : 7.8%, \t r <=40 0.0, \t r > 40 68.62745098039215\n","[Episode 400]\t rewards globals : 6.135 \tavg rewards : 7.255,\tavg loss: : 0.095226,\tbuffer size : 2886,\tepsilon : 6.5%, \t r <=40 0.0, \t r > 40 72.54901960784314\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 8.5\n","[task_2_tmax40] 100 run(s) avg rewards : 8.5\n","Point: 8.5\n","[Episode 450]\t rewards globals : 6.297 \tavg rewards : 7.647,\tavg loss: : 0.096456,\tbuffer size : 3252,\tepsilon : 5.4%, \t r <=40 0.0, \t r > 40 76.47058823529412\n","[Episode 500]\t rewards globals : 6.467 \tavg rewards : 7.843,\tavg loss: : 0.093530,\tbuffer size : 3611,\tepsilon : 4.5%, \t r <=40 0.0, \t r > 40 78.43137254901961\n","[Episode 550]\t rewards globals : 6.624 \tavg rewards : 8.235,\tavg loss: : 0.090185,\tbuffer size : 3953,\tepsilon : 3.8%, \t r <=40 0.0, \t r > 40 82.35294117647058\n","[Episode 600]\t rewards globals : 6.739 \tavg rewards : 8.039,\tavg loss: : 0.089721,\tbuffer size : 4336,\tepsilon : 3.2%, \t r <=40 0.0, \t r > 40 80.3921568627451\n","[Episode 650]\t rewards globals : 6.805 \tavg rewards : 7.647,\tavg loss: : 0.092621,\tbuffer size : 4692,\tepsilon : 2.8%, \t r <=40 0.0, \t r > 40 76.47058823529412\n","[Episode 700]\t rewards globals : 6.890 \tavg rewards : 8.039,\tavg loss: : 0.090692,\tbuffer size : 5065,\tepsilon : 2.4%, \t r <=40 0.0, \t r > 40 80.3921568627451\n","[Episode 750]\t rewards globals : 6.991 \tavg rewards : 8.235,\tavg loss: : 0.088327,\tbuffer size : 5419,\tepsilon : 2.2%, \t r <=40 0.0, \t r > 40 82.35294117647058\n","[Episode 800]\t rewards globals : 7.079 \tavg rewards : 8.235,\tavg loss: : 0.086801,\tbuffer size : 5792,\tepsilon : 1.9%, \t r <=40 0.0, \t r > 40 82.35294117647058\n","[Episode 850]\t rewards globals : 7.192 \tavg rewards : 9.020,\tavg loss: : 0.087321,\tbuffer size : 6146,\tepsilon : 1.7%, \t r <=40 0.0, \t r > 40 90.19607843137256\n","error while saving final\n","device is: cuda\n","config : \n"," x debut 15, y_debut 2, max ep 1300, max epsilon 0.3 \n"," Epsilon_decay 325, test_interval 500\n","Save_interval 1000, batch_size 64, buffer_limit 16000 \n"," Methode Mixed Monte Carlo + Atari DQN, gamma_nstep 0.5, nstep 3\n","Start learning from : x_debut 15 y_debut 2\n","[Episode 50]\t rewards globals : 5.294 \tavg rewards : 5.294,\tavg loss: : 0.443577,\tbuffer size : 422,\tepsilon : 25.9%, \t r <=40 0.0, \t r > 40 52.94117647058824\n","[Episode 100]\t rewards globals : 5.050 \tavg rewards : 4.706,\tavg loss: : 0.137557,\tbuffer size : 787,\tepsilon : 22.3%, \t r <=40 0.0, \t r > 40 47.05882352941176\n","[Episode 150]\t rewards globals : 5.166 \tavg rewards : 5.294,\tavg loss: : 0.095744,\tbuffer size : 1163,\tepsilon : 19.3%, \t r <=40 0.0, \t r > 40 52.94117647058824\n","[Episode 200]\t rewards globals : 5.274 \tavg rewards : 5.490,\tavg loss: : 0.084543,\tbuffer size : 1567,\tepsilon : 16.7%, \t r <=40 0.0, \t r > 40 54.90196078431373\n","[Episode 250]\t rewards globals : 5.378 \tavg rewards : 5.882,\tavg loss: : 0.084140,\tbuffer size : 1926,\tepsilon : 14.4%, \t r <=40 0.0, \t r > 40 58.82352941176471\n","[Episode 300]\t rewards globals : 5.581 \tavg rewards : 6.667,\tavg loss: : 0.079696,\tbuffer size : 2292,\tepsilon : 12.5%, \t r <=40 0.0, \t r > 40 66.66666666666666\n","[Episode 350]\t rewards globals : 5.556 \tavg rewards : 5.294,\tavg loss: : 0.077940,\tbuffer size : 2658,\tepsilon : 10.9%, \t r <=40 0.0, \t r > 40 52.94117647058824\n","[Episode 400]\t rewards globals : 5.786 \tavg rewards : 7.255,\tavg loss: : 0.075492,\tbuffer size : 3053,\tepsilon : 9.5%, \t r <=40 0.0, \t r > 40 72.54901960784314\n","[Episode 450]\t rewards globals : 5.854 \tavg rewards : 6.471,\tavg loss: : 0.078453,\tbuffer size : 3452,\tepsilon : 8.3%, \t r <=40 0.0, \t r > 40 64.70588235294117\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 7.2\n","[task_2_tmax40] 100 run(s) avg rewards : 8.0\n","Point: 7.6\n","[Episode 500]\t rewards globals : 5.968 \tavg rewards : 7.059,\tavg loss: : 0.075244,\tbuffer size : 3802,\tepsilon : 7.2%, \t r <=40 0.0, \t r > 40 70.58823529411765\n","[Episode 550]\t rewards globals : 6.080 \tavg rewards : 7.255,\tavg loss: : 0.074280,\tbuffer size : 4164,\tepsilon : 6.3%, \t r <=40 0.0, \t r > 40 72.54901960784314\n","[Episode 600]\t rewards globals : 6.256 \tavg rewards : 8.039,\tavg loss: : 0.072446,\tbuffer size : 4553,\tepsilon : 5.6%, \t r <=40 0.0, \t r > 40 80.3921568627451\n","[Episode 650]\t rewards globals : 6.329 \tavg rewards : 7.255,\tavg loss: : 0.074188,\tbuffer size : 4900,\tepsilon : 4.9%, \t r <=40 0.0, \t r > 40 72.54901960784314\n","[Episode 700]\t rewards globals : 6.305 \tavg rewards : 5.882,\tavg loss: : 0.074300,\tbuffer size : 5227,\tepsilon : 4.4%, \t r <=40 0.0, \t r > 40 58.82352941176471\n","[Episode 750]\t rewards globals : 6.325 \tavg rewards : 6.667,\tavg loss: : 0.073292,\tbuffer size : 5602,\tepsilon : 3.9%, \t r <=40 0.0, \t r > 40 66.66666666666666\n","[Episode 800]\t rewards globals : 6.442 \tavg rewards : 8.235,\tavg loss: : 0.071224,\tbuffer size : 5993,\tepsilon : 3.5%, \t r <=40 0.0, \t r > 40 82.35294117647058\n","[Episode 850]\t rewards globals : 6.475 \tavg rewards : 7.059,\tavg loss: : 0.073842,\tbuffer size : 6372,\tepsilon : 3.1%, \t r <=40 0.0, \t r > 40 70.58823529411765\n","[Episode 900]\t rewards globals : 6.526 \tavg rewards : 7.451,\tavg loss: : 0.072600,\tbuffer size : 6764,\tepsilon : 2.8%, \t r <=40 0.0, \t r > 40 74.50980392156863\n","[Episode 950]\t rewards globals : 6.583 \tavg rewards : 7.647,\tavg loss: : 0.071258,\tbuffer size : 7132,\tepsilon : 2.6%, \t r <=40 0.0, \t r > 40 76.47058823529412\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 8.5\n","[task_2_tmax40] 100 run(s) avg rewards : 7.9\n","Point: 8.2\n","[Episode 1000]\t rewards globals : 6.663 \tavg rewards : 8.039,\tavg loss: : 0.070187,\tbuffer size : 7495,\tepsilon : 2.3%, \t r <=40 0.0, \t r > 40 80.3921568627451\n","[Episode 1050]\t rewards globals : 6.708 \tavg rewards : 7.647,\tavg loss: : 0.070834,\tbuffer size : 7876,\tepsilon : 2.1%, \t r <=40 0.0, \t r > 40 76.47058823529412\n","[Episode 1100]\t rewards globals : 6.748 \tavg rewards : 7.647,\tavg loss: : 0.070370,\tbuffer size : 8246,\tepsilon : 2.0%, \t r <=40 0.0, \t r > 40 76.47058823529412\n","[Episode 1150]\t rewards globals : 6.811 \tavg rewards : 8.235,\tavg loss: : 0.069657,\tbuffer size : 8619,\tepsilon : 1.8%, \t r <=40 0.0, \t r > 40 82.35294117647058\n","[Episode 1200]\t rewards globals : 6.886 \tavg rewards : 8.627,\tavg loss: : 0.068382,\tbuffer size : 9010,\tepsilon : 1.7%, \t r <=40 0.0, \t r > 40 86.27450980392157\n","[Episode 1250]\t rewards globals : 6.954 \tavg rewards : 8.627,\tavg loss: : 0.068969,\tbuffer size : 9416,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 86.27450980392157\n","error while saving final\n","device is: cuda\n","config : \n"," x debut 20, y_debut 0, max ep 750, max epsilon 0.3333333333333333 \n"," Epsilon_decay 187, test_interval 375\n","Save_interval 1000, batch_size 64, buffer_limit 12000 \n"," Methode Mixed Monte Carlo + Atari DQN, gamma_nstep 0.5, nstep 3\n","Start learning from : x_debut 20 y_debut 0\n","[Episode 50]\t rewards globals : 4.902 \tavg rewards : 4.902,\tavg loss: : 0.435800,\tbuffer size : 416,\tepsilon : 25.7%, \t r <=40 0.0, \t r > 40 49.01960784313725\n","[Episode 100]\t rewards globals : 5.545 \tavg rewards : 6.078,\tavg loss: : 0.134580,\tbuffer size : 820,\tepsilon : 19.9%, \t r <=40 0.0, \t r > 40 60.78431372549019\n","[Episode 150]\t rewards globals : 5.695 \tavg rewards : 6.078,\tavg loss: : 0.098461,\tbuffer size : 1302,\tepsilon : 15.5%, \t r <=40 0.0, \t r > 40 60.78431372549019\n","[Episode 200]\t rewards globals : 6.169 \tavg rewards : 7.647,\tavg loss: : 0.079780,\tbuffer size : 1794,\tepsilon : 12.1%, \t r <=40 0.0, \t r > 40 76.47058823529412\n","[Episode 250]\t rewards globals : 6.454 \tavg rewards : 7.451,\tavg loss: : 0.076045,\tbuffer size : 2293,\tepsilon : 9.5%, \t r <=40 0.0, \t r > 40 74.50980392156863\n","[Episode 300]\t rewards globals : 6.777 \tavg rewards : 8.431,\tavg loss: : 0.067991,\tbuffer size : 2826,\tepsilon : 7.5%, \t r <=40 0.0, \t r > 40 84.31372549019608\n","[Episode 350]\t rewards globals : 7.094 \tavg rewards : 9.020,\tavg loss: : 0.063000,\tbuffer size : 3352,\tepsilon : 6.0%, \t r <=40 0.0, \t r > 40 90.19607843137256\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 9.3\n","[task_2_tmax40] 100 run(s) avg rewards : 9.6\n","Point: 9.45\n","Re do test: double check\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 9.6\n","[task_2_tmax40] 100 run(s) avg rewards : 9.6\n","Point: 9.6\n","Training should stop\n","error while saving final\n","device is: cuda\n","config : \n"," x debut 20, y_debut 1, max ep 1150, max epsilon 0.3 \n"," Epsilon_decay 287, test_interval 500\n","Save_interval 1000, batch_size 64, buffer_limit 16000 \n"," Methode Mixed Monte Carlo + Atari DQN, gamma_nstep 0.5, nstep 3\n","Start learning from : x_debut 20 y_debut 1\n","[Episode 50]\t rewards globals : 5.882 \tavg rewards : 5.882,\tavg loss: : 0.268968,\tbuffer size : 414,\tepsilon : 25.4%, \t r <=40 0.0, \t r > 40 58.82352941176471\n","[Episode 100]\t rewards globals : 6.139 \tavg rewards : 6.471,\tavg loss: : 0.096376,\tbuffer size : 877,\tepsilon : 21.5%, \t r <=40 0.0, \t r > 40 64.70588235294117\n","[Episode 150]\t rewards globals : 6.159 \tavg rewards : 6.078,\tavg loss: : 0.072942,\tbuffer size : 1331,\tepsilon : 18.2%, \t r <=40 0.0, \t r > 40 60.78431372549019\n","[Episode 200]\t rewards globals : 6.269 \tavg rewards : 6.667,\tavg loss: : 0.065496,\tbuffer size : 1797,\tepsilon : 15.4%, \t r <=40 0.0, \t r > 40 66.66666666666666\n","[Episode 250]\t rewards globals : 6.494 \tavg rewards : 7.451,\tavg loss: : 0.064778,\tbuffer size : 2269,\tepsilon : 13.1%, \t r <=40 0.0, \t r > 40 74.50980392156863\n","[Episode 300]\t rewards globals : 6.545 \tavg rewards : 6.863,\tavg loss: : 0.059468,\tbuffer size : 2677,\tepsilon : 11.2%, \t r <=40 0.0, \t r > 40 68.62745098039215\n","[Episode 350]\t rewards globals : 6.809 \tavg rewards : 8.431,\tavg loss: : 0.056125,\tbuffer size : 3166,\tepsilon : 9.6%, \t r <=40 0.0, \t r > 40 84.31372549019608\n","[Episode 400]\t rewards globals : 6.908 \tavg rewards : 7.647,\tavg loss: : 0.053202,\tbuffer size : 3607,\tepsilon : 8.2%, \t r <=40 0.0, \t r > 40 76.47058823529412\n","[Episode 450]\t rewards globals : 7.051 \tavg rewards : 8.235,\tavg loss: : 0.054152,\tbuffer size : 4047,\tepsilon : 7.0%, \t r <=40 0.0, \t r > 40 82.35294117647058\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 8.8\n","[task_2_tmax40] 100 run(s) avg rewards : 8.9\n","Point: 8.850000000000001\n","Re do test: double check\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 8.8\n","[task_2_tmax40] 100 run(s) avg rewards : 9.1\n","Point: 8.95\n","Training should stop\n","error while saving final\n","device is: cuda\n","config : \n"," x debut 20, y_debut 2, max ep 1550, max epsilon 0.3 \n"," Epsilon_decay 387, test_interval 500\n","Save_interval 1000, batch_size 64, buffer_limit 20000 \n"," Methode Mixed Monte Carlo + Atari DQN, gamma_nstep 0.5, nstep 3\n","Start learning from : x_debut 20 y_debut 2\n","[Episode 50]\t rewards globals : 3.725 \tavg rewards : 3.725,\tavg loss: : 0.413116,\tbuffer size : 414,\tepsilon : 26.5%, \t r <=40 0.0, \t r > 40 37.254901960784316\n","[Episode 100]\t rewards globals : 4.356 \tavg rewards : 5.098,\tavg loss: : 0.118256,\tbuffer size : 840,\tepsilon : 23.4%, \t r <=40 0.0, \t r > 40 50.98039215686274\n","[Episode 150]\t rewards globals : 4.437 \tavg rewards : 4.706,\tavg loss: : 0.089808,\tbuffer size : 1265,\tepsilon : 20.7%, \t r <=40 0.0, \t r > 40 47.05882352941176\n","[Episode 200]\t rewards globals : 4.876 \tavg rewards : 6.078,\tavg loss: : 0.075654,\tbuffer size : 1697,\tepsilon : 18.3%, \t r <=40 0.0, \t r > 40 60.78431372549019\n","[Episode 250]\t rewards globals : 5.100 \tavg rewards : 6.078,\tavg loss: : 0.074788,\tbuffer size : 2149,\tepsilon : 16.2%, \t r <=40 0.0, \t r > 40 60.78431372549019\n","[Episode 300]\t rewards globals : 5.150 \tavg rewards : 5.490,\tavg loss: : 0.071990,\tbuffer size : 2613,\tepsilon : 14.4%, \t r <=40 0.0, \t r > 40 54.90196078431373\n","[Episode 350]\t rewards globals : 5.299 \tavg rewards : 6.275,\tavg loss: : 0.067906,\tbuffer size : 3071,\tepsilon : 12.7%, \t r <=40 0.0, \t r > 40 62.745098039215684\n","[Episode 400]\t rewards globals : 5.387 \tavg rewards : 6.078,\tavg loss: : 0.064610,\tbuffer size : 3483,\tepsilon : 11.3%, \t r <=40 0.0, \t r > 40 60.78431372549019\n","[Episode 450]\t rewards globals : 5.565 \tavg rewards : 7.059,\tavg loss: : 0.064000,\tbuffer size : 3931,\tepsilon : 10.1%, \t r <=40 0.0, \t r > 40 70.58823529411765\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 8.2\n","[task_2_tmax40] 100 run(s) avg rewards : 8.2\n","Point: 8.2\n","[Episode 500]\t rewards globals : 5.649 \tavg rewards : 6.471,\tavg loss: : 0.062623,\tbuffer size : 4409,\tepsilon : 9.0%, \t r <=40 0.0, \t r > 40 64.70588235294117\n","[Episode 550]\t rewards globals : 5.699 \tavg rewards : 6.078,\tavg loss: : 0.061555,\tbuffer size : 4862,\tepsilon : 8.0%, \t r <=40 0.0, \t r > 40 60.78431372549019\n","[Episode 600]\t rewards globals : 5.724 \tavg rewards : 6.078,\tavg loss: : 0.061138,\tbuffer size : 5312,\tepsilon : 7.2%, \t r <=40 0.0, \t r > 40 60.78431372549019\n","[Episode 650]\t rewards globals : 5.776 \tavg rewards : 6.471,\tavg loss: : 0.062045,\tbuffer size : 5752,\tepsilon : 6.4%, \t r <=40 0.0, \t r > 40 64.70588235294117\n","[Episode 700]\t rewards globals : 5.920 \tavg rewards : 7.843,\tavg loss: : 0.062040,\tbuffer size : 6232,\tepsilon : 5.8%, \t r <=40 0.0, \t r > 40 78.43137254901961\n","[Episode 750]\t rewards globals : 5.979 \tavg rewards : 6.863,\tavg loss: : 0.061724,\tbuffer size : 6694,\tepsilon : 5.2%, \t r <=40 0.0, \t r > 40 68.62745098039215\n","[Episode 800]\t rewards globals : 6.105 \tavg rewards : 8.039,\tavg loss: : 0.060679,\tbuffer size : 7208,\tepsilon : 4.7%, \t r <=40 0.0, \t r > 40 80.3921568627451\n","[Episode 850]\t rewards globals : 6.193 \tavg rewards : 7.647,\tavg loss: : 0.061123,\tbuffer size : 7679,\tepsilon : 4.2%, \t r <=40 0.0, \t r > 40 76.47058823529412\n","[Episode 900]\t rewards globals : 6.282 \tavg rewards : 7.647,\tavg loss: : 0.060666,\tbuffer size : 8172,\tepsilon : 3.8%, \t r <=40 0.0, \t r > 40 76.47058823529412\n","[Episode 950]\t rewards globals : 6.393 \tavg rewards : 8.431,\tavg loss: : 0.060378,\tbuffer size : 8665,\tepsilon : 3.5%, \t r <=40 0.0, \t r > 40 84.31372549019608\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 8.9\n","[task_2_tmax40] 100 run(s) avg rewards : 8.1\n","Point: 8.5\n","[Episode 1000]\t rewards globals : 6.454 \tavg rewards : 7.647,\tavg loss: : 0.059652,\tbuffer size : 9124,\tepsilon : 3.2%, \t r <=40 0.0, \t r > 40 76.47058823529412\n","[Episode 1050]\t rewards globals : 6.546 \tavg rewards : 8.235,\tavg loss: : 0.060398,\tbuffer size : 9604,\tepsilon : 2.9%, \t r <=40 0.0, \t r > 40 82.35294117647058\n","[Episode 1100]\t rewards globals : 6.612 \tavg rewards : 8.039,\tavg loss: : 0.060256,\tbuffer size : 10104,\tepsilon : 2.7%, \t r <=40 0.0, \t r > 40 80.3921568627451\n","[Episode 1150]\t rewards globals : 6.690 \tavg rewards : 8.431,\tavg loss: : 0.059378,\tbuffer size : 10594,\tepsilon : 2.5%, \t r <=40 0.0, \t r > 40 84.31372549019608\n","[Episode 1200]\t rewards globals : 6.736 \tavg rewards : 7.843,\tavg loss: : 0.058726,\tbuffer size : 11050,\tepsilon : 2.3%, \t r <=40 0.0, \t r > 40 78.43137254901961\n","[Episode 1250]\t rewards globals : 6.771 \tavg rewards : 7.647,\tavg loss: : 0.059513,\tbuffer size : 11528,\tepsilon : 2.1%, \t r <=40 0.0, \t r > 40 76.47058823529412\n","[Episode 1300]\t rewards globals : 6.864 \tavg rewards : 9.216,\tavg loss: : 0.059597,\tbuffer size : 12022,\tepsilon : 2.0%, \t r <=40 0.0, \t r > 40 92.15686274509804\n","[Episode 1350]\t rewards globals : 6.921 \tavg rewards : 8.431,\tavg loss: : 0.058905,\tbuffer size : 12483,\tepsilon : 1.9%, \t r <=40 0.0, \t r > 40 84.31372549019608\n","[Episode 1400]\t rewards globals : 6.938 \tavg rewards : 7.451,\tavg loss: : 0.058810,\tbuffer size : 12953,\tepsilon : 1.8%, \t r <=40 0.0, \t r > 40 74.50980392156863\n","[Episode 1450]\t rewards globals : 6.975 \tavg rewards : 7.843,\tavg loss: : 0.059089,\tbuffer size : 13404,\tepsilon : 1.7%, \t r <=40 0.0, \t r > 40 78.43137254901961\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 8.7\n","[task_2_tmax40] 100 run(s) avg rewards : 8.1\n","Point: 8.399999999999999\n","[Episode 1500]\t rewards globals : 7.015 \tavg rewards : 8.235,\tavg loss: : 0.059155,\tbuffer size : 13891,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 82.35294117647058\n","error while saving final\n","device is: cuda\n","config : \n"," x debut 20, y_debut 3, max ep 1950, max epsilon 0.3 \n"," Epsilon_decay 487, test_interval 500\n","Save_interval 1000, batch_size 64, buffer_limit 24000 \n"," Methode Mixed Monte Carlo + Atari DQN, gamma_nstep 0.5, nstep 3\n","Start learning from : x_debut 20 y_debut 3\n","[Episode 50]\t rewards globals : 4.118 \tavg rewards : 4.118,\tavg loss: : 0.273157,\tbuffer size : 426,\tepsilon : 27.2%, \t r <=40 0.0, \t r > 40 41.17647058823529\n","[Episode 100]\t rewards globals : 3.663 \tavg rewards : 3.333,\tavg loss: : 0.091330,\tbuffer size : 856,\tepsilon : 24.6%, \t r <=40 0.0, \t r > 40 33.33333333333333\n","[Episode 150]\t rewards globals : 3.709 \tavg rewards : 3.725,\tavg loss: : 0.069581,\tbuffer size : 1318,\tepsilon : 22.3%, \t r <=40 0.0, \t r > 40 37.254901960784316\n","[Episode 200]\t rewards globals : 4.179 \tavg rewards : 5.686,\tavg loss: : 0.060949,\tbuffer size : 1789,\tepsilon : 20.2%, \t r <=40 0.0, \t r > 40 56.86274509803921\n","[Episode 250]\t rewards globals : 4.024 \tavg rewards : 3.333,\tavg loss: : 0.065129,\tbuffer size : 2277,\tepsilon : 18.4%, \t r <=40 0.0, \t r > 40 33.33333333333333\n","[Episode 300]\t rewards globals : 4.219 \tavg rewards : 5.098,\tavg loss: : 0.061664,\tbuffer size : 2744,\tepsilon : 16.7%, \t r <=40 0.0, \t r > 40 50.98039215686274\n","[Episode 350]\t rewards globals : 4.217 \tavg rewards : 4.118,\tavg loss: : 0.058367,\tbuffer size : 3213,\tepsilon : 15.1%, \t r <=40 0.0, \t r > 40 41.17647058823529\n","[Episode 400]\t rewards globals : 4.339 \tavg rewards : 5.098,\tavg loss: : 0.055750,\tbuffer size : 3654,\tepsilon : 13.8%, \t r <=40 0.0, \t r > 40 50.98039215686274\n","[Episode 450]\t rewards globals : 4.479 \tavg rewards : 5.490,\tavg loss: : 0.057227,\tbuffer size : 4108,\tepsilon : 12.5%, \t r <=40 0.0, \t r > 40 54.90196078431373\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 7.2\n","[task_2_tmax40] 100 run(s) avg rewards : 7.4\n","Point: 7.300000000000001\n","[Episode 500]\t rewards globals : 4.671 \tavg rewards : 6.471,\tavg loss: : 0.055196,\tbuffer size : 4581,\tepsilon : 11.4%, \t r <=40 0.0, \t r > 40 64.70588235294117\n","[Episode 550]\t rewards globals : 4.664 \tavg rewards : 4.706,\tavg loss: : 0.054932,\tbuffer size : 5007,\tepsilon : 10.4%, \t r <=40 0.0, \t r > 40 47.05882352941176\n","[Episode 600]\t rewards globals : 4.742 \tavg rewards : 5.490,\tavg loss: : 0.054618,\tbuffer size : 5464,\tepsilon : 9.5%, \t r <=40 0.0, \t r > 40 54.90196078431373\n","[Episode 650]\t rewards globals : 4.839 \tavg rewards : 5.882,\tavg loss: : 0.056128,\tbuffer size : 5944,\tepsilon : 8.6%, \t r <=40 0.0, \t r > 40 58.82352941176471\n","[Episode 700]\t rewards globals : 4.979 \tavg rewards : 6.667,\tavg loss: : 0.056183,\tbuffer size : 6424,\tepsilon : 7.9%, \t r <=40 0.0, \t r > 40 66.66666666666666\n","[Episode 750]\t rewards globals : 5.020 \tavg rewards : 5.490,\tavg loss: : 0.056042,\tbuffer size : 6924,\tepsilon : 7.2%, \t r <=40 0.0, \t r > 40 54.90196078431373\n","[Episode 800]\t rewards globals : 5.119 \tavg rewards : 6.471,\tavg loss: : 0.055580,\tbuffer size : 7401,\tepsilon : 6.6%, \t r <=40 0.0, \t r > 40 64.70588235294117\n","[Episode 850]\t rewards globals : 5.159 \tavg rewards : 5.882,\tavg loss: : 0.056862,\tbuffer size : 7872,\tepsilon : 6.1%, \t r <=40 0.0, \t r > 40 58.82352941176471\n","[Episode 900]\t rewards globals : 5.316 \tavg rewards : 8.039,\tavg loss: : 0.056284,\tbuffer size : 8353,\tepsilon : 5.6%, \t r <=40 0.0, \t r > 40 80.3921568627451\n","[Episode 950]\t rewards globals : 5.447 \tavg rewards : 7.647,\tavg loss: : 0.055776,\tbuffer size : 8854,\tepsilon : 5.1%, \t r <=40 0.0, \t r > 40 76.47058823529412\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 7.3\n","[task_2_tmax40] 100 run(s) avg rewards : 7.1\n","Point: 7.199999999999999\n","[Episode 1000]\t rewards globals : 5.534 \tavg rewards : 7.255,\tavg loss: : 0.055891,\tbuffer size : 9331,\tepsilon : 4.7%, \t r <=40 0.0, \t r > 40 72.54901960784314\n","[Episode 1050]\t rewards globals : 5.623 \tavg rewards : 7.451,\tavg loss: : 0.056681,\tbuffer size : 9833,\tepsilon : 4.4%, \t r <=40 0.0, \t r > 40 74.50980392156863\n","[Episode 1100]\t rewards globals : 5.686 \tavg rewards : 6.863,\tavg loss: : 0.057012,\tbuffer size : 10322,\tepsilon : 4.0%, \t r <=40 0.0, \t r > 40 68.62745098039215\n","[Episode 1150]\t rewards globals : 5.786 \tavg rewards : 7.843,\tavg loss: : 0.056667,\tbuffer size : 10798,\tepsilon : 3.7%, \t r <=40 0.0, \t r > 40 78.43137254901961\n","[Episode 1200]\t rewards globals : 5.803 \tavg rewards : 6.275,\tavg loss: : 0.056491,\tbuffer size : 11280,\tepsilon : 3.5%, \t r <=40 0.0, \t r > 40 62.745098039215684\n","[Episode 1250]\t rewards globals : 5.883 \tavg rewards : 7.843,\tavg loss: : 0.057348,\tbuffer size : 11763,\tepsilon : 3.2%, \t r <=40 0.0, \t r > 40 78.43137254901961\n","[Episode 1300]\t rewards globals : 5.926 \tavg rewards : 7.059,\tavg loss: : 0.057060,\tbuffer size : 12233,\tepsilon : 3.0%, \t r <=40 0.0, \t r > 40 70.58823529411765\n","[Episode 1350]\t rewards globals : 6.018 \tavg rewards : 8.235,\tavg loss: : 0.056694,\tbuffer size : 12706,\tepsilon : 2.8%, \t r <=40 0.0, \t r > 40 82.35294117647058\n","[Episode 1400]\t rewards globals : 6.089 \tavg rewards : 8.039,\tavg loss: : 0.056465,\tbuffer size : 13189,\tepsilon : 2.6%, \t r <=40 0.0, \t r > 40 80.3921568627451\n","[Episode 1450]\t rewards globals : 6.079 \tavg rewards : 5.882,\tavg loss: : 0.057590,\tbuffer size : 13660,\tepsilon : 2.5%, \t r <=40 0.0, \t r > 40 58.82352941176471\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 7.5\n","[task_2_tmax40] 100 run(s) avg rewards : 6.9\n","Point: 7.2\n","[Episode 1500]\t rewards globals : 6.096 \tavg rewards : 6.471,\tavg loss: : 0.058462,\tbuffer size : 14130,\tepsilon : 2.3%, \t r <=40 0.0, \t r > 40 64.70588235294117\n","[Episode 1550]\t rewards globals : 6.144 \tavg rewards : 7.647,\tavg loss: : 0.058044,\tbuffer size : 14619,\tepsilon : 2.2%, \t r <=40 0.0, \t r > 40 76.47058823529412\n","[Episode 1600]\t rewards globals : 6.171 \tavg rewards : 7.059,\tavg loss: : 0.057971,\tbuffer size : 15100,\tepsilon : 2.1%, \t r <=40 0.0, \t r > 40 70.58823529411765\n","[Episode 1650]\t rewards globals : 6.214 \tavg rewards : 7.647,\tavg loss: : 0.058389,\tbuffer size : 15577,\tepsilon : 2.0%, \t r <=40 0.0, \t r > 40 76.47058823529412\n","[Episode 1700]\t rewards globals : 6.232 \tavg rewards : 6.863,\tavg loss: : 0.058770,\tbuffer size : 16056,\tepsilon : 1.9%, \t r <=40 0.0, \t r > 40 68.62745098039215\n","[Episode 1750]\t rewards globals : 6.276 \tavg rewards : 7.843,\tavg loss: : 0.059022,\tbuffer size : 16540,\tepsilon : 1.8%, \t r <=40 0.0, \t r > 40 78.43137254901961\n","[Episode 1800]\t rewards globals : 6.285 \tavg rewards : 6.667,\tavg loss: : 0.058849,\tbuffer size : 17003,\tepsilon : 1.7%, \t r <=40 0.0, \t r > 40 66.66666666666666\n","[Episode 1850]\t rewards globals : 6.316 \tavg rewards : 7.451,\tavg loss: : 0.059605,\tbuffer size : 17484,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 74.50980392156863\n","[Episode 1900]\t rewards globals : 6.339 \tavg rewards : 7.059,\tavg loss: : 0.059693,\tbuffer size : 17974,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 70.58823529411765\n","error while saving final\n","device is: cuda\n","config : \n"," x debut 25, y_debut 0, max ep 1000, max epsilon 0.3 \n"," Epsilon_decay 250, test_interval 500\n","Save_interval 1000, batch_size 64, buffer_limit 16000 \n"," Methode Mixed Monte Carlo + Atari DQN, gamma_nstep 0.5, nstep 3\n","Start learning from : x_debut 25 y_debut 0\n","[Episode 50]\t rewards globals : 6.078 \tavg rewards : 6.078,\tavg loss: : 0.270220,\tbuffer size : 573,\tepsilon : 24.7%, \t r <=40 0.0, \t r > 40 60.78431372549019\n","[Episode 100]\t rewards globals : 5.644 \tavg rewards : 5.098,\tavg loss: : 0.131978,\tbuffer size : 1118,\tepsilon : 20.4%, \t r <=40 0.0, \t r > 40 50.98039215686274\n","[Episode 150]\t rewards globals : 5.960 \tavg rewards : 6.471,\tavg loss: : 0.097576,\tbuffer size : 1712,\tepsilon : 16.9%, \t r <=40 0.0, \t r > 40 64.70588235294117\n","[Episode 200]\t rewards globals : 6.070 \tavg rewards : 6.275,\tavg loss: : 0.085854,\tbuffer size : 2286,\tepsilon : 14.0%, \t r <=40 0.0, \t r > 40 62.745098039215684\n","[Episode 250]\t rewards globals : 6.056 \tavg rewards : 6.078,\tavg loss: : 0.083332,\tbuffer size : 2856,\tepsilon : 11.7%, \t r <=40 0.0, \t r > 40 60.78431372549019\n","[Episode 300]\t rewards globals : 6.080 \tavg rewards : 6.275,\tavg loss: : 0.076315,\tbuffer size : 3408,\tepsilon : 9.7%, \t r <=40 0.0, \t r > 40 62.745098039215684\n","[Episode 350]\t rewards globals : 6.296 \tavg rewards : 7.451,\tavg loss: : 0.073101,\tbuffer size : 4070,\tepsilon : 8.2%, \t r <=40 0.0, \t r > 40 74.50980392156863\n","[Episode 400]\t rewards globals : 6.534 \tavg rewards : 8.235,\tavg loss: : 0.068872,\tbuffer size : 4704,\tepsilon : 6.9%, \t r <=40 0.0, \t r > 40 82.35294117647058\n","[Episode 450]\t rewards globals : 6.696 \tavg rewards : 8.039,\tavg loss: : 0.068220,\tbuffer size : 5351,\tepsilon : 5.8%, \t r <=40 0.0, \t r > 40 80.3921568627451\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 9.1\n","[task_2_tmax40] 100 run(s) avg rewards : 9.7\n","Point: 9.399999999999999\n","Re do test: double check\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 9.7\n","[task_2_tmax40] 100 run(s) avg rewards : 9.6\n","Point: 9.649999999999999\n","Training should stop\n","error while saving final\n","device is: cuda\n","config : \n"," x debut 25, y_debut 1, max ep 1400, max epsilon 0.3 \n"," Epsilon_decay 350, test_interval 500\n","Save_interval 1000, batch_size 64, buffer_limit 20000 \n"," Methode Mixed Monte Carlo + Atari DQN, gamma_nstep 0.5, nstep 3\n","Start learning from : x_debut 25 y_debut 1\n","[Episode 50]\t rewards globals : 4.118 \tavg rewards : 4.118,\tavg loss: : 0.237330,\tbuffer size : 490,\tepsilon : 26.1%, \t r <=40 0.0, \t r > 40 41.17647058823529\n","[Episode 100]\t rewards globals : 4.653 \tavg rewards : 5.098,\tavg loss: : 0.110302,\tbuffer size : 1015,\tepsilon : 22.8%, \t r <=40 0.0, \t r > 40 50.98039215686274\n","[Episode 150]\t rewards globals : 4.967 \tavg rewards : 5.490,\tavg loss: : 0.085421,\tbuffer size : 1560,\tepsilon : 19.9%, \t r <=40 0.0, \t r > 40 54.90196078431373\n","[Episode 200]\t rewards globals : 5.224 \tavg rewards : 6.078,\tavg loss: : 0.073395,\tbuffer size : 2099,\tepsilon : 17.4%, \t r <=40 0.0, \t r > 40 60.78431372549019\n","[Episode 250]\t rewards globals : 5.219 \tavg rewards : 5.294,\tavg loss: : 0.072667,\tbuffer size : 2631,\tepsilon : 15.2%, \t r <=40 0.0, \t r > 40 52.94117647058824\n","[Episode 300]\t rewards globals : 5.282 \tavg rewards : 5.490,\tavg loss: : 0.069827,\tbuffer size : 3175,\tepsilon : 13.3%, \t r <=40 0.0, \t r > 40 54.90196078431373\n","[Episode 350]\t rewards globals : 5.556 \tavg rewards : 7.255,\tavg loss: : 0.066583,\tbuffer size : 3768,\tepsilon : 11.7%, \t r <=40 0.0, \t r > 40 72.54901960784314\n","[Episode 400]\t rewards globals : 5.736 \tavg rewards : 6.863,\tavg loss: : 0.063532,\tbuffer size : 4337,\tepsilon : 10.2%, \t r <=40 0.0, \t r > 40 68.62745098039215\n","[Episode 450]\t rewards globals : 5.920 \tavg rewards : 7.451,\tavg loss: : 0.063992,\tbuffer size : 4892,\tepsilon : 9.0%, \t r <=40 0.0, \t r > 40 74.50980392156863\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 9.0\n","[task_2_tmax40] 100 run(s) avg rewards : 9.6\n","Point: 9.3\n","Re do test: double check\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 9.2\n","[task_2_tmax40] 100 run(s) avg rewards : 9.6\n","Point: 9.399999999999999\n","Training should stop\n","error while saving final\n","device is: cuda\n","config : \n"," x debut 25, y_debut 2, max ep 1800, max epsilon 0.3 \n"," Epsilon_decay 450, test_interval 500\n","Save_interval 1000, batch_size 64, buffer_limit 24000 \n"," Methode Mixed Monte Carlo + Atari DQN, gamma_nstep 0.5, nstep 3\n","Start learning from : x_debut 25 y_debut 2\n","[Episode 50]\t rewards globals : 3.137 \tavg rewards : 3.137,\tavg loss: : 0.396833,\tbuffer size : 399,\tepsilon : 27.0%, \t r <=40 0.0, \t r > 40 31.372549019607842\n","[Episode 100]\t rewards globals : 4.158 \tavg rewards : 5.098,\tavg loss: : 0.100754,\tbuffer size : 956,\tepsilon : 24.2%, \t r <=40 0.0, \t r > 40 50.98039215686274\n","[Episode 150]\t rewards globals : 4.702 \tavg rewards : 5.686,\tavg loss: : 0.074364,\tbuffer size : 1528,\tepsilon : 21.8%, \t r <=40 0.0, \t r > 40 56.86274509803921\n","[Episode 200]\t rewards globals : 4.826 \tavg rewards : 5.294,\tavg loss: : 0.068743,\tbuffer size : 2068,\tepsilon : 19.6%, \t r <=40 0.0, \t r > 40 52.94117647058824\n","[Episode 250]\t rewards globals : 4.821 \tavg rewards : 4.902,\tavg loss: : 0.066200,\tbuffer size : 2597,\tepsilon : 17.6%, \t r <=40 0.0, \t r > 40 49.01960784313725\n","[Episode 300]\t rewards globals : 5.116 \tavg rewards : 6.471,\tavg loss: : 0.060030,\tbuffer size : 3147,\tepsilon : 15.9%, \t r <=40 0.0, \t r > 40 64.70588235294117\n","[Episode 350]\t rewards globals : 5.185 \tavg rewards : 5.686,\tavg loss: : 0.057965,\tbuffer size : 3655,\tepsilon : 14.3%, \t r <=40 0.0, \t r > 40 56.86274509803921\n","[Episode 400]\t rewards globals : 5.262 \tavg rewards : 5.686,\tavg loss: : 0.054874,\tbuffer size : 4179,\tepsilon : 12.9%, \t r <=40 0.0, \t r > 40 56.86274509803921\n","[Episode 450]\t rewards globals : 5.477 \tavg rewards : 7.255,\tavg loss: : 0.055724,\tbuffer size : 4773,\tepsilon : 11.7%, \t r <=40 0.0, \t r > 40 72.54901960784314\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 8.3\n","[task_2_tmax40] 100 run(s) avg rewards : 8.7\n","Point: 8.5\n","[Episode 500]\t rewards globals : 5.589 \tavg rewards : 6.471,\tavg loss: : 0.055530,\tbuffer size : 5333,\tepsilon : 10.5%, \t r <=40 0.0, \t r > 40 64.70588235294117\n","[Episode 550]\t rewards globals : 5.789 \tavg rewards : 7.843,\tavg loss: : 0.053898,\tbuffer size : 5893,\tepsilon : 9.5%, \t r <=40 0.0, \t r > 40 78.43137254901961\n","[Episode 600]\t rewards globals : 5.907 \tavg rewards : 7.255,\tavg loss: : 0.052576,\tbuffer size : 6438,\tepsilon : 8.6%, \t r <=40 0.0, \t r > 40 72.54901960784314\n","[Episode 650]\t rewards globals : 6.022 \tavg rewards : 7.451,\tavg loss: : 0.053560,\tbuffer size : 6976,\tepsilon : 7.8%, \t r <=40 0.0, \t r > 40 74.50980392156863\n","[Episode 700]\t rewards globals : 6.134 \tavg rewards : 7.647,\tavg loss: : 0.052384,\tbuffer size : 7534,\tepsilon : 7.1%, \t r <=40 0.0, \t r > 40 76.47058823529412\n","[Episode 750]\t rewards globals : 6.258 \tavg rewards : 7.843,\tavg loss: : 0.051628,\tbuffer size : 8122,\tepsilon : 6.5%, \t r <=40 0.0, \t r > 40 78.43137254901961\n","[Episode 800]\t rewards globals : 6.355 \tavg rewards : 7.843,\tavg loss: : 0.051502,\tbuffer size : 8668,\tepsilon : 5.9%, \t r <=40 0.0, \t r > 40 78.43137254901961\n","[Episode 850]\t rewards globals : 6.439 \tavg rewards : 7.843,\tavg loss: : 0.052024,\tbuffer size : 9240,\tepsilon : 5.4%, \t r <=40 0.0, \t r > 40 78.43137254901961\n","[Episode 900]\t rewards globals : 6.493 \tavg rewards : 7.451,\tavg loss: : 0.051910,\tbuffer size : 9805,\tepsilon : 4.9%, \t r <=40 0.0, \t r > 40 74.50980392156863\n","[Episode 950]\t rewards globals : 6.562 \tavg rewards : 7.843,\tavg loss: : 0.050769,\tbuffer size : 10378,\tepsilon : 4.5%, \t r <=40 0.0, \t r > 40 78.43137254901961\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 8.9\n","[task_2_tmax40] 100 run(s) avg rewards : 8.7\n","Point: 8.8\n","Re do test: double check\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 8.3\n","[task_2_tmax40] 100 run(s) avg rewards : 8.5\n","Point: 8.4\n","[Episode 1000]\t rewards globals : 6.603 \tavg rewards : 7.451,\tavg loss: : 0.050133,\tbuffer size : 10958,\tepsilon : 4.1%, \t r <=40 0.0, \t r > 40 74.50980392156863\n","[Episode 1050]\t rewards globals : 6.641 \tavg rewards : 7.451,\tavg loss: : 0.051026,\tbuffer size : 11548,\tepsilon : 3.8%, \t r <=40 0.0, \t r > 40 74.50980392156863\n","[Episode 1100]\t rewards globals : 6.712 \tavg rewards : 8.235,\tavg loss: : 0.050890,\tbuffer size : 12126,\tepsilon : 3.5%, \t r <=40 0.0, \t r > 40 82.35294117647058\n","[Episode 1150]\t rewards globals : 6.768 \tavg rewards : 8.039,\tavg loss: : 0.050854,\tbuffer size : 12697,\tepsilon : 3.3%, \t r <=40 0.0, \t r > 40 80.3921568627451\n","[Episode 1200]\t rewards globals : 6.844 \tavg rewards : 8.627,\tavg loss: : 0.050637,\tbuffer size : 13242,\tepsilon : 3.0%, \t r <=40 0.0, \t r > 40 86.27450980392157\n","[Episode 1250]\t rewards globals : 6.930 \tavg rewards : 9.020,\tavg loss: : 0.051142,\tbuffer size : 13838,\tepsilon : 2.8%, \t r <=40 0.0, \t r > 40 90.19607843137256\n","[Episode 1300]\t rewards globals : 6.987 \tavg rewards : 8.431,\tavg loss: : 0.050856,\tbuffer size : 14413,\tepsilon : 2.6%, \t r <=40 0.0, \t r > 40 84.31372549019608\n","[Episode 1350]\t rewards globals : 7.032 \tavg rewards : 8.235,\tavg loss: : 0.050594,\tbuffer size : 14998,\tepsilon : 2.4%, \t r <=40 0.0, \t r > 40 82.35294117647058\n","[Episode 1400]\t rewards globals : 7.059 \tavg rewards : 7.843,\tavg loss: : 0.050302,\tbuffer size : 15598,\tepsilon : 2.3%, \t r <=40 0.0, \t r > 40 78.43137254901961\n","[Episode 1450]\t rewards globals : 7.119 \tavg rewards : 8.824,\tavg loss: : 0.050790,\tbuffer size : 16196,\tepsilon : 2.2%, \t r <=40 0.0, \t r > 40 88.23529411764706\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 9.0\n","[task_2_tmax40] 100 run(s) avg rewards : 8.6\n","Point: 8.8\n","Re do test: double check\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 9.0\n","[task_2_tmax40] 100 run(s) avg rewards : 8.8\n","Point: 8.9\n","Training should stop\n","error while saving final\n","device is: cuda\n","config : \n"," x debut 25, y_debut 3, max ep 2200, max epsilon 0.3 \n"," Epsilon_decay 550, test_interval 500\n","Save_interval 1000, batch_size 64, buffer_limit 28000 \n"," Methode Mixed Monte Carlo + Atari DQN, gamma_nstep 0.5, nstep 3\n","Start learning from : x_debut 25 y_debut 3\n","[Episode 50]\t rewards globals : 3.137 \tavg rewards : 3.137,\tavg loss: : 0.208212,\tbuffer size : 497,\tepsilon : 27.5%, \t r <=40 0.0, \t r > 40 31.372549019607842\n","[Episode 100]\t rewards globals : 3.465 \tavg rewards : 3.725,\tavg loss: : 0.097402,\tbuffer size : 999,\tepsilon : 25.2%, \t r <=40 0.0, \t r > 40 37.254901960784316\n","[Episode 150]\t rewards globals : 3.775 \tavg rewards : 4.510,\tavg loss: : 0.075266,\tbuffer size : 1520,\tepsilon : 23.1%, \t r <=40 0.0, \t r > 40 45.09803921568628\n","[Episode 200]\t rewards globals : 3.781 \tavg rewards : 3.922,\tavg loss: : 0.069790,\tbuffer size : 2092,\tepsilon : 21.2%, \t r <=40 0.0, \t r > 40 39.21568627450981\n","[Episode 250]\t rewards globals : 3.944 \tavg rewards : 4.510,\tavg loss: : 0.068043,\tbuffer size : 2672,\tepsilon : 19.4%, \t r <=40 0.0, \t r > 40 45.09803921568628\n","[Episode 300]\t rewards globals : 4.120 \tavg rewards : 5.098,\tavg loss: : 0.064236,\tbuffer size : 3231,\tepsilon : 17.8%, \t r <=40 0.0, \t r > 40 50.98039215686274\n","[Episode 350]\t rewards globals : 4.046 \tavg rewards : 3.725,\tavg loss: : 0.063753,\tbuffer size : 3807,\tepsilon : 16.3%, \t r <=40 0.0, \t r > 40 37.254901960784316\n","[Episode 400]\t rewards globals : 4.239 \tavg rewards : 5.490,\tavg loss: : 0.060573,\tbuffer size : 4362,\tepsilon : 15.0%, \t r <=40 0.0, \t r > 40 54.90196078431373\n","[Episode 450]\t rewards globals : 4.302 \tavg rewards : 4.706,\tavg loss: : 0.060978,\tbuffer size : 4929,\tepsilon : 13.8%, \t r <=40 0.0, \t r > 40 47.05882352941176\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 7.3\n","[task_2_tmax40] 100 run(s) avg rewards : 6.9\n","Point: 7.1\n","[Episode 500]\t rewards globals : 4.391 \tavg rewards : 5.294,\tavg loss: : 0.060360,\tbuffer size : 5500,\tepsilon : 12.7%, \t r <=40 0.0, \t r > 40 52.94117647058824\n","[Episode 550]\t rewards globals : 4.446 \tavg rewards : 4.902,\tavg loss: : 0.059109,\tbuffer size : 6032,\tepsilon : 11.7%, \t r <=40 0.0, \t r > 40 49.01960784313725\n","[Episode 600]\t rewards globals : 4.493 \tavg rewards : 5.098,\tavg loss: : 0.058235,\tbuffer size : 6611,\tepsilon : 10.7%, \t r <=40 0.0, \t r > 40 50.98039215686274\n","[Episode 650]\t rewards globals : 4.593 \tavg rewards : 5.686,\tavg loss: : 0.059989,\tbuffer size : 7149,\tepsilon : 9.9%, \t r <=40 0.0, \t r > 40 56.86274509803921\n","[Episode 700]\t rewards globals : 4.750 \tavg rewards : 6.863,\tavg loss: : 0.058630,\tbuffer size : 7728,\tepsilon : 9.1%, \t r <=40 0.0, \t r > 40 68.62745098039215\n","[Episode 750]\t rewards globals : 4.767 \tavg rewards : 5.098,\tavg loss: : 0.057543,\tbuffer size : 8276,\tepsilon : 8.4%, \t r <=40 0.0, \t r > 40 50.98039215686274\n","[Episode 800]\t rewards globals : 4.881 \tavg rewards : 6.471,\tavg loss: : 0.057377,\tbuffer size : 8870,\tepsilon : 7.8%, \t r <=40 0.0, \t r > 40 64.70588235294117\n","[Episode 850]\t rewards globals : 4.971 \tavg rewards : 6.471,\tavg loss: : 0.057817,\tbuffer size : 9468,\tepsilon : 7.2%, \t r <=40 0.0, \t r > 40 64.70588235294117\n","[Episode 900]\t rewards globals : 5.105 \tavg rewards : 7.255,\tavg loss: : 0.056974,\tbuffer size : 10076,\tepsilon : 6.6%, \t r <=40 0.0, \t r > 40 72.54901960784314\n","[Episode 950]\t rewards globals : 5.121 \tavg rewards : 5.490,\tavg loss: : 0.056330,\tbuffer size : 10611,\tepsilon : 6.2%, \t r <=40 0.0, \t r > 40 54.90196078431373\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 7.2\n","[task_2_tmax40] 100 run(s) avg rewards : 7.2\n","Point: 7.2\n","[Episode 1000]\t rewards globals : 5.205 \tavg rewards : 6.667,\tavg loss: : 0.056217,\tbuffer size : 11198,\tepsilon : 5.7%, \t r <=40 0.0, \t r > 40 66.66666666666666\n","[Episode 1050]\t rewards globals : 5.290 \tavg rewards : 6.863,\tavg loss: : 0.057509,\tbuffer size : 11757,\tepsilon : 5.3%, \t r <=40 0.0, \t r > 40 68.62745098039215\n","[Episode 1100]\t rewards globals : 5.359 \tavg rewards : 6.667,\tavg loss: : 0.057428,\tbuffer size : 12376,\tepsilon : 4.9%, \t r <=40 0.0, \t r > 40 66.66666666666666\n","[Episode 1150]\t rewards globals : 5.465 \tavg rewards : 7.843,\tavg loss: : 0.056507,\tbuffer size : 12973,\tepsilon : 4.6%, \t r <=40 0.0, \t r > 40 78.43137254901961\n","[Episode 1200]\t rewards globals : 5.537 \tavg rewards : 7.255,\tavg loss: : 0.055500,\tbuffer size : 13543,\tepsilon : 4.3%, \t r <=40 0.0, \t r > 40 72.54901960784314\n","[Episode 1250]\t rewards globals : 5.612 \tavg rewards : 7.255,\tavg loss: : 0.056590,\tbuffer size : 14128,\tepsilon : 4.0%, \t r <=40 0.0, \t r > 40 72.54901960784314\n","[Episode 1300]\t rewards globals : 5.711 \tavg rewards : 8.235,\tavg loss: : 0.056601,\tbuffer size : 14717,\tepsilon : 3.7%, \t r <=40 0.0, \t r > 40 82.35294117647058\n","[Episode 1350]\t rewards globals : 5.744 \tavg rewards : 6.667,\tavg loss: : 0.055881,\tbuffer size : 15280,\tepsilon : 3.5%, \t r <=40 0.0, \t r > 40 66.66666666666666\n","[Episode 1400]\t rewards globals : 5.789 \tavg rewards : 7.059,\tavg loss: : 0.055751,\tbuffer size : 15855,\tepsilon : 3.3%, \t r <=40 0.0, \t r > 40 70.58823529411765\n","[Episode 1450]\t rewards globals : 5.830 \tavg rewards : 7.059,\tavg loss: : 0.056238,\tbuffer size : 16437,\tepsilon : 3.1%, \t r <=40 0.0, \t r > 40 70.58823529411765\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 7.8\n","[task_2_tmax40] 100 run(s) avg rewards : 8.3\n","Point: 8.05\n","[Episode 1500]\t rewards globals : 5.889 \tavg rewards : 7.451,\tavg loss: : 0.056202,\tbuffer size : 16999,\tepsilon : 2.9%, \t r <=40 0.0, \t r > 40 74.50980392156863\n","[Episode 1550]\t rewards globals : 5.919 \tavg rewards : 6.863,\tavg loss: : 0.055847,\tbuffer size : 17575,\tepsilon : 2.7%, \t r <=40 0.0, \t r > 40 68.62745098039215\n","[Episode 1600]\t rewards globals : 5.971 \tavg rewards : 7.647,\tavg loss: : 0.055874,\tbuffer size : 18157,\tepsilon : 2.6%, \t r <=40 0.0, \t r > 40 76.47058823529412\n","[Episode 1650]\t rewards globals : 6.002 \tavg rewards : 7.059,\tavg loss: : 0.056625,\tbuffer size : 18749,\tepsilon : 2.4%, \t r <=40 0.0, \t r > 40 70.58823529411765\n","[Episode 1700]\t rewards globals : 6.055 \tavg rewards : 7.647,\tavg loss: : 0.056860,\tbuffer size : 19347,\tepsilon : 2.3%, \t r <=40 0.0, \t r > 40 76.47058823529412\n","[Episode 1750]\t rewards globals : 6.099 \tavg rewards : 7.647,\tavg loss: : 0.056877,\tbuffer size : 19919,\tepsilon : 2.2%, \t r <=40 0.0, \t r > 40 76.47058823529412\n","[Episode 1800]\t rewards globals : 6.124 \tavg rewards : 6.863,\tavg loss: : 0.056709,\tbuffer size : 20517,\tepsilon : 2.1%, \t r <=40 0.0, \t r > 40 68.62745098039215\n","[Episode 1850]\t rewards globals : 6.159 \tavg rewards : 7.451,\tavg loss: : 0.057209,\tbuffer size : 21093,\tepsilon : 2.0%, \t r <=40 0.0, \t r > 40 74.50980392156863\n","[Episode 1900]\t rewards globals : 6.191 \tavg rewards : 7.451,\tavg loss: : 0.057547,\tbuffer size : 21698,\tepsilon : 1.9%, \t r <=40 0.0, \t r > 40 74.50980392156863\n","[Episode 1950]\t rewards globals : 6.228 \tavg rewards : 7.647,\tavg loss: : 0.057423,\tbuffer size : 22276,\tepsilon : 1.8%, \t r <=40 0.0, \t r > 40 76.47058823529412\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 7.8\n","[task_2_tmax40] 100 run(s) avg rewards : 8.2\n","Point: 8.0\n","[Episode 2000]\t rewards globals : 6.257 \tavg rewards : 7.451,\tavg loss: : 0.057129,\tbuffer size : 22876,\tepsilon : 1.8%, \t r <=40 0.0, \t r > 40 74.50980392156863\n","[Episode 2050]\t rewards globals : 6.265 \tavg rewards : 6.667,\tavg loss: : 0.057641,\tbuffer size : 23484,\tepsilon : 1.7%, \t r <=40 0.0, \t r > 40 66.66666666666666\n","[Episode 2100]\t rewards globals : 6.292 \tavg rewards : 7.451,\tavg loss: : 0.057746,\tbuffer size : 24095,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 74.50980392156863\n","[Episode 2150]\t rewards globals : 6.313 \tavg rewards : 7.255,\tavg loss: : 0.057729,\tbuffer size : 24683,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 72.54901960784314\n","error while saving final\n","device is: cuda\n","config : \n"," x debut 25, y_debut 4, max ep 2600, max epsilon 0.3 \n"," Epsilon_decay 650, test_interval 500\n","Save_interval 1000, batch_size 64, buffer_limit 32000 \n"," Methode Mixed Monte Carlo + Atari DQN, gamma_nstep 0.5, nstep 3\n","Start learning from : x_debut 25 y_debut 4\n","[Episode 50]\t rewards globals : 2.549 \tavg rewards : 2.549,\tavg loss: : 0.300911,\tbuffer size : 484,\tepsilon : 27.9%, \t r <=40 0.0, \t r > 40 25.49019607843137\n","[Episode 100]\t rewards globals : 2.475 \tavg rewards : 2.549,\tavg loss: : 0.105275,\tbuffer size : 1000,\tepsilon : 25.9%, \t r <=40 0.0, \t r > 40 25.49019607843137\n","[Episode 150]\t rewards globals : 2.384 \tavg rewards : 2.353,\tavg loss: : 0.083538,\tbuffer size : 1535,\tepsilon : 24.0%, \t r <=40 0.0, \t r > 40 23.52941176470588\n","[Episode 200]\t rewards globals : 2.189 \tavg rewards : 1.569,\tavg loss: : 0.075060,\tbuffer size : 2024,\tepsilon : 22.3%, \t r <=40 0.0, \t r > 40 15.686274509803921\n","[Episode 250]\t rewards globals : 2.231 \tavg rewards : 2.353,\tavg loss: : 0.071254,\tbuffer size : 2517,\tepsilon : 20.7%, \t r <=40 0.0, \t r > 40 23.52941176470588\n","[Episode 300]\t rewards globals : 2.193 \tavg rewards : 1.961,\tavg loss: : 0.067067,\tbuffer size : 2996,\tepsilon : 19.3%, \t r <=40 0.0, \t r > 40 19.607843137254903\n","[Episode 350]\t rewards globals : 2.365 \tavg rewards : 3.333,\tavg loss: : 0.064322,\tbuffer size : 3496,\tepsilon : 17.9%, \t r <=40 0.0, \t r > 40 33.33333333333333\n","[Episode 400]\t rewards globals : 2.594 \tavg rewards : 4.314,\tavg loss: : 0.061801,\tbuffer size : 4014,\tepsilon : 16.7%, \t r <=40 0.0, \t r > 40 43.13725490196079\n","[Episode 450]\t rewards globals : 2.772 \tavg rewards : 4.314,\tavg loss: : 0.061355,\tbuffer size : 4591,\tepsilon : 15.5%, \t r <=40 0.0, \t r > 40 43.13725490196079\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 5.5\n","[task_2_tmax40] 100 run(s) avg rewards : 5.8\n","Point: 5.65\n","[Episode 500]\t rewards globals : 2.954 \tavg rewards : 4.706,\tavg loss: : 0.060307,\tbuffer size : 5121,\tepsilon : 14.4%, \t r <=40 0.0, \t r > 40 47.05882352941176\n","[Episode 550]\t rewards globals : 3.031 \tavg rewards : 3.922,\tavg loss: : 0.058995,\tbuffer size : 5639,\tepsilon : 13.4%, \t r <=40 0.0, \t r > 40 39.21568627450981\n","[Episode 600]\t rewards globals : 3.161 \tavg rewards : 4.706,\tavg loss: : 0.058500,\tbuffer size : 6247,\tepsilon : 12.5%, \t r <=40 0.0, \t r > 40 47.05882352941176\n","[Episode 650]\t rewards globals : 3.164 \tavg rewards : 3.137,\tavg loss: : 0.060318,\tbuffer size : 6782,\tepsilon : 11.7%, \t r <=40 0.0, \t r > 40 31.372549019607842\n","[Episode 700]\t rewards globals : 3.267 \tavg rewards : 4.706,\tavg loss: : 0.059475,\tbuffer size : 7402,\tepsilon : 10.9%, \t r <=40 0.0, \t r > 40 47.05882352941176\n","[Episode 750]\t rewards globals : 3.289 \tavg rewards : 3.725,\tavg loss: : 0.058700,\tbuffer size : 7974,\tepsilon : 10.1%, \t r <=40 0.0, \t r > 40 37.254901960784316\n","[Episode 800]\t rewards globals : 3.396 \tavg rewards : 4.902,\tavg loss: : 0.059184,\tbuffer size : 8557,\tepsilon : 9.5%, \t r <=40 0.0, \t r > 40 49.01960784313725\n","[Episode 850]\t rewards globals : 3.490 \tavg rewards : 5.098,\tavg loss: : 0.059656,\tbuffer size : 9130,\tepsilon : 8.8%, \t r <=40 0.0, \t r > 40 50.98039215686274\n","[Episode 900]\t rewards globals : 3.585 \tavg rewards : 5.294,\tavg loss: : 0.059444,\tbuffer size : 9689,\tepsilon : 8.3%, \t r <=40 0.0, \t r > 40 52.94117647058824\n","[Episode 950]\t rewards globals : 3.628 \tavg rewards : 4.510,\tavg loss: : 0.058796,\tbuffer size : 10246,\tepsilon : 7.7%, \t r <=40 0.0, \t r > 40 45.09803921568628\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 6.7\n","[task_2_tmax40] 100 run(s) avg rewards : 6.8\n","Point: 6.75\n","[Episode 1000]\t rewards globals : 3.666 \tavg rewards : 4.510,\tavg loss: : 0.058607,\tbuffer size : 10848,\tepsilon : 7.2%, \t r <=40 0.0, \t r > 40 45.09803921568628\n","[Episode 1050]\t rewards globals : 3.777 \tavg rewards : 5.882,\tavg loss: : 0.059357,\tbuffer size : 11473,\tepsilon : 6.8%, \t r <=40 0.0, \t r > 40 58.82352941176471\n","[Episode 1100]\t rewards globals : 3.878 \tavg rewards : 5.882,\tavg loss: : 0.059220,\tbuffer size : 12105,\tepsilon : 6.3%, \t r <=40 0.0, \t r > 40 58.82352941176471\n","[Episode 1150]\t rewards globals : 3.944 \tavg rewards : 5.294,\tavg loss: : 0.058974,\tbuffer size : 12710,\tepsilon : 5.9%, \t r <=40 0.0, \t r > 40 52.94117647058824\n","[Episode 1200]\t rewards globals : 3.997 \tavg rewards : 5.098,\tavg loss: : 0.058614,\tbuffer size : 13267,\tepsilon : 5.6%, \t r <=40 0.0, \t r > 40 50.98039215686274\n","[Episode 1250]\t rewards globals : 4.053 \tavg rewards : 5.490,\tavg loss: : 0.058991,\tbuffer size : 13836,\tepsilon : 5.2%, \t r <=40 0.0, \t r > 40 54.90196078431373\n","[Episode 1300]\t rewards globals : 4.143 \tavg rewards : 6.471,\tavg loss: : 0.059125,\tbuffer size : 14421,\tepsilon : 4.9%, \t r <=40 0.0, \t r > 40 64.70588235294117\n","[Episode 1350]\t rewards globals : 4.197 \tavg rewards : 5.490,\tavg loss: : 0.058490,\tbuffer size : 15034,\tepsilon : 4.6%, \t r <=40 0.0, \t r > 40 54.90196078431373\n","[Episode 1400]\t rewards globals : 4.290 \tavg rewards : 6.667,\tavg loss: : 0.058498,\tbuffer size : 15651,\tepsilon : 4.4%, \t r <=40 0.0, \t r > 40 66.66666666666666\n","[Episode 1450]\t rewards globals : 4.307 \tavg rewards : 4.902,\tavg loss: : 0.059795,\tbuffer size : 16218,\tepsilon : 4.1%, \t r <=40 0.0, \t r > 40 49.01960784313725\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 7.3\n","[task_2_tmax40] 100 run(s) avg rewards : 6.8\n","Point: 7.05\n","[Episode 1500]\t rewards globals : 4.364 \tavg rewards : 5.882,\tavg loss: : 0.060261,\tbuffer size : 16817,\tepsilon : 3.9%, \t r <=40 0.0, \t r > 40 58.82352941176471\n","[Episode 1550]\t rewards globals : 4.436 \tavg rewards : 6.667,\tavg loss: : 0.060389,\tbuffer size : 17423,\tepsilon : 3.7%, \t r <=40 0.0, \t r > 40 66.66666666666666\n","[Episode 1600]\t rewards globals : 4.441 \tavg rewards : 4.510,\tavg loss: : 0.059995,\tbuffer size : 17981,\tepsilon : 3.5%, \t r <=40 0.0, \t r > 40 45.09803921568628\n","[Episode 1650]\t rewards globals : 4.476 \tavg rewards : 5.490,\tavg loss: : 0.060733,\tbuffer size : 18567,\tepsilon : 3.3%, \t r <=40 0.0, \t r > 40 54.90196078431373\n","[Episode 1700]\t rewards globals : 4.521 \tavg rewards : 6.078,\tavg loss: : 0.060872,\tbuffer size : 19163,\tepsilon : 3.1%, \t r <=40 0.0, \t r > 40 60.78431372549019\n","[Episode 1750]\t rewards globals : 4.540 \tavg rewards : 5.294,\tavg loss: : 0.060789,\tbuffer size : 19780,\tepsilon : 3.0%, \t r <=40 0.0, \t r > 40 52.94117647058824\n","[Episode 1800]\t rewards globals : 4.592 \tavg rewards : 6.471,\tavg loss: : 0.060843,\tbuffer size : 20346,\tepsilon : 2.8%, \t r <=40 0.0, \t r > 40 64.70588235294117\n","[Episode 1850]\t rewards globals : 4.630 \tavg rewards : 5.882,\tavg loss: : 0.061285,\tbuffer size : 20925,\tepsilon : 2.7%, \t r <=40 0.0, \t r > 40 58.82352941176471\n","[Episode 1900]\t rewards globals : 4.666 \tavg rewards : 5.882,\tavg loss: : 0.061379,\tbuffer size : 21524,\tepsilon : 2.6%, \t r <=40 0.0, \t r > 40 58.82352941176471\n","[Episode 1950]\t rewards globals : 4.705 \tavg rewards : 6.275,\tavg loss: : 0.061424,\tbuffer size : 22134,\tepsilon : 2.4%, \t r <=40 0.0, \t r > 40 62.745098039215684\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 6.4\n","[task_2_tmax40] 100 run(s) avg rewards : 6.9\n","Point: 6.65\n","[Episode 2000]\t rewards globals : 4.743 \tavg rewards : 6.078,\tavg loss: : 0.061541,\tbuffer size : 22727,\tepsilon : 2.3%, \t r <=40 0.0, \t r > 40 60.78431372549019\n","[Episode 2050]\t rewards globals : 4.773 \tavg rewards : 5.882,\tavg loss: : 0.061899,\tbuffer size : 23308,\tepsilon : 2.2%, \t r <=40 0.0, \t r > 40 58.82352941176471\n","[Episode 2100]\t rewards globals : 4.831 \tavg rewards : 7.059,\tavg loss: : 0.062020,\tbuffer size : 23916,\tepsilon : 2.1%, \t r <=40 0.0, \t r > 40 70.58823529411765\n","[Episode 2150]\t rewards globals : 4.854 \tavg rewards : 5.686,\tavg loss: : 0.062095,\tbuffer size : 24538,\tepsilon : 2.1%, \t r <=40 0.0, \t r > 40 56.86274509803921\n","[Episode 2200]\t rewards globals : 4.907 \tavg rewards : 7.059,\tavg loss: : 0.061973,\tbuffer size : 25146,\tepsilon : 2.0%, \t r <=40 0.0, \t r > 40 70.58823529411765\n","[Episode 2250]\t rewards globals : 4.927 \tavg rewards : 5.686,\tavg loss: : 0.062378,\tbuffer size : 25708,\tepsilon : 1.9%, \t r <=40 0.0, \t r > 40 56.86274509803921\n","[Episode 2300]\t rewards globals : 4.937 \tavg rewards : 5.294,\tavg loss: : 0.062627,\tbuffer size : 26280,\tepsilon : 1.8%, \t r <=40 0.0, \t r > 40 52.94117647058824\n","[Episode 2350]\t rewards globals : 4.985 \tavg rewards : 7.255,\tavg loss: : 0.062939,\tbuffer size : 26891,\tepsilon : 1.8%, \t r <=40 0.0, \t r > 40 72.54901960784314\n","[Episode 2400]\t rewards globals : 5.019 \tavg rewards : 6.667,\tavg loss: : 0.063081,\tbuffer size : 27487,\tepsilon : 1.7%, \t r <=40 0.0, \t r > 40 66.66666666666666\n","[Episode 2450]\t rewards globals : 5.043 \tavg rewards : 6.275,\tavg loss: : 0.063505,\tbuffer size : 28060,\tepsilon : 1.7%, \t r <=40 0.0, \t r > 40 62.745098039215684\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 6.7\n","[task_2_tmax40] 100 run(s) avg rewards : 5.9\n","Point: 6.300000000000001\n","[Episode 2500]\t rewards globals : 5.074 \tavg rewards : 6.667,\tavg loss: : 0.063555,\tbuffer size : 28675,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 66.66666666666666\n","[Episode 2550]\t rewards globals : 5.112 \tavg rewards : 7.059,\tavg loss: : 0.063465,\tbuffer size : 29246,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 70.58823529411765\n","error while saving final\n","device is: cuda\n","config : \n"," x debut 30, y_debut 0, max ep 1250, max epsilon 0.3 \n"," Epsilon_decay 312, test_interval 500\n","Save_interval 1000, batch_size 64, buffer_limit 20000 \n"," Methode Mixed Monte Carlo + Atari DQN, gamma_nstep 0.5, nstep 3\n","Start learning from : x_debut 30 y_debut 0\n","[Episode 50]\t rewards globals : 4.118 \tavg rewards : 4.118,\tavg loss: : 0.193434,\tbuffer size : 664,\tepsilon : 25.7%, \t r <=40 0.0, \t r > 40 41.17647058823529\n","[Episode 100]\t rewards globals : 4.059 \tavg rewards : 3.922,\tavg loss: : 0.113252,\tbuffer size : 1292,\tepsilon : 22.0%, \t r <=40 0.0, \t r > 40 39.21568627450981\n","[Episode 150]\t rewards globals : 4.570 \tavg rewards : 5.686,\tavg loss: : 0.086816,\tbuffer size : 1941,\tepsilon : 18.9%, \t r <=40 0.0, \t r > 40 56.86274509803921\n","[Episode 200]\t rewards globals : 4.975 \tavg rewards : 6.275,\tavg loss: : 0.076934,\tbuffer size : 2611,\tepsilon : 16.3%, \t r <=40 0.0, \t r > 40 62.745098039215684\n","[Episode 250]\t rewards globals : 5.139 \tavg rewards : 5.882,\tavg loss: : 0.075297,\tbuffer size : 3266,\tepsilon : 14.0%, \t r <=40 0.0, \t r > 40 58.82352941176471\n","[Episode 300]\t rewards globals : 5.449 \tavg rewards : 6.863,\tavg loss: : 0.069158,\tbuffer size : 3958,\tepsilon : 12.1%, \t r <=40 0.0, \t r > 40 68.62745098039215\n","[Episode 350]\t rewards globals : 5.470 \tavg rewards : 5.686,\tavg loss: : 0.065229,\tbuffer size : 4567,\tepsilon : 10.4%, \t r <=40 0.0, \t r > 40 56.86274509803921\n","[Episode 400]\t rewards globals : 5.661 \tavg rewards : 7.059,\tavg loss: : 0.063418,\tbuffer size : 5289,\tepsilon : 9.0%, \t r <=40 0.0, \t r > 40 70.58823529411765\n","[Episode 450]\t rewards globals : 5.831 \tavg rewards : 7.059,\tavg loss: : 0.063507,\tbuffer size : 6027,\tepsilon : 7.9%, \t r <=40 0.0, \t r > 40 70.58823529411765\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 9.2\n","[task_2_tmax40] 100 run(s) avg rewards : 9.4\n","Point: 9.3\n","Re do test: double check\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 9.7\n","[task_2_tmax40] 100 run(s) avg rewards : 9.6\n","Point: 9.649999999999999\n","Training should stop\n","error while saving final\n","device is: cuda\n","config : \n"," x debut 30, y_debut 1, max ep 1650, max epsilon 0.3 \n"," Epsilon_decay 412, test_interval 500\n","Save_interval 1000, batch_size 64, buffer_limit 24000 \n"," Methode Mixed Monte Carlo + Atari DQN, gamma_nstep 0.5, nstep 3\n","Start learning from : x_debut 30 y_debut 1\n","[Episode 50]\t rewards globals : 3.725 \tavg rewards : 3.725,\tavg loss: : 0.194444,\tbuffer size : 583,\tepsilon : 26.7%, \t r <=40 0.0, \t r > 40 37.254901960784316\n","[Episode 100]\t rewards globals : 4.356 \tavg rewards : 5.098,\tavg loss: : 0.105774,\tbuffer size : 1224,\tepsilon : 23.8%, \t r <=40 0.0, \t r > 40 50.98039215686274\n","[Episode 150]\t rewards globals : 4.172 \tavg rewards : 3.725,\tavg loss: : 0.084870,\tbuffer size : 1713,\tepsilon : 21.2%, \t r <=40 0.0, \t r > 40 37.254901960784316\n","[Episode 200]\t rewards globals : 4.428 \tavg rewards : 5.098,\tavg loss: : 0.075646,\tbuffer size : 2336,\tepsilon : 18.8%, \t r <=40 0.0, \t r > 40 50.98039215686274\n","[Episode 250]\t rewards globals : 4.382 \tavg rewards : 4.314,\tavg loss: : 0.073702,\tbuffer size : 2863,\tepsilon : 16.8%, \t r <=40 0.0, \t r > 40 43.13725490196079\n","[Episode 300]\t rewards globals : 4.751 \tavg rewards : 6.471,\tavg loss: : 0.070469,\tbuffer size : 3532,\tepsilon : 15.0%, \t r <=40 0.0, \t r > 40 64.70588235294117\n","[Episode 350]\t rewards globals : 5.071 \tavg rewards : 6.863,\tavg loss: : 0.068216,\tbuffer size : 4201,\tepsilon : 13.4%, \t r <=40 0.0, \t r > 40 68.62745098039215\n","[Episode 400]\t rewards globals : 5.287 \tavg rewards : 6.863,\tavg loss: : 0.064599,\tbuffer size : 4875,\tepsilon : 12.0%, \t r <=40 0.0, \t r > 40 68.62745098039215\n","[Episode 450]\t rewards globals : 5.344 \tavg rewards : 5.882,\tavg loss: : 0.064204,\tbuffer size : 5528,\tepsilon : 10.7%, \t r <=40 0.0, \t r > 40 58.82352941176471\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 9.4\n","[task_2_tmax40] 100 run(s) avg rewards : 8.3\n","Point: 8.850000000000001\n","Re do test: double check\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 9.5\n","[task_2_tmax40] 100 run(s) avg rewards : 9.4\n","Point: 9.45\n","Training should stop\n","error while saving final\n","device is: cuda\n","config : \n"," x debut 30, y_debut 2, max ep 2050, max epsilon 0.3 \n"," Epsilon_decay 512, test_interval 500\n","Save_interval 1000, batch_size 64, buffer_limit 28000 \n"," Methode Mixed Monte Carlo + Atari DQN, gamma_nstep 0.5, nstep 3\n","Start learning from : x_debut 30 y_debut 2\n","[Episode 50]\t rewards globals : 3.137 \tavg rewards : 3.137,\tavg loss: : 0.178540,\tbuffer size : 553,\tepsilon : 27.3%, \t r <=40 0.0, \t r > 40 31.372549019607842\n","[Episode 100]\t rewards globals : 3.564 \tavg rewards : 3.922,\tavg loss: : 0.087562,\tbuffer size : 1142,\tepsilon : 24.9%, \t r <=40 0.0, \t r > 40 39.21568627450981\n","[Episode 150]\t rewards globals : 3.642 \tavg rewards : 3.725,\tavg loss: : 0.071035,\tbuffer size : 1764,\tepsilon : 22.6%, \t r <=40 0.0, \t r > 40 37.254901960784316\n","[Episode 200]\t rewards globals : 3.831 \tavg rewards : 4.314,\tavg loss: : 0.068219,\tbuffer size : 2337,\tepsilon : 20.6%, \t r <=40 0.0, \t r > 40 43.13725490196079\n","[Episode 250]\t rewards globals : 3.984 \tavg rewards : 4.706,\tavg loss: : 0.065754,\tbuffer size : 2862,\tepsilon : 18.8%, \t r <=40 0.0, \t r > 40 47.05882352941176\n","[Episode 300]\t rewards globals : 4.053 \tavg rewards : 4.314,\tavg loss: : 0.062160,\tbuffer size : 3474,\tepsilon : 17.1%, \t r <=40 0.0, \t r > 40 43.13725490196079\n","[Episode 350]\t rewards globals : 4.245 \tavg rewards : 5.294,\tavg loss: : 0.060471,\tbuffer size : 4057,\tepsilon : 15.6%, \t r <=40 0.0, \t r > 40 52.94117647058824\n","[Episode 400]\t rewards globals : 4.314 \tavg rewards : 4.706,\tavg loss: : 0.057728,\tbuffer size : 4631,\tepsilon : 14.3%, \t r <=40 0.0, \t r > 40 47.05882352941176\n","[Episode 450]\t rewards globals : 4.501 \tavg rewards : 6.078,\tavg loss: : 0.058405,\tbuffer size : 5287,\tepsilon : 13.0%, \t r <=40 0.0, \t r > 40 60.78431372549019\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 8.0\n","[task_2_tmax40] 100 run(s) avg rewards : 7.9\n","Point: 7.95\n","[Episode 500]\t rewards globals : 4.631 \tavg rewards : 5.686,\tavg loss: : 0.057486,\tbuffer size : 5877,\tepsilon : 11.9%, \t r <=40 0.0, \t r > 40 56.86274509803921\n","[Episode 550]\t rewards globals : 4.755 \tavg rewards : 6.078,\tavg loss: : 0.057193,\tbuffer size : 6540,\tepsilon : 10.9%, \t r <=40 0.0, \t r > 40 60.78431372549019\n","[Episode 600]\t rewards globals : 4.825 \tavg rewards : 5.686,\tavg loss: : 0.057262,\tbuffer size : 7175,\tepsilon : 10.0%, \t r <=40 0.0, \t r > 40 56.86274509803921\n","[Episode 650]\t rewards globals : 4.916 \tavg rewards : 6.078,\tavg loss: : 0.058157,\tbuffer size : 7846,\tepsilon : 9.1%, \t r <=40 0.0, \t r > 40 60.78431372549019\n","[Episode 700]\t rewards globals : 5.050 \tavg rewards : 6.667,\tavg loss: : 0.057839,\tbuffer size : 8504,\tepsilon : 8.4%, \t r <=40 0.0, \t r > 40 66.66666666666666\n","[Episode 750]\t rewards globals : 5.126 \tavg rewards : 6.078,\tavg loss: : 0.057059,\tbuffer size : 9147,\tepsilon : 7.7%, \t r <=40 0.0, \t r > 40 60.78431372549019\n","[Episode 800]\t rewards globals : 5.106 \tavg rewards : 4.706,\tavg loss: : 0.056980,\tbuffer size : 9750,\tepsilon : 7.1%, \t r <=40 0.0, \t r > 40 47.05882352941176\n","[Episode 850]\t rewards globals : 5.182 \tavg rewards : 6.275,\tavg loss: : 0.057961,\tbuffer size : 10351,\tepsilon : 6.5%, \t r <=40 0.0, \t r > 40 62.745098039215684\n","[Episode 900]\t rewards globals : 5.239 \tavg rewards : 6.078,\tavg loss: : 0.057931,\tbuffer size : 10946,\tepsilon : 6.0%, \t r <=40 0.0, \t r > 40 60.78431372549019\n","[Episode 950]\t rewards globals : 5.363 \tavg rewards : 7.647,\tavg loss: : 0.057045,\tbuffer size : 11636,\tepsilon : 5.5%, \t r <=40 0.0, \t r > 40 76.47058823529412\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 7.9\n","[task_2_tmax40] 100 run(s) avg rewards : 8.2\n","Point: 8.05\n","[Episode 1000]\t rewards globals : 5.455 \tavg rewards : 7.059,\tavg loss: : 0.056596,\tbuffer size : 12303,\tepsilon : 5.1%, \t r <=40 0.0, \t r > 40 70.58823529411765\n","[Episode 1050]\t rewards globals : 5.614 \tavg rewards : 8.824,\tavg loss: : 0.056835,\tbuffer size : 13007,\tepsilon : 4.7%, \t r <=40 0.0, \t r > 40 88.23529411764706\n","[Episode 1100]\t rewards globals : 5.695 \tavg rewards : 7.451,\tavg loss: : 0.057013,\tbuffer size : 13683,\tepsilon : 4.4%, \t r <=40 0.0, \t r > 40 74.50980392156863\n","[Episode 1150]\t rewards globals : 5.760 \tavg rewards : 7.255,\tavg loss: : 0.057043,\tbuffer size : 14355,\tepsilon : 4.1%, \t r <=40 0.0, \t r > 40 72.54901960784314\n","[Episode 1200]\t rewards globals : 5.853 \tavg rewards : 8.039,\tavg loss: : 0.056505,\tbuffer size : 15069,\tepsilon : 3.8%, \t r <=40 0.0, \t r > 40 80.3921568627451\n","[Episode 1250]\t rewards globals : 5.939 \tavg rewards : 8.039,\tavg loss: : 0.056558,\tbuffer size : 15764,\tepsilon : 3.5%, \t r <=40 0.0, \t r > 40 80.3921568627451\n","[Episode 1300]\t rewards globals : 5.988 \tavg rewards : 7.059,\tavg loss: : 0.057005,\tbuffer size : 16406,\tepsilon : 3.3%, \t r <=40 0.0, \t r > 40 70.58823529411765\n","[Episode 1350]\t rewards globals : 6.040 \tavg rewards : 7.451,\tavg loss: : 0.056828,\tbuffer size : 17066,\tepsilon : 3.1%, \t r <=40 0.0, \t r > 40 74.50980392156863\n","[Episode 1400]\t rewards globals : 6.081 \tavg rewards : 7.255,\tavg loss: : 0.056594,\tbuffer size : 17752,\tepsilon : 2.9%, \t r <=40 0.0, \t r > 40 72.54901960784314\n","[Episode 1450]\t rewards globals : 6.168 \tavg rewards : 8.431,\tavg loss: : 0.056825,\tbuffer size : 18479,\tepsilon : 2.7%, \t r <=40 0.0, \t r > 40 84.31372549019608\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 9.3\n","[task_2_tmax40] 100 run(s) avg rewards : 8.5\n","Point: 8.9\n","Re do test: double check\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 8.2\n","[task_2_tmax40] 100 run(s) avg rewards : 8.6\n","Point: 8.399999999999999\n","[Episode 1500]\t rewards globals : 6.216 \tavg rewards : 7.647,\tavg loss: : 0.056635,\tbuffer size : 19136,\tepsilon : 2.5%, \t r <=40 0.0, \t r > 40 76.47058823529412\n","[Episode 1550]\t rewards globals : 6.260 \tavg rewards : 7.451,\tavg loss: : 0.056891,\tbuffer size : 19808,\tepsilon : 2.4%, \t r <=40 0.0, \t r > 40 74.50980392156863\n","[Episode 1600]\t rewards globals : 6.296 \tavg rewards : 7.451,\tavg loss: : 0.056883,\tbuffer size : 20539,\tepsilon : 2.3%, \t r <=40 0.0, \t r > 40 74.50980392156863\n","[Episode 1650]\t rewards globals : 6.348 \tavg rewards : 8.039,\tavg loss: : 0.056875,\tbuffer size : 21271,\tepsilon : 2.2%, \t r <=40 0.0, \t r > 40 80.3921568627451\n","[Episode 1700]\t rewards globals : 6.390 \tavg rewards : 7.647,\tavg loss: : 0.057019,\tbuffer size : 21962,\tepsilon : 2.0%, \t r <=40 0.0, \t r > 40 76.47058823529412\n","[Episode 1750]\t rewards globals : 6.448 \tavg rewards : 8.431,\tavg loss: : 0.057207,\tbuffer size : 22646,\tepsilon : 2.0%, \t r <=40 0.0, \t r > 40 84.31372549019608\n","[Episode 1800]\t rewards globals : 6.519 \tavg rewards : 9.020,\tavg loss: : 0.057185,\tbuffer size : 23325,\tepsilon : 1.9%, \t r <=40 0.0, \t r > 40 90.19607843137256\n","[Episode 1850]\t rewards globals : 6.553 \tavg rewards : 7.843,\tavg loss: : 0.057407,\tbuffer size : 23964,\tepsilon : 1.8%, \t r <=40 0.0, \t r > 40 78.43137254901961\n","[Episode 1900]\t rewards globals : 6.560 \tavg rewards : 6.863,\tavg loss: : 0.057602,\tbuffer size : 24601,\tepsilon : 1.7%, \t r <=40 0.0, \t r > 40 68.62745098039215\n","[Episode 1950]\t rewards globals : 6.612 \tavg rewards : 8.627,\tavg loss: : 0.057598,\tbuffer size : 25302,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 86.27450980392157\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 8.1\n","[task_2_tmax40] 100 run(s) avg rewards : 8.5\n","Point: 8.3\n","[Episode 2000]\t rewards globals : 6.637 \tavg rewards : 7.647,\tavg loss: : 0.057367,\tbuffer size : 26001,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 76.47058823529412\n","error while saving final\n","device is: cuda\n","config : \n"," x debut 30, y_debut 3, max ep 2450, max epsilon 0.3 \n"," Epsilon_decay 612, test_interval 500\n","Save_interval 1000, batch_size 64, buffer_limit 32000 \n"," Methode Mixed Monte Carlo + Atari DQN, gamma_nstep 0.5, nstep 3\n","Start learning from : x_debut 30 y_debut 3\n","[Episode 50]\t rewards globals : 2.549 \tavg rewards : 2.549,\tavg loss: : 0.182261,\tbuffer size : 568,\tepsilon : 27.7%, \t r <=40 0.0, \t r > 40 25.49019607843137\n","[Episode 100]\t rewards globals : 2.970 \tavg rewards : 3.529,\tavg loss: : 0.092172,\tbuffer size : 1129,\tepsilon : 25.6%, \t r <=40 0.0, \t r > 40 35.294117647058826\n","[Episode 150]\t rewards globals : 3.245 \tavg rewards : 3.725,\tavg loss: : 0.072438,\tbuffer size : 1660,\tepsilon : 23.7%, \t r <=40 0.0, \t r > 40 37.254901960784316\n","[Episode 200]\t rewards globals : 3.284 \tavg rewards : 3.333,\tavg loss: : 0.064585,\tbuffer size : 2212,\tepsilon : 21.9%, \t r <=40 0.0, \t r > 40 33.33333333333333\n","[Episode 250]\t rewards globals : 3.546 \tavg rewards : 4.510,\tavg loss: : 0.065264,\tbuffer size : 2796,\tepsilon : 20.3%, \t r <=40 0.0, \t r > 40 45.09803921568628\n","[Episode 300]\t rewards globals : 3.422 \tavg rewards : 2.941,\tavg loss: : 0.061940,\tbuffer size : 3351,\tepsilon : 18.8%, \t r <=40 0.0, \t r > 40 29.411764705882355\n","[Episode 350]\t rewards globals : 3.590 \tavg rewards : 4.706,\tavg loss: : 0.059862,\tbuffer size : 4010,\tepsilon : 17.4%, \t r <=40 0.0, \t r > 40 47.05882352941176\n","[Episode 400]\t rewards globals : 3.865 \tavg rewards : 5.882,\tavg loss: : 0.058065,\tbuffer size : 4692,\tepsilon : 16.1%, \t r <=40 0.0, \t r > 40 58.82352941176471\n","[Episode 450]\t rewards globals : 4.013 \tavg rewards : 5.098,\tavg loss: : 0.059826,\tbuffer size : 5284,\tepsilon : 14.9%, \t r <=40 0.0, \t r > 40 50.98039215686274\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 7.7\n","[task_2_tmax40] 100 run(s) avg rewards : 7.5\n","Point: 7.6\n","[Episode 500]\t rewards globals : 4.172 \tavg rewards : 5.490,\tavg loss: : 0.059554,\tbuffer size : 5946,\tepsilon : 13.8%, \t r <=40 0.0, \t r > 40 54.90196078431373\n","[Episode 550]\t rewards globals : 4.319 \tavg rewards : 5.882,\tavg loss: : 0.058005,\tbuffer size : 6594,\tepsilon : 12.8%, \t r <=40 0.0, \t r > 40 58.82352941176471\n","[Episode 600]\t rewards globals : 4.443 \tavg rewards : 5.686,\tavg loss: : 0.057107,\tbuffer size : 7190,\tepsilon : 11.9%, \t r <=40 0.0, \t r > 40 56.86274509803921\n","[Episode 650]\t rewards globals : 4.593 \tavg rewards : 6.275,\tavg loss: : 0.059506,\tbuffer size : 7832,\tepsilon : 11.0%, \t r <=40 0.0, \t r > 40 62.745098039215684\n","[Episode 700]\t rewards globals : 4.665 \tavg rewards : 5.490,\tavg loss: : 0.058799,\tbuffer size : 8435,\tepsilon : 10.2%, \t r <=40 0.0, \t r > 40 54.90196078431373\n","[Episode 750]\t rewards globals : 4.714 \tavg rewards : 5.490,\tavg loss: : 0.058319,\tbuffer size : 9068,\tepsilon : 9.5%, \t r <=40 0.0, \t r > 40 54.90196078431373\n","[Episode 800]\t rewards globals : 4.769 \tavg rewards : 5.490,\tavg loss: : 0.057661,\tbuffer size : 9675,\tepsilon : 8.8%, \t r <=40 0.0, \t r > 40 54.90196078431373\n","[Episode 850]\t rewards globals : 4.830 \tavg rewards : 5.882,\tavg loss: : 0.058505,\tbuffer size : 10330,\tepsilon : 8.2%, \t r <=40 0.0, \t r > 40 58.82352941176471\n","[Episode 900]\t rewards globals : 4.895 \tavg rewards : 6.078,\tavg loss: : 0.058767,\tbuffer size : 10991,\tepsilon : 7.7%, \t r <=40 0.0, \t r > 40 60.78431372549019\n","[Episode 950]\t rewards globals : 5.016 \tavg rewards : 7.255,\tavg loss: : 0.058216,\tbuffer size : 11667,\tepsilon : 7.1%, \t r <=40 0.0, \t r > 40 72.54901960784314\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 7.1\n","[task_2_tmax40] 100 run(s) avg rewards : 8.3\n","Point: 7.7\n","[Episode 1000]\t rewards globals : 5.025 \tavg rewards : 5.294,\tavg loss: : 0.058228,\tbuffer size : 12341,\tepsilon : 6.7%, \t r <=40 0.0, \t r > 40 52.94117647058824\n","[Episode 1050]\t rewards globals : 5.119 \tavg rewards : 7.059,\tavg loss: : 0.059252,\tbuffer size : 12975,\tepsilon : 6.2%, \t r <=40 0.0, \t r > 40 70.58823529411765\n","[Episode 1100]\t rewards globals : 5.213 \tavg rewards : 7.255,\tavg loss: : 0.058867,\tbuffer size : 13636,\tepsilon : 5.8%, \t r <=40 0.0, \t r > 40 72.54901960784314\n","[Episode 1150]\t rewards globals : 5.274 \tavg rewards : 6.667,\tavg loss: : 0.058838,\tbuffer size : 14310,\tepsilon : 5.4%, \t r <=40 0.0, \t r > 40 66.66666666666666\n","[Episode 1200]\t rewards globals : 5.329 \tavg rewards : 6.471,\tavg loss: : 0.058386,\tbuffer size : 14946,\tepsilon : 5.1%, \t r <=40 0.0, \t r > 40 64.70588235294117\n","[Episode 1250]\t rewards globals : 5.420 \tavg rewards : 7.647,\tavg loss: : 0.059062,\tbuffer size : 15608,\tepsilon : 4.8%, \t r <=40 0.0, \t r > 40 76.47058823529412\n","[Episode 1300]\t rewards globals : 5.473 \tavg rewards : 6.667,\tavg loss: : 0.058924,\tbuffer size : 16263,\tepsilon : 4.5%, \t r <=40 0.0, \t r > 40 66.66666666666666\n","[Episode 1350]\t rewards globals : 5.537 \tavg rewards : 7.059,\tavg loss: : 0.058596,\tbuffer size : 16912,\tepsilon : 4.2%, \t r <=40 0.0, \t r > 40 70.58823529411765\n","[Episode 1400]\t rewards globals : 5.603 \tavg rewards : 7.451,\tavg loss: : 0.058350,\tbuffer size : 17597,\tepsilon : 3.9%, \t r <=40 0.0, \t r > 40 74.50980392156863\n","[Episode 1450]\t rewards globals : 5.631 \tavg rewards : 6.471,\tavg loss: : 0.059021,\tbuffer size : 18281,\tepsilon : 3.7%, \t r <=40 0.0, \t r > 40 64.70588235294117\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 8.3\n","[task_2_tmax40] 100 run(s) avg rewards : 8.3\n","Point: 8.3\n","[Episode 1500]\t rewards globals : 5.650 \tavg rewards : 6.275,\tavg loss: : 0.059064,\tbuffer size : 18962,\tepsilon : 3.5%, \t r <=40 0.0, \t r > 40 62.745098039215684\n","[Episode 1550]\t rewards globals : 5.661 \tavg rewards : 6.078,\tavg loss: : 0.059186,\tbuffer size : 19641,\tepsilon : 3.3%, \t r <=40 0.0, \t r > 40 60.78431372549019\n","[Episode 1600]\t rewards globals : 5.728 \tavg rewards : 7.843,\tavg loss: : 0.058833,\tbuffer size : 20306,\tepsilon : 3.1%, \t r <=40 0.0, \t r > 40 78.43137254901961\n","[Episode 1650]\t rewards globals : 5.748 \tavg rewards : 6.275,\tavg loss: : 0.059140,\tbuffer size : 20977,\tepsilon : 3.0%, \t r <=40 0.0, \t r > 40 62.745098039215684\n","[Episode 1700]\t rewards globals : 5.785 \tavg rewards : 6.863,\tavg loss: : 0.059490,\tbuffer size : 21661,\tepsilon : 2.8%, \t r <=40 0.0, \t r > 40 68.62745098039215\n","[Episode 1750]\t rewards globals : 5.825 \tavg rewards : 7.255,\tavg loss: : 0.059503,\tbuffer size : 22325,\tepsilon : 2.7%, \t r <=40 0.0, \t r > 40 72.54901960784314\n","[Episode 1800]\t rewards globals : 5.847 \tavg rewards : 6.667,\tavg loss: : 0.059619,\tbuffer size : 23023,\tepsilon : 2.5%, \t r <=40 0.0, \t r > 40 66.66666666666666\n","[Episode 1850]\t rewards globals : 5.883 \tavg rewards : 7.255,\tavg loss: : 0.060001,\tbuffer size : 23701,\tepsilon : 2.4%, \t r <=40 0.0, \t r > 40 72.54901960784314\n","[Episode 1900]\t rewards globals : 5.934 \tavg rewards : 7.843,\tavg loss: : 0.060007,\tbuffer size : 24375,\tepsilon : 2.3%, \t r <=40 0.0, \t r > 40 78.43137254901961\n","[Episode 1950]\t rewards globals : 5.976 \tavg rewards : 7.647,\tavg loss: : 0.059897,\tbuffer size : 25010,\tepsilon : 2.2%, \t r <=40 0.0, \t r > 40 76.47058823529412\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 8.3\n","[task_2_tmax40] 100 run(s) avg rewards : 7.6\n","Point: 7.95\n","[Episode 2000]\t rewards globals : 6.022 \tavg rewards : 7.843,\tavg loss: : 0.059855,\tbuffer size : 25675,\tepsilon : 2.1%, \t r <=40 0.0, \t r > 40 78.43137254901961\n","[Episode 2050]\t rewards globals : 6.041 \tavg rewards : 6.863,\tavg loss: : 0.060093,\tbuffer size : 26346,\tepsilon : 2.0%, \t r <=40 0.0, \t r > 40 68.62745098039215\n","[Episode 2100]\t rewards globals : 6.073 \tavg rewards : 7.255,\tavg loss: : 0.059973,\tbuffer size : 27037,\tepsilon : 1.9%, \t r <=40 0.0, \t r > 40 72.54901960784314\n","[Episode 2150]\t rewards globals : 6.099 \tavg rewards : 7.255,\tavg loss: : 0.060090,\tbuffer size : 27698,\tepsilon : 1.9%, \t r <=40 0.0, \t r > 40 72.54901960784314\n","[Episode 2200]\t rewards globals : 6.134 \tavg rewards : 7.647,\tavg loss: : 0.060042,\tbuffer size : 28344,\tepsilon : 1.8%, \t r <=40 0.0, \t r > 40 76.47058823529412\n","[Episode 2250]\t rewards globals : 6.179 \tavg rewards : 8.235,\tavg loss: : 0.060407,\tbuffer size : 29038,\tepsilon : 1.7%, \t r <=40 0.0, \t r > 40 82.35294117647058\n","[Episode 2300]\t rewards globals : 6.202 \tavg rewards : 7.255,\tavg loss: : 0.060408,\tbuffer size : 29679,\tepsilon : 1.7%, \t r <=40 0.0, \t r > 40 72.54901960784314\n","[Episode 2350]\t rewards globals : 6.248 \tavg rewards : 8.431,\tavg loss: : 0.060155,\tbuffer size : 30353,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 84.31372549019608\n","[Episode 2400]\t rewards globals : 6.260 \tavg rewards : 6.863,\tavg loss: : 0.059937,\tbuffer size : 31010,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 68.62745098039215\n","error while saving final\n","device is: cuda\n","config : \n"," x debut 30, y_debut 4, max ep 2850, max epsilon 0.3 \n"," Epsilon_decay 712, test_interval 500\n","Save_interval 1000, batch_size 64, buffer_limit 36000 \n"," Methode Mixed Monte Carlo + Atari DQN, gamma_nstep 0.5, nstep 3\n","Start learning from : x_debut 30 y_debut 4\n","[Episode 50]\t rewards globals : 2.157 \tavg rewards : 2.157,\tavg loss: : 0.155694,\tbuffer size : 556,\tepsilon : 28.0%, \t r <=40 0.0, \t r > 40 21.568627450980394\n","[Episode 100]\t rewards globals : 2.673 \tavg rewards : 3.137,\tavg loss: : 0.079016,\tbuffer size : 1181,\tepsilon : 26.2%, \t r <=40 0.0, \t r > 40 31.372549019607842\n","[Episode 150]\t rewards globals : 2.583 \tavg rewards : 2.549,\tavg loss: : 0.069209,\tbuffer size : 1764,\tepsilon : 24.5%, \t r <=40 0.0, \t r > 40 25.49019607843137\n","[Episode 200]\t rewards globals : 2.736 \tavg rewards : 3.137,\tavg loss: : 0.064748,\tbuffer size : 2452,\tepsilon : 22.9%, \t r <=40 0.0, \t r > 40 31.372549019607842\n","[Episode 250]\t rewards globals : 2.629 \tavg rewards : 2.157,\tavg loss: : 0.063133,\tbuffer size : 2936,\tepsilon : 21.4%, \t r <=40 0.0, \t r > 40 21.568627450980394\n","[Episode 300]\t rewards globals : 2.625 \tavg rewards : 2.549,\tavg loss: : 0.059716,\tbuffer size : 3504,\tepsilon : 20.0%, \t r <=40 0.0, \t r > 40 25.49019607843137\n","[Episode 350]\t rewards globals : 2.735 \tavg rewards : 3.529,\tavg loss: : 0.057369,\tbuffer size : 4137,\tepsilon : 18.7%, \t r <=40 0.0, \t r > 40 35.294117647058826\n","[Episode 400]\t rewards globals : 2.818 \tavg rewards : 3.529,\tavg loss: : 0.055680,\tbuffer size : 4699,\tepsilon : 17.5%, \t r <=40 0.0, \t r > 40 35.294117647058826\n","[Episode 450]\t rewards globals : 2.993 \tavg rewards : 4.314,\tavg loss: : 0.056320,\tbuffer size : 5269,\tepsilon : 16.4%, \t r <=40 0.0, \t r > 40 43.13725490196079\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 6.1\n","[task_2_tmax40] 100 run(s) avg rewards : 6.4\n","Point: 6.25\n","[Episode 500]\t rewards globals : 3.034 \tavg rewards : 3.333,\tavg loss: : 0.054538,\tbuffer size : 5856,\tepsilon : 15.4%, \t r <=40 0.0, \t r > 40 33.33333333333333\n","[Episode 550]\t rewards globals : 3.103 \tavg rewards : 3.922,\tavg loss: : 0.054691,\tbuffer size : 6470,\tepsilon : 14.4%, \t r <=40 0.0, \t r > 40 39.21568627450981\n","[Episode 600]\t rewards globals : 3.178 \tavg rewards : 3.922,\tavg loss: : 0.054264,\tbuffer size : 7075,\tepsilon : 13.5%, \t r <=40 0.0, \t r > 40 39.21568627450981\n","[Episode 650]\t rewards globals : 3.272 \tavg rewards : 4.314,\tavg loss: : 0.055250,\tbuffer size : 7708,\tepsilon : 12.6%, \t r <=40 0.0, \t r > 40 43.13725490196079\n","[Episode 700]\t rewards globals : 3.310 \tavg rewards : 3.725,\tavg loss: : 0.055130,\tbuffer size : 8320,\tepsilon : 11.8%, \t r <=40 0.0, \t r > 40 37.254901960784316\n","[Episode 750]\t rewards globals : 3.369 \tavg rewards : 4.118,\tavg loss: : 0.054009,\tbuffer size : 8922,\tepsilon : 11.1%, \t r <=40 0.0, \t r > 40 41.17647058823529\n","[Episode 800]\t rewards globals : 3.346 \tavg rewards : 2.941,\tavg loss: : 0.053676,\tbuffer size : 9549,\tepsilon : 10.4%, \t r <=40 0.0, \t r > 40 29.411764705882355\n","[Episode 850]\t rewards globals : 3.431 \tavg rewards : 4.706,\tavg loss: : 0.054855,\tbuffer size : 10213,\tepsilon : 9.8%, \t r <=40 0.0, \t r > 40 47.05882352941176\n","[Episode 900]\t rewards globals : 3.496 \tavg rewards : 4.510,\tavg loss: : 0.055011,\tbuffer size : 10852,\tepsilon : 9.2%, \t r <=40 0.0, \t r > 40 45.09803921568628\n","[Episode 950]\t rewards globals : 3.565 \tavg rewards : 4.706,\tavg loss: : 0.055491,\tbuffer size : 11487,\tepsilon : 8.6%, \t r <=40 0.0, \t r > 40 47.05882352941176\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 6.8\n","[task_2_tmax40] 100 run(s) avg rewards : 6.4\n","Point: 6.6\n","[Episode 1000]\t rewards globals : 3.596 \tavg rewards : 4.314,\tavg loss: : 0.055480,\tbuffer size : 12166,\tepsilon : 8.1%, \t r <=40 0.0, \t r > 40 43.13725490196079\n","[Episode 1050]\t rewards globals : 3.682 \tavg rewards : 5.294,\tavg loss: : 0.056667,\tbuffer size : 12763,\tepsilon : 7.6%, \t r <=40 0.0, \t r > 40 52.94117647058824\n","[Episode 1100]\t rewards globals : 3.778 \tavg rewards : 5.882,\tavg loss: : 0.056896,\tbuffer size : 13445,\tepsilon : 7.2%, \t r <=40 0.0, \t r > 40 58.82352941176471\n","[Episode 1150]\t rewards globals : 3.875 \tavg rewards : 6.078,\tavg loss: : 0.057026,\tbuffer size : 14144,\tepsilon : 6.8%, \t r <=40 0.0, \t r > 40 60.78431372549019\n","[Episode 1200]\t rewards globals : 3.947 \tavg rewards : 5.686,\tavg loss: : 0.056812,\tbuffer size : 14785,\tepsilon : 6.4%, \t r <=40 0.0, \t r > 40 56.86274509803921\n","[Episode 1250]\t rewards globals : 3.997 \tavg rewards : 5.098,\tavg loss: : 0.057723,\tbuffer size : 15426,\tepsilon : 6.0%, \t r <=40 0.0, \t r > 40 50.98039215686274\n","[Episode 1300]\t rewards globals : 4.066 \tavg rewards : 5.882,\tavg loss: : 0.057920,\tbuffer size : 16101,\tepsilon : 5.7%, \t r <=40 0.0, \t r > 40 58.82352941176471\n","[Episode 1350]\t rewards globals : 4.145 \tavg rewards : 6.078,\tavg loss: : 0.057823,\tbuffer size : 16749,\tepsilon : 5.4%, \t r <=40 0.0, \t r > 40 60.78431372549019\n","[Episode 1400]\t rewards globals : 4.211 \tavg rewards : 6.078,\tavg loss: : 0.058213,\tbuffer size : 17437,\tepsilon : 5.1%, \t r <=40 0.0, \t r > 40 60.78431372549019\n","[Episode 1450]\t rewards globals : 4.280 \tavg rewards : 6.275,\tavg loss: : 0.058654,\tbuffer size : 18147,\tepsilon : 4.8%, \t r <=40 0.0, \t r > 40 62.745098039215684\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 6.2\n","[task_2_tmax40] 100 run(s) avg rewards : 7.1\n","Point: 6.65\n","[Episode 1500]\t rewards globals : 4.290 \tavg rewards : 4.706,\tavg loss: : 0.058745,\tbuffer size : 18823,\tepsilon : 4.5%, \t r <=40 0.0, \t r > 40 47.05882352941176\n","[Episode 1550]\t rewards globals : 4.333 \tavg rewards : 5.490,\tavg loss: : 0.058665,\tbuffer size : 19521,\tepsilon : 4.3%, \t r <=40 0.0, \t r > 40 54.90196078431373\n","[Episode 1600]\t rewards globals : 4.372 \tavg rewards : 5.686,\tavg loss: : 0.059020,\tbuffer size : 20214,\tepsilon : 4.1%, \t r <=40 0.0, \t r > 40 56.86274509803921\n","[Episode 1650]\t rewards globals : 4.428 \tavg rewards : 6.275,\tavg loss: : 0.059480,\tbuffer size : 20892,\tepsilon : 3.9%, \t r <=40 0.0, \t r > 40 62.745098039215684\n","[Episode 1700]\t rewards globals : 4.480 \tavg rewards : 6.275,\tavg loss: : 0.059391,\tbuffer size : 21582,\tepsilon : 3.7%, \t r <=40 0.0, \t r > 40 62.745098039215684\n","[Episode 1750]\t rewards globals : 4.529 \tavg rewards : 6.078,\tavg loss: : 0.059267,\tbuffer size : 22318,\tepsilon : 3.5%, \t r <=40 0.0, \t r > 40 60.78431372549019\n","[Episode 1800]\t rewards globals : 4.542 \tavg rewards : 5.098,\tavg loss: : 0.059535,\tbuffer size : 23000,\tepsilon : 3.3%, \t r <=40 0.0, \t r > 40 50.98039215686274\n","[Episode 1850]\t rewards globals : 4.527 \tavg rewards : 3.922,\tavg loss: : 0.060102,\tbuffer size : 23658,\tepsilon : 3.2%, \t r <=40 0.0, \t r > 40 39.21568627450981\n","[Episode 1900]\t rewards globals : 4.555 \tavg rewards : 5.490,\tavg loss: : 0.060384,\tbuffer size : 24370,\tepsilon : 3.0%, \t r <=40 0.0, \t r > 40 54.90196078431373\n","[Episode 1950]\t rewards globals : 4.577 \tavg rewards : 5.490,\tavg loss: : 0.060494,\tbuffer size : 25050,\tepsilon : 2.9%, \t r <=40 0.0, \t r > 40 54.90196078431373\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 7.3\n","[task_2_tmax40] 100 run(s) avg rewards : 5.5\n","Point: 6.4\n","[Episode 2000]\t rewards globals : 4.628 \tavg rewards : 6.667,\tavg loss: : 0.060505,\tbuffer size : 25700,\tepsilon : 2.7%, \t r <=40 0.0, \t r > 40 66.66666666666666\n","[Episode 2050]\t rewards globals : 4.642 \tavg rewards : 5.098,\tavg loss: : 0.061008,\tbuffer size : 26398,\tepsilon : 2.6%, \t r <=40 0.0, \t r > 40 50.98039215686274\n","[Episode 2100]\t rewards globals : 4.674 \tavg rewards : 6.078,\tavg loss: : 0.061040,\tbuffer size : 27081,\tepsilon : 2.5%, \t r <=40 0.0, \t r > 40 60.78431372549019\n","[Episode 2150]\t rewards globals : 4.691 \tavg rewards : 5.490,\tavg loss: : 0.061161,\tbuffer size : 27757,\tepsilon : 2.4%, \t r <=40 0.0, \t r > 40 54.90196078431373\n","[Episode 2200]\t rewards globals : 4.739 \tavg rewards : 6.863,\tavg loss: : 0.061323,\tbuffer size : 28472,\tepsilon : 2.3%, \t r <=40 0.0, \t r > 40 68.62745098039215\n","[Episode 2250]\t rewards globals : 4.767 \tavg rewards : 5.882,\tavg loss: : 0.062052,\tbuffer size : 29181,\tepsilon : 2.2%, \t r <=40 0.0, \t r > 40 58.82352941176471\n","[Episode 2300]\t rewards globals : 4.789 \tavg rewards : 5.686,\tavg loss: : 0.062473,\tbuffer size : 29885,\tepsilon : 2.1%, \t r <=40 0.0, \t r > 40 56.86274509803921\n","[Episode 2350]\t rewards globals : 4.845 \tavg rewards : 7.255,\tavg loss: : 0.062652,\tbuffer size : 30571,\tepsilon : 2.1%, \t r <=40 0.0, \t r > 40 72.54901960784314\n","[Episode 2400]\t rewards globals : 4.885 \tavg rewards : 6.863,\tavg loss: : 0.062778,\tbuffer size : 31283,\tepsilon : 2.0%, \t r <=40 0.0, \t r > 40 68.62745098039215\n","[Episode 2450]\t rewards globals : 4.912 \tavg rewards : 6.275,\tavg loss: : 0.063481,\tbuffer size : 31991,\tepsilon : 1.9%, \t r <=40 0.0, \t r > 40 62.745098039215684\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 7.1\n","[task_2_tmax40] 100 run(s) avg rewards : 6.7\n","Point: 6.9\n","[Episode 2500]\t rewards globals : 4.962 \tavg rewards : 7.451,\tavg loss: : 0.063670,\tbuffer size : 32687,\tepsilon : 1.9%, \t r <=40 0.0, \t r > 40 74.50980392156863\n","[Episode 2550]\t rewards globals : 4.982 \tavg rewards : 6.078,\tavg loss: : 0.063745,\tbuffer size : 33368,\tepsilon : 1.8%, \t r <=40 0.0, \t r > 40 60.78431372549019\n","[Episode 2600]\t rewards globals : 5.010 \tavg rewards : 6.471,\tavg loss: : 0.063726,\tbuffer size : 34081,\tepsilon : 1.8%, \t r <=40 0.0, \t r > 40 64.70588235294117\n","[Episode 2650]\t rewards globals : 5.047 \tavg rewards : 7.059,\tavg loss: : 0.063903,\tbuffer size : 34787,\tepsilon : 1.7%, \t r <=40 0.0, \t r > 40 70.58823529411765\n","[Episode 2700]\t rewards globals : 5.102 \tavg rewards : 7.843,\tavg loss: : 0.063873,\tbuffer size : 35502,\tepsilon : 1.7%, \t r <=40 0.0, \t r > 40 78.43137254901961\n","[Episode 2750]\t rewards globals : 5.111 \tavg rewards : 5.686,\tavg loss: : 0.064117,\tbuffer size : 36000,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 56.86274509803921\n","[Episode 2800]\t rewards globals : 5.123 \tavg rewards : 5.882,\tavg loss: : 0.064143,\tbuffer size : 36000,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 58.82352941176471\n","error while saving final\n","device is: cuda\n","config : \n"," x debut 30, y_debut 5, max ep 3250, max epsilon 0.3 \n"," Epsilon_decay 812, test_interval 500\n","Save_interval 1000, batch_size 64, buffer_limit 40000 \n"," Methode Mixed Monte Carlo + Atari DQN, gamma_nstep 0.5, nstep 3\n","Start learning from : x_debut 30 y_debut 5\n","[Episode 50]\t rewards globals : 0.980 \tavg rewards : 0.980,\tavg loss: : 0.159584,\tbuffer size : 615,\tepsilon : 28.3%, \t r <=40 0.0, \t r > 40 9.803921568627452\n","[Episode 100]\t rewards globals : 1.386 \tavg rewards : 1.765,\tavg loss: : 0.095816,\tbuffer size : 1234,\tepsilon : 26.6%, \t r <=40 0.0, \t r > 40 17.647058823529413\n","[Episode 150]\t rewards globals : 1.457 \tavg rewards : 1.569,\tavg loss: : 0.083387,\tbuffer size : 1781,\tepsilon : 25.1%, \t r <=40 0.0, \t r > 40 15.686274509803921\n","[Episode 200]\t rewards globals : 1.592 \tavg rewards : 1.961,\tavg loss: : 0.077428,\tbuffer size : 2423,\tepsilon : 23.7%, \t r <=40 0.0, \t r > 40 19.607843137254903\n","[Episode 250]\t rewards globals : 1.434 \tavg rewards : 0.784,\tavg loss: : 0.080161,\tbuffer size : 2968,\tepsilon : 22.3%, \t r <=40 0.0, \t r > 40 7.8431372549019605\n","[Episode 300]\t rewards globals : 1.329 \tavg rewards : 0.784,\tavg loss: : 0.076739,\tbuffer size : 3538,\tepsilon : 21.0%, \t r <=40 0.0, \t r > 40 7.8431372549019605\n","[Episode 350]\t rewards globals : 1.396 \tavg rewards : 1.765,\tavg loss: : 0.073512,\tbuffer size : 4137,\tepsilon : 19.8%, \t r <=40 0.0, \t r > 40 17.647058823529413\n","[Episode 400]\t rewards globals : 1.471 \tavg rewards : 1.961,\tavg loss: : 0.071453,\tbuffer size : 4797,\tepsilon : 18.7%, \t r <=40 0.0, \t r > 40 19.607843137254903\n","[Episode 450]\t rewards globals : 1.552 \tavg rewards : 2.157,\tavg loss: : 0.072360,\tbuffer size : 5401,\tepsilon : 17.7%, \t r <=40 0.0, \t r > 40 21.568627450980394\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 3.0\n","[task_2_tmax40] 100 run(s) avg rewards : 4.3\n","Point: 3.65\n","[Episode 500]\t rewards globals : 1.517 \tavg rewards : 1.176,\tavg loss: : 0.070175,\tbuffer size : 5996,\tepsilon : 16.7%, \t r <=40 0.0, \t r > 40 11.76470588235294\n","[Episode 550]\t rewards globals : 1.470 \tavg rewards : 0.980,\tavg loss: : 0.068802,\tbuffer size : 6575,\tepsilon : 15.7%, \t r <=40 0.0, \t r > 40 9.803921568627452\n","[Episode 600]\t rewards globals : 1.547 \tavg rewards : 2.549,\tavg loss: : 0.068281,\tbuffer size : 7251,\tepsilon : 14.9%, \t r <=40 0.0, \t r > 40 25.49019607843137\n","[Episode 650]\t rewards globals : 1.567 \tavg rewards : 1.765,\tavg loss: : 0.068355,\tbuffer size : 7931,\tepsilon : 14.0%, \t r <=40 0.0, \t r > 40 17.647058823529413\n","[Episode 700]\t rewards globals : 1.626 \tavg rewards : 2.353,\tavg loss: : 0.067465,\tbuffer size : 8598,\tepsilon : 13.2%, \t r <=40 0.0, \t r > 40 23.52941176470588\n","[Episode 750]\t rewards globals : 1.691 \tavg rewards : 2.549,\tavg loss: : 0.066715,\tbuffer size : 9242,\tepsilon : 12.5%, \t r <=40 0.0, \t r > 40 25.49019607843137\n","[Episode 800]\t rewards globals : 1.760 \tavg rewards : 2.941,\tavg loss: : 0.066336,\tbuffer size : 9955,\tepsilon : 11.8%, \t r <=40 0.0, \t r > 40 29.411764705882355\n","[Episode 850]\t rewards globals : 1.763 \tavg rewards : 1.765,\tavg loss: : 0.066566,\tbuffer size : 10595,\tepsilon : 11.2%, \t r <=40 0.0, \t r > 40 17.647058823529413\n","[Episode 900]\t rewards globals : 1.853 \tavg rewards : 3.529,\tavg loss: : 0.066084,\tbuffer size : 11286,\tepsilon : 10.6%, \t r <=40 0.0, \t r > 40 35.294117647058826\n","[Episode 950]\t rewards globals : 1.903 \tavg rewards : 2.745,\tavg loss: : 0.065505,\tbuffer size : 11942,\tepsilon : 10.0%, \t r <=40 0.0, \t r > 40 27.450980392156865\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 3.9\n","[task_2_tmax40] 100 run(s) avg rewards : 4.0\n","Point: 3.95\n","[Episode 1000]\t rewards globals : 1.928 \tavg rewards : 2.353,\tavg loss: : 0.064966,\tbuffer size : 12614,\tepsilon : 9.5%, \t r <=40 0.0, \t r > 40 23.52941176470588\n","[Episode 1050]\t rewards globals : 1.941 \tavg rewards : 2.157,\tavg loss: : 0.065349,\tbuffer size : 13325,\tepsilon : 9.0%, \t r <=40 0.0, \t r > 40 21.568627450980394\n","[Episode 1100]\t rewards globals : 1.998 \tavg rewards : 3.137,\tavg loss: : 0.065308,\tbuffer size : 14030,\tepsilon : 8.5%, \t r <=40 0.0, \t r > 40 31.372549019607842\n","[Episode 1150]\t rewards globals : 2.007 \tavg rewards : 2.157,\tavg loss: : 0.064771,\tbuffer size : 14677,\tepsilon : 8.0%, \t r <=40 0.0, \t r > 40 21.568627450980394\n","[Episode 1200]\t rewards globals : 2.032 \tavg rewards : 2.549,\tavg loss: : 0.064305,\tbuffer size : 15354,\tepsilon : 7.6%, \t r <=40 0.0, \t r > 40 25.49019607843137\n","[Episode 1250]\t rewards globals : 2.054 \tavg rewards : 2.745,\tavg loss: : 0.064689,\tbuffer size : 16001,\tepsilon : 7.2%, \t r <=40 0.0, \t r > 40 27.450980392156865\n","[Episode 1300]\t rewards globals : 2.129 \tavg rewards : 3.922,\tavg loss: : 0.064815,\tbuffer size : 16732,\tepsilon : 6.8%, \t r <=40 0.0, \t r > 40 39.21568627450981\n","[Episode 1350]\t rewards globals : 2.147 \tavg rewards : 2.745,\tavg loss: : 0.064475,\tbuffer size : 17388,\tepsilon : 6.5%, \t r <=40 0.0, \t r > 40 27.450980392156865\n","[Episode 1400]\t rewards globals : 2.134 \tavg rewards : 1.765,\tavg loss: : 0.064236,\tbuffer size : 18059,\tepsilon : 6.2%, \t r <=40 0.0, \t r > 40 17.647058823529413\n","[Episode 1450]\t rewards globals : 2.164 \tavg rewards : 2.941,\tavg loss: : 0.064476,\tbuffer size : 18756,\tepsilon : 5.9%, \t r <=40 0.0, \t r > 40 29.411764705882355\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 4.9\n","[task_2_tmax40] 100 run(s) avg rewards : 4.9\n","Point: 4.9\n","[Episode 1500]\t rewards globals : 2.212 \tavg rewards : 3.725,\tavg loss: : 0.064201,\tbuffer size : 19458,\tepsilon : 5.6%, \t r <=40 0.0, \t r > 40 37.254901960784316\n","[Episode 1550]\t rewards globals : 2.276 \tavg rewards : 4.314,\tavg loss: : 0.064285,\tbuffer size : 20160,\tepsilon : 5.3%, \t r <=40 0.0, \t r > 40 43.13725490196079\n","[Episode 1600]\t rewards globals : 2.311 \tavg rewards : 3.529,\tavg loss: : 0.064511,\tbuffer size : 20884,\tepsilon : 5.0%, \t r <=40 0.0, \t r > 40 35.294117647058826\n","[Episode 1650]\t rewards globals : 2.326 \tavg rewards : 2.745,\tavg loss: : 0.065221,\tbuffer size : 21606,\tepsilon : 4.8%, \t r <=40 0.0, \t r > 40 27.450980392156865\n","[Episode 1700]\t rewards globals : 2.346 \tavg rewards : 3.137,\tavg loss: : 0.065296,\tbuffer size : 22311,\tepsilon : 4.6%, \t r <=40 0.0, \t r > 40 31.372549019607842\n","[Episode 1750]\t rewards globals : 2.359 \tavg rewards : 2.745,\tavg loss: : 0.065332,\tbuffer size : 22973,\tepsilon : 4.4%, \t r <=40 0.0, \t r > 40 27.450980392156865\n","[Episode 1800]\t rewards globals : 2.365 \tavg rewards : 2.549,\tavg loss: : 0.065122,\tbuffer size : 23596,\tepsilon : 4.2%, \t r <=40 0.0, \t r > 40 25.49019607843137\n","[Episode 1850]\t rewards globals : 2.388 \tavg rewards : 3.137,\tavg loss: : 0.065499,\tbuffer size : 24234,\tepsilon : 4.0%, \t r <=40 0.0, \t r > 40 31.372549019607842\n","[Episode 1900]\t rewards globals : 2.415 \tavg rewards : 3.529,\tavg loss: : 0.065285,\tbuffer size : 24962,\tepsilon : 3.8%, \t r <=40 0.0, \t r > 40 35.294117647058826\n","[Episode 1950]\t rewards globals : 2.450 \tavg rewards : 3.725,\tavg loss: : 0.065110,\tbuffer size : 25739,\tepsilon : 3.6%, \t r <=40 0.0, \t r > 40 37.254901960784316\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 4.9\n","[task_2_tmax40] 100 run(s) avg rewards : 3.9\n","Point: 4.4\n","[Episode 2000]\t rewards globals : 2.464 \tavg rewards : 2.941,\tavg loss: : 0.065112,\tbuffer size : 26420,\tepsilon : 3.5%, \t r <=40 0.0, \t r > 40 29.411764705882355\n","[Episode 2050]\t rewards globals : 2.501 \tavg rewards : 3.922,\tavg loss: : 0.065346,\tbuffer size : 27157,\tepsilon : 3.3%, \t r <=40 0.0, \t r > 40 39.21568627450981\n","[Episode 2100]\t rewards globals : 2.527 \tavg rewards : 3.529,\tavg loss: : 0.065450,\tbuffer size : 27898,\tepsilon : 3.2%, \t r <=40 0.0, \t r > 40 35.294117647058826\n","[Episode 2150]\t rewards globals : 2.562 \tavg rewards : 4.118,\tavg loss: : 0.065407,\tbuffer size : 28671,\tepsilon : 3.1%, \t r <=40 0.0, \t r > 40 41.17647058823529\n","[Episode 2200]\t rewards globals : 2.603 \tavg rewards : 4.314,\tavg loss: : 0.065185,\tbuffer size : 29413,\tepsilon : 2.9%, \t r <=40 0.0, \t r > 40 43.13725490196079\n","[Episode 2250]\t rewards globals : 2.648 \tavg rewards : 4.510,\tavg loss: : 0.065297,\tbuffer size : 30131,\tepsilon : 2.8%, \t r <=40 0.0, \t r > 40 45.09803921568628\n","[Episode 2300]\t rewards globals : 2.664 \tavg rewards : 3.333,\tavg loss: : 0.065315,\tbuffer size : 30814,\tepsilon : 2.7%, \t r <=40 0.0, \t r > 40 33.33333333333333\n","[Episode 2350]\t rewards globals : 2.684 \tavg rewards : 3.529,\tavg loss: : 0.065087,\tbuffer size : 31560,\tepsilon : 2.6%, \t r <=40 0.0, \t r > 40 35.294117647058826\n","[Episode 2400]\t rewards globals : 2.720 \tavg rewards : 4.314,\tavg loss: : 0.064843,\tbuffer size : 32326,\tepsilon : 2.5%, \t r <=40 0.0, \t r > 40 43.13725490196079\n","[Episode 2450]\t rewards globals : 2.778 \tavg rewards : 5.686,\tavg loss: : 0.064824,\tbuffer size : 33045,\tepsilon : 2.4%, \t r <=40 0.0, \t r > 40 56.86274509803921\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 3.8\n","[task_2_tmax40] 100 run(s) avg rewards : 5.0\n","Point: 4.4\n","[Episode 2500]\t rewards globals : 2.819 \tavg rewards : 4.902,\tavg loss: : 0.064734,\tbuffer size : 33748,\tepsilon : 2.3%, \t r <=40 0.0, \t r > 40 49.01960784313725\n","[Episode 2550]\t rewards globals : 2.838 \tavg rewards : 3.922,\tavg loss: : 0.064976,\tbuffer size : 34464,\tepsilon : 2.3%, \t r <=40 0.0, \t r > 40 39.21568627450981\n","[Episode 2600]\t rewards globals : 2.864 \tavg rewards : 4.118,\tavg loss: : 0.064889,\tbuffer size : 35163,\tepsilon : 2.2%, \t r <=40 0.0, \t r > 40 41.17647058823529\n","[Episode 2650]\t rewards globals : 2.886 \tavg rewards : 3.922,\tavg loss: : 0.064845,\tbuffer size : 35871,\tepsilon : 2.1%, \t r <=40 0.0, \t r > 40 39.21568627450981\n","[Episode 2700]\t rewards globals : 2.899 \tavg rewards : 3.725,\tavg loss: : 0.064766,\tbuffer size : 36579,\tepsilon : 2.0%, \t r <=40 0.0, \t r > 40 37.254901960784316\n","[Episode 2750]\t rewards globals : 2.912 \tavg rewards : 3.529,\tavg loss: : 0.064800,\tbuffer size : 37304,\tepsilon : 2.0%, \t r <=40 0.0, \t r > 40 35.294117647058826\n","[Episode 2800]\t rewards globals : 2.928 \tavg rewards : 3.922,\tavg loss: : 0.064857,\tbuffer size : 38018,\tepsilon : 1.9%, \t r <=40 0.0, \t r > 40 39.21568627450981\n","[Episode 2850]\t rewards globals : 2.936 \tavg rewards : 3.333,\tavg loss: : 0.065095,\tbuffer size : 38752,\tepsilon : 1.9%, \t r <=40 0.0, \t r > 40 33.33333333333333\n","[Episode 2900]\t rewards globals : 2.971 \tavg rewards : 4.902,\tavg loss: : 0.065223,\tbuffer size : 39504,\tepsilon : 1.8%, \t r <=40 0.0, \t r > 40 49.01960784313725\n","[Episode 2950]\t rewards globals : 2.982 \tavg rewards : 3.529,\tavg loss: : 0.065521,\tbuffer size : 40000,\tepsilon : 1.8%, \t r <=40 0.0, \t r > 40 35.294117647058826\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 4.1\n","[task_2_tmax40] 100 run(s) avg rewards : 4.6\n","Point: 4.35\n","[Episode 3000]\t rewards globals : 2.989 \tavg rewards : 3.333,\tavg loss: : 0.065445,\tbuffer size : 40000,\tepsilon : 1.7%, \t r <=40 0.0, \t r > 40 33.33333333333333\n","[Episode 3050]\t rewards globals : 3.022 \tavg rewards : 5.098,\tavg loss: : 0.065567,\tbuffer size : 40000,\tepsilon : 1.7%, \t r <=40 0.0, \t r > 40 50.98039215686274\n","[Episode 3100]\t rewards globals : 3.044 \tavg rewards : 4.510,\tavg loss: : 0.065527,\tbuffer size : 40000,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 45.09803921568628\n","[Episode 3150]\t rewards globals : 3.056 \tavg rewards : 3.725,\tavg loss: : 0.065812,\tbuffer size : 40000,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 37.254901960784316\n","[Episode 3200]\t rewards globals : 3.071 \tavg rewards : 4.118,\tavg loss: : 0.066051,\tbuffer size : 40000,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 41.17647058823529\n","error while saving final\n","device is: cuda\n","config : \n"," x debut 35, y_debut 0, max ep 1500, max epsilon 0.3 \n"," Epsilon_decay 375, test_interval 500\n","Save_interval 1000, batch_size 64, buffer_limit 24000 \n"," Methode Mixed Monte Carlo + Atari DQN, gamma_nstep 0.5, nstep 3\n","Start learning from : x_debut 35 y_debut 0\n","[Episode 50]\t rewards globals : 3.529 \tavg rewards : 3.529,\tavg loss: : 0.233379,\tbuffer size : 687,\tepsilon : 26.4%, \t r <=40 0.0, \t r > 40 35.294117647058826\n","[Episode 100]\t rewards globals : 3.663 \tavg rewards : 3.725,\tavg loss: : 0.131513,\tbuffer size : 1356,\tepsilon : 23.2%, \t r <=40 0.0, \t r > 40 37.254901960784316\n","[Episode 150]\t rewards globals : 4.106 \tavg rewards : 4.902,\tavg loss: : 0.103864,\tbuffer size : 2030,\tepsilon : 20.4%, \t r <=40 0.0, \t r > 40 49.01960784313725\n","[Episode 200]\t rewards globals : 4.129 \tavg rewards : 4.118,\tavg loss: : 0.092023,\tbuffer size : 2794,\tepsilon : 18.0%, \t r <=40 0.0, \t r > 40 41.17647058823529\n","[Episode 250]\t rewards globals : 4.382 \tavg rewards : 5.294,\tavg loss: : 0.091578,\tbuffer size : 3600,\tepsilon : 15.9%, \t r <=40 0.0, \t r > 40 52.94117647058824\n","[Episode 300]\t rewards globals : 4.518 \tavg rewards : 5.294,\tavg loss: : 0.086528,\tbuffer size : 4307,\tepsilon : 14.0%, \t r <=40 0.0, \t r > 40 52.94117647058824\n","[Episode 350]\t rewards globals : 4.530 \tavg rewards : 4.706,\tavg loss: : 0.082509,\tbuffer size : 5061,\tepsilon : 12.4%, \t r <=40 0.0, \t r > 40 47.05882352941176\n","[Episode 400]\t rewards globals : 4.713 \tavg rewards : 5.882,\tavg loss: : 0.079291,\tbuffer size : 5913,\tepsilon : 11.0%, \t r <=40 0.0, \t r > 40 58.82352941176471\n","[Episode 450]\t rewards globals : 4.900 \tavg rewards : 6.275,\tavg loss: : 0.080104,\tbuffer size : 6723,\tepsilon : 9.7%, \t r <=40 0.0, \t r > 40 62.745098039215684\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 8.3\n","[task_2_tmax40] 100 run(s) avg rewards : 8.1\n","Point: 8.2\n","[Episode 500]\t rewards globals : 5.170 \tavg rewards : 7.647,\tavg loss: : 0.076909,\tbuffer size : 7627,\tepsilon : 8.6%, \t r <=40 0.0, \t r > 40 76.47058823529412\n","[Episode 550]\t rewards globals : 5.281 \tavg rewards : 6.471,\tavg loss: : 0.074556,\tbuffer size : 8461,\tepsilon : 7.7%, \t r <=40 0.0, \t r > 40 64.70588235294117\n","[Episode 600]\t rewards globals : 5.291 \tavg rewards : 5.490,\tavg loss: : 0.074025,\tbuffer size : 9277,\tepsilon : 6.9%, \t r <=40 0.0, \t r > 40 54.90196078431373\n","[Episode 650]\t rewards globals : 5.422 \tavg rewards : 6.863,\tavg loss: : 0.074495,\tbuffer size : 10130,\tepsilon : 6.1%, \t r <=40 0.0, \t r > 40 68.62745098039215\n","[Episode 700]\t rewards globals : 5.578 \tavg rewards : 7.647,\tavg loss: : 0.073281,\tbuffer size : 11032,\tepsilon : 5.5%, \t r <=40 0.0, \t r > 40 76.47058823529412\n","[Episode 750]\t rewards globals : 5.739 \tavg rewards : 7.843,\tavg loss: : 0.071677,\tbuffer size : 11944,\tepsilon : 4.9%, \t r <=40 0.0, \t r > 40 78.43137254901961\n","[Episode 800]\t rewards globals : 5.893 \tavg rewards : 8.039,\tavg loss: : 0.070335,\tbuffer size : 12846,\tepsilon : 4.4%, \t r <=40 0.0, \t r > 40 80.3921568627451\n","[Episode 850]\t rewards globals : 6.052 \tavg rewards : 8.627,\tavg loss: : 0.070523,\tbuffer size : 13817,\tepsilon : 4.0%, \t r <=40 0.0, \t r > 40 86.27450980392157\n","[Episode 900]\t rewards globals : 6.182 \tavg rewards : 8.431,\tavg loss: : 0.069534,\tbuffer size : 14715,\tepsilon : 3.6%, \t r <=40 0.0, \t r > 40 84.31372549019608\n","[Episode 950]\t rewards globals : 6.236 \tavg rewards : 7.255,\tavg loss: : 0.069190,\tbuffer size : 15604,\tepsilon : 3.3%, \t r <=40 0.0, \t r > 40 72.54901960784314\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 9.1\n","[task_2_tmax40] 100 run(s) avg rewards : 9.0\n","Point: 9.05\n","Re do test: double check\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 9.6\n","[task_2_tmax40] 100 run(s) avg rewards : 9.4\n","Point: 9.5\n","Training should stop\n","error while saving final\n","device is: cuda\n","config : \n"," x debut 35, y_debut 1, max ep 1900, max epsilon 0.3 \n"," Epsilon_decay 475, test_interval 500\n","Save_interval 1000, batch_size 64, buffer_limit 28000 \n"," Methode Mixed Monte Carlo + Atari DQN, gamma_nstep 0.5, nstep 3\n","Start learning from : x_debut 35 y_debut 1\n","[Episode 50]\t rewards globals : 2.157 \tavg rewards : 2.157,\tavg loss: : 0.203829,\tbuffer size : 575,\tepsilon : 27.1%, \t r <=40 0.0, \t r > 40 21.568627450980394\n","[Episode 100]\t rewards globals : 2.772 \tavg rewards : 3.333,\tavg loss: : 0.100998,\tbuffer size : 1173,\tepsilon : 24.5%, \t r <=40 0.0, \t r > 40 33.33333333333333\n","[Episode 150]\t rewards globals : 3.510 \tavg rewards : 4.902,\tavg loss: : 0.081713,\tbuffer size : 1839,\tepsilon : 22.1%, \t r <=40 0.0, \t r > 40 49.01960784313725\n","[Episode 200]\t rewards globals : 3.682 \tavg rewards : 4.118,\tavg loss: : 0.072529,\tbuffer size : 2465,\tepsilon : 20.0%, \t r <=40 0.0, \t r > 40 41.17647058823529\n","[Episode 250]\t rewards globals : 3.825 \tavg rewards : 4.314,\tavg loss: : 0.071078,\tbuffer size : 3107,\tepsilon : 18.1%, \t r <=40 0.0, \t r > 40 43.13725490196079\n","[Episode 300]\t rewards globals : 3.854 \tavg rewards : 3.922,\tavg loss: : 0.066763,\tbuffer size : 3776,\tepsilon : 16.4%, \t r <=40 0.0, \t r > 40 39.21568627450981\n","[Episode 350]\t rewards globals : 4.074 \tavg rewards : 5.490,\tavg loss: : 0.065810,\tbuffer size : 4556,\tepsilon : 14.9%, \t r <=40 0.0, \t r > 40 54.90196078431373\n","[Episode 400]\t rewards globals : 4.264 \tavg rewards : 5.490,\tavg loss: : 0.064448,\tbuffer size : 5259,\tepsilon : 13.5%, \t r <=40 0.0, \t r > 40 54.90196078431373\n","[Episode 450]\t rewards globals : 4.390 \tavg rewards : 5.490,\tavg loss: : 0.066169,\tbuffer size : 5998,\tepsilon : 12.2%, \t r <=40 0.0, \t r > 40 54.90196078431373\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 9.3\n","[task_2_tmax40] 100 run(s) avg rewards : 9.3\n","Point: 9.3\n","Re do test: double check\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 8.9\n","[task_2_tmax40] 100 run(s) avg rewards : 9.1\n","Point: 9.0\n","Training should stop\n","error while saving final\n","device is: cuda\n","config : \n"," x debut 35, y_debut 2, max ep 2300, max epsilon 0.3 \n"," Epsilon_decay 575, test_interval 500\n","Save_interval 1000, batch_size 64, buffer_limit 32000 \n"," Methode Mixed Monte Carlo + Atari DQN, gamma_nstep 0.5, nstep 3\n","Start learning from : x_debut 35 y_debut 2\n","[Episode 50]\t rewards globals : 2.941 \tavg rewards : 2.941,\tavg loss: : 0.221116,\tbuffer size : 565,\tepsilon : 27.6%, \t r <=40 0.0, \t r > 40 29.411764705882355\n","[Episode 100]\t rewards globals : 3.366 \tavg rewards : 3.725,\tavg loss: : 0.099329,\tbuffer size : 1166,\tepsilon : 25.4%, \t r <=40 0.0, \t r > 40 37.254901960784316\n","[Episode 150]\t rewards globals : 3.377 \tavg rewards : 3.529,\tavg loss: : 0.080063,\tbuffer size : 1799,\tepsilon : 23.3%, \t r <=40 0.0, \t r > 40 35.294117647058826\n","[Episode 200]\t rewards globals : 3.682 \tavg rewards : 4.510,\tavg loss: : 0.074733,\tbuffer size : 2555,\tepsilon : 21.5%, \t r <=40 0.0, \t r > 40 45.09803921568628\n","[Episode 250]\t rewards globals : 3.825 \tavg rewards : 4.510,\tavg loss: : 0.079115,\tbuffer size : 3261,\tepsilon : 19.8%, \t r <=40 0.0, \t r > 40 45.09803921568628\n","[Episode 300]\t rewards globals : 3.621 \tavg rewards : 2.549,\tavg loss: : 0.077278,\tbuffer size : 3899,\tepsilon : 18.2%, \t r <=40 0.0, \t r > 40 25.49019607843137\n","[Episode 350]\t rewards globals : 3.704 \tavg rewards : 4.118,\tavg loss: : 0.073962,\tbuffer size : 4515,\tepsilon : 16.8%, \t r <=40 0.0, \t r > 40 41.17647058823529\n","[Episode 400]\t rewards globals : 3.990 \tavg rewards : 6.078,\tavg loss: : 0.071432,\tbuffer size : 5194,\tepsilon : 15.5%, \t r <=40 0.0, \t r > 40 60.78431372549019\n","[Episode 450]\t rewards globals : 4.146 \tavg rewards : 5.490,\tavg loss: : 0.072186,\tbuffer size : 5849,\tepsilon : 14.3%, \t r <=40 0.0, \t r > 40 54.90196078431373\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 7.9\n","[task_2_tmax40] 100 run(s) avg rewards : 8.4\n","Point: 8.15\n","[Episode 500]\t rewards globals : 4.212 \tavg rewards : 4.902,\tavg loss: : 0.071783,\tbuffer size : 6550,\tepsilon : 13.2%, \t r <=40 0.0, \t r > 40 49.01960784313725\n","[Episode 550]\t rewards globals : 4.338 \tavg rewards : 5.686,\tavg loss: : 0.071383,\tbuffer size : 7292,\tepsilon : 12.1%, \t r <=40 0.0, \t r > 40 56.86274509803921\n","[Episode 600]\t rewards globals : 4.426 \tavg rewards : 5.490,\tavg loss: : 0.071546,\tbuffer size : 7964,\tepsilon : 11.2%, \t r <=40 0.0, \t r > 40 54.90196078431373\n","[Episode 650]\t rewards globals : 4.593 \tavg rewards : 6.667,\tavg loss: : 0.071528,\tbuffer size : 8738,\tepsilon : 10.4%, \t r <=40 0.0, \t r > 40 66.66666666666666\n","[Episode 700]\t rewards globals : 4.750 \tavg rewards : 6.667,\tavg loss: : 0.070998,\tbuffer size : 9561,\tepsilon : 9.6%, \t r <=40 0.0, \t r > 40 66.66666666666666\n","[Episode 750]\t rewards globals : 4.807 \tavg rewards : 5.490,\tavg loss: : 0.071384,\tbuffer size : 10256,\tepsilon : 8.9%, \t r <=40 0.0, \t r > 40 54.90196078431373\n","[Episode 800]\t rewards globals : 4.844 \tavg rewards : 5.294,\tavg loss: : 0.070699,\tbuffer size : 11030,\tepsilon : 8.2%, \t r <=40 0.0, \t r > 40 52.94117647058824\n","[Episode 850]\t rewards globals : 4.818 \tavg rewards : 4.510,\tavg loss: : 0.071601,\tbuffer size : 11728,\tepsilon : 7.6%, \t r <=40 0.0, \t r > 40 45.09803921568628\n","[Episode 900]\t rewards globals : 4.883 \tavg rewards : 6.078,\tavg loss: : 0.071138,\tbuffer size : 12479,\tepsilon : 7.1%, \t r <=40 0.0, \t r > 40 60.78431372549019\n","[Episode 950]\t rewards globals : 5.005 \tavg rewards : 7.255,\tavg loss: : 0.070695,\tbuffer size : 13275,\tepsilon : 6.6%, \t r <=40 0.0, \t r > 40 72.54901960784314\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 8.2\n","[task_2_tmax40] 100 run(s) avg rewards : 8.6\n","Point: 8.399999999999999\n","[Episode 1000]\t rewards globals : 5.175 \tavg rewards : 8.431,\tavg loss: : 0.070162,\tbuffer size : 14134,\tepsilon : 6.1%, \t r <=40 0.0, \t r > 40 84.31372549019608\n","[Episode 1050]\t rewards globals : 5.224 \tavg rewards : 6.275,\tavg loss: : 0.070402,\tbuffer size : 14952,\tepsilon : 5.7%, \t r <=40 0.0, \t r > 40 62.745098039215684\n","[Episode 1100]\t rewards globals : 5.277 \tavg rewards : 6.471,\tavg loss: : 0.070306,\tbuffer size : 15755,\tepsilon : 5.3%, \t r <=40 0.0, \t r > 40 64.70588235294117\n","[Episode 1150]\t rewards globals : 5.334 \tavg rewards : 6.667,\tavg loss: : 0.069690,\tbuffer size : 16501,\tepsilon : 4.9%, \t r <=40 0.0, \t r > 40 66.66666666666666\n","[Episode 1200]\t rewards globals : 5.445 \tavg rewards : 7.843,\tavg loss: : 0.069323,\tbuffer size : 17333,\tepsilon : 4.6%, \t r <=40 0.0, \t r > 40 78.43137254901961\n","[Episode 1250]\t rewards globals : 5.452 \tavg rewards : 5.686,\tavg loss: : 0.069652,\tbuffer size : 18082,\tepsilon : 4.3%, \t r <=40 0.0, \t r > 40 56.86274509803921\n","[Episode 1300]\t rewards globals : 5.550 \tavg rewards : 7.843,\tavg loss: : 0.069855,\tbuffer size : 18902,\tepsilon : 4.0%, \t r <=40 0.0, \t r > 40 78.43137254901961\n","[Episode 1350]\t rewards globals : 5.611 \tavg rewards : 7.255,\tavg loss: : 0.070156,\tbuffer size : 19642,\tepsilon : 3.8%, \t r <=40 0.0, \t r > 40 72.54901960784314\n","[Episode 1400]\t rewards globals : 5.696 \tavg rewards : 8.039,\tavg loss: : 0.070534,\tbuffer size : 20448,\tepsilon : 3.5%, \t r <=40 0.0, \t r > 40 80.3921568627451\n","[Episode 1450]\t rewards globals : 5.768 \tavg rewards : 7.843,\tavg loss: : 0.071068,\tbuffer size : 21220,\tepsilon : 3.3%, \t r <=40 0.0, \t r > 40 78.43137254901961\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 8.7\n","[task_2_tmax40] 100 run(s) avg rewards : 9.1\n","Point: 8.899999999999999\n","Re do test: double check\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 8.1\n","[task_2_tmax40] 100 run(s) avg rewards : 8.9\n","Point: 8.5\n","[Episode 1500]\t rewards globals : 5.829 \tavg rewards : 7.647,\tavg loss: : 0.070869,\tbuffer size : 22046,\tepsilon : 3.1%, \t r <=40 0.0, \t r > 40 76.47058823529412\n","[Episode 1550]\t rewards globals : 5.906 \tavg rewards : 8.235,\tavg loss: : 0.070566,\tbuffer size : 22853,\tepsilon : 3.0%, \t r <=40 0.0, \t r > 40 82.35294117647058\n","[Episode 1600]\t rewards globals : 5.978 \tavg rewards : 8.235,\tavg loss: : 0.070263,\tbuffer size : 23704,\tepsilon : 2.8%, \t r <=40 0.0, \t r > 40 82.35294117647058\n","[Episode 1650]\t rewards globals : 6.045 \tavg rewards : 8.235,\tavg loss: : 0.070456,\tbuffer size : 24565,\tepsilon : 2.6%, \t r <=40 0.0, \t r > 40 82.35294117647058\n","[Episode 1700]\t rewards globals : 6.085 \tavg rewards : 7.451,\tavg loss: : 0.070150,\tbuffer size : 25328,\tepsilon : 2.5%, \t r <=40 0.0, \t r > 40 74.50980392156863\n","[Episode 1750]\t rewards globals : 6.105 \tavg rewards : 6.863,\tavg loss: : 0.069896,\tbuffer size : 26084,\tepsilon : 2.4%, \t r <=40 0.0, \t r > 40 68.62745098039215\n","[Episode 1800]\t rewards globals : 6.163 \tavg rewards : 8.235,\tavg loss: : 0.070007,\tbuffer size : 26941,\tepsilon : 2.3%, \t r <=40 0.0, \t r > 40 82.35294117647058\n","[Episode 1850]\t rewards globals : 6.202 \tavg rewards : 7.647,\tavg loss: : 0.070460,\tbuffer size : 27765,\tepsilon : 2.2%, \t r <=40 0.0, \t r > 40 76.47058823529412\n","[Episode 1900]\t rewards globals : 6.244 \tavg rewards : 7.843,\tavg loss: : 0.070633,\tbuffer size : 28568,\tepsilon : 2.1%, \t r <=40 0.0, \t r > 40 78.43137254901961\n","[Episode 1950]\t rewards globals : 6.284 \tavg rewards : 7.647,\tavg loss: : 0.070560,\tbuffer size : 29381,\tepsilon : 2.0%, \t r <=40 0.0, \t r > 40 76.47058823529412\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 8.1\n","[task_2_tmax40] 100 run(s) avg rewards : 8.9\n","Point: 8.5\n","[Episode 2000]\t rewards globals : 6.322 \tavg rewards : 7.843,\tavg loss: : 0.070448,\tbuffer size : 30214,\tepsilon : 1.9%, \t r <=40 0.0, \t r > 40 78.43137254901961\n","[Episode 2050]\t rewards globals : 6.353 \tavg rewards : 7.647,\tavg loss: : 0.070771,\tbuffer size : 31006,\tepsilon : 1.8%, \t r <=40 0.0, \t r > 40 76.47058823529412\n","[Episode 2100]\t rewards globals : 6.402 \tavg rewards : 8.431,\tavg loss: : 0.070902,\tbuffer size : 31866,\tepsilon : 1.8%, \t r <=40 0.0, \t r > 40 84.31372549019608\n","[Episode 2150]\t rewards globals : 6.457 \tavg rewards : 8.824,\tavg loss: : 0.070821,\tbuffer size : 32000,\tepsilon : 1.7%, \t r <=40 0.0, \t r > 40 88.23529411764706\n","[Episode 2200]\t rewards globals : 6.474 \tavg rewards : 7.255,\tavg loss: : 0.070792,\tbuffer size : 32000,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 72.54901960784314\n","[Episode 2250]\t rewards globals : 6.495 \tavg rewards : 7.451,\tavg loss: : 0.071301,\tbuffer size : 32000,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 74.50980392156863\n","error while saving final\n","device is: cuda\n","config : \n"," x debut 35, y_debut 3, max ep 2700, max epsilon 0.3 \n"," Epsilon_decay 675, test_interval 500\n","Save_interval 1000, batch_size 64, buffer_limit 36000 \n"," Methode Mixed Monte Carlo + Atari DQN, gamma_nstep 0.5, nstep 3\n","Start learning from : x_debut 35 y_debut 3\n","[Episode 50]\t rewards globals : 1.961 \tavg rewards : 1.961,\tavg loss: : 0.224171,\tbuffer size : 584,\tepsilon : 27.9%, \t r <=40 0.0, \t r > 40 19.607843137254903\n","[Episode 100]\t rewards globals : 2.475 \tavg rewards : 2.941,\tavg loss: : 0.116803,\tbuffer size : 1226,\tepsilon : 26.0%, \t r <=40 0.0, \t r > 40 29.411764705882355\n","[Episode 150]\t rewards globals : 2.384 \tavg rewards : 2.157,\tavg loss: : 0.097399,\tbuffer size : 1901,\tepsilon : 24.2%, \t r <=40 0.0, \t r > 40 21.568627450980394\n","[Episode 200]\t rewards globals : 2.488 \tavg rewards : 2.745,\tavg loss: : 0.091642,\tbuffer size : 2572,\tepsilon : 22.6%, \t r <=40 0.0, \t r > 40 27.450980392156865\n","[Episode 250]\t rewards globals : 2.709 \tavg rewards : 3.529,\tavg loss: : 0.090480,\tbuffer size : 3259,\tepsilon : 21.0%, \t r <=40 0.0, \t r > 40 35.294117647058826\n","[Episode 300]\t rewards globals : 2.957 \tavg rewards : 4.118,\tavg loss: : 0.085472,\tbuffer size : 4005,\tepsilon : 19.6%, \t r <=40 0.0, \t r > 40 41.17647058823529\n","[Episode 350]\t rewards globals : 3.077 \tavg rewards : 3.725,\tavg loss: : 0.080057,\tbuffer size : 4671,\tepsilon : 18.3%, \t r <=40 0.0, \t r > 40 37.254901960784316\n","[Episode 400]\t rewards globals : 3.217 \tavg rewards : 4.118,\tavg loss: : 0.077102,\tbuffer size : 5380,\tepsilon : 17.0%, \t r <=40 0.0, \t r > 40 41.17647058823529\n","[Episode 450]\t rewards globals : 3.282 \tavg rewards : 3.922,\tavg loss: : 0.078740,\tbuffer size : 6062,\tepsilon : 15.9%, \t r <=40 0.0, \t r > 40 39.21568627450981\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 6.3\n","[task_2_tmax40] 100 run(s) avg rewards : 6.8\n","Point: 6.55\n","[Episode 500]\t rewards globals : 3.353 \tavg rewards : 3.922,\tavg loss: : 0.077776,\tbuffer size : 6792,\tepsilon : 14.8%, \t r <=40 0.0, \t r > 40 39.21568627450981\n","[Episode 550]\t rewards globals : 3.539 \tavg rewards : 5.294,\tavg loss: : 0.076612,\tbuffer size : 7482,\tepsilon : 13.8%, \t r <=40 0.0, \t r > 40 52.94117647058824\n","[Episode 600]\t rewards globals : 3.644 \tavg rewards : 4.902,\tavg loss: : 0.075905,\tbuffer size : 8174,\tepsilon : 12.9%, \t r <=40 0.0, \t r > 40 49.01960784313725\n","[Episode 650]\t rewards globals : 3.733 \tavg rewards : 4.902,\tavg loss: : 0.077602,\tbuffer size : 8897,\tepsilon : 12.1%, \t r <=40 0.0, \t r > 40 49.01960784313725\n","[Episode 700]\t rewards globals : 3.795 \tavg rewards : 4.510,\tavg loss: : 0.076702,\tbuffer size : 9600,\tepsilon : 11.3%, \t r <=40 0.0, \t r > 40 45.09803921568628\n","[Episode 750]\t rewards globals : 3.928 \tavg rewards : 5.686,\tavg loss: : 0.076103,\tbuffer size : 10314,\tepsilon : 10.5%, \t r <=40 0.0, \t r > 40 56.86274509803921\n","[Episode 800]\t rewards globals : 4.132 \tavg rewards : 7.255,\tavg loss: : 0.075346,\tbuffer size : 11055,\tepsilon : 9.9%, \t r <=40 0.0, \t r > 40 72.54901960784314\n","[Episode 850]\t rewards globals : 4.277 \tavg rewards : 6.667,\tavg loss: : 0.076420,\tbuffer size : 11820,\tepsilon : 9.2%, \t r <=40 0.0, \t r > 40 66.66666666666666\n","[Episode 900]\t rewards globals : 4.329 \tavg rewards : 5.294,\tavg loss: : 0.075928,\tbuffer size : 12520,\tepsilon : 8.6%, \t r <=40 0.0, \t r > 40 52.94117647058824\n","[Episode 950]\t rewards globals : 4.427 \tavg rewards : 6.078,\tavg loss: : 0.075125,\tbuffer size : 13249,\tepsilon : 8.1%, \t r <=40 0.0, \t r > 40 60.78431372549019\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 7.5\n","[task_2_tmax40] 100 run(s) avg rewards : 7.6\n","Point: 7.55\n","[Episode 1000]\t rewards globals : 4.466 \tavg rewards : 5.294,\tavg loss: : 0.075342,\tbuffer size : 14017,\tepsilon : 7.6%, \t r <=40 0.0, \t r > 40 52.94117647058824\n","[Episode 1050]\t rewards globals : 4.548 \tavg rewards : 6.078,\tavg loss: : 0.076104,\tbuffer size : 14788,\tepsilon : 7.1%, \t r <=40 0.0, \t r > 40 60.78431372549019\n","[Episode 1100]\t rewards globals : 4.623 \tavg rewards : 6.078,\tavg loss: : 0.075855,\tbuffer size : 15568,\tepsilon : 6.7%, \t r <=40 0.0, \t r > 40 60.78431372549019\n","[Episode 1150]\t rewards globals : 4.674 \tavg rewards : 5.882,\tavg loss: : 0.076202,\tbuffer size : 16285,\tepsilon : 6.3%, \t r <=40 0.0, \t r > 40 58.82352941176471\n","[Episode 1200]\t rewards globals : 4.721 \tavg rewards : 5.882,\tavg loss: : 0.075882,\tbuffer size : 16975,\tepsilon : 5.9%, \t r <=40 0.0, \t r > 40 58.82352941176471\n","[Episode 1250]\t rewards globals : 4.764 \tavg rewards : 5.882,\tavg loss: : 0.076580,\tbuffer size : 17675,\tepsilon : 5.6%, \t r <=40 0.0, \t r > 40 58.82352941176471\n","[Episode 1300]\t rewards globals : 4.842 \tavg rewards : 6.667,\tavg loss: : 0.076194,\tbuffer size : 18403,\tepsilon : 5.2%, \t r <=40 0.0, \t r > 40 66.66666666666666\n","[Episode 1350]\t rewards globals : 4.893 \tavg rewards : 6.275,\tavg loss: : 0.075826,\tbuffer size : 19130,\tepsilon : 4.9%, \t r <=40 0.0, \t r > 40 62.745098039215684\n","[Episode 1400]\t rewards globals : 4.975 \tavg rewards : 7.059,\tavg loss: : 0.076061,\tbuffer size : 19876,\tepsilon : 4.6%, \t r <=40 0.0, \t r > 40 70.58823529411765\n","[Episode 1450]\t rewards globals : 5.059 \tavg rewards : 7.451,\tavg loss: : 0.076160,\tbuffer size : 20659,\tepsilon : 4.4%, \t r <=40 0.0, \t r > 40 74.50980392156863\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 8.0\n","[task_2_tmax40] 100 run(s) avg rewards : 7.9\n","Point: 7.95\n","[Episode 1500]\t rewards globals : 5.110 \tavg rewards : 6.471,\tavg loss: : 0.076053,\tbuffer size : 21392,\tepsilon : 4.1%, \t r <=40 0.0, \t r > 40 64.70588235294117\n","[Episode 1550]\t rewards globals : 5.171 \tavg rewards : 6.863,\tavg loss: : 0.075819,\tbuffer size : 22137,\tepsilon : 3.9%, \t r <=40 0.0, \t r > 40 68.62745098039215\n","[Episode 1600]\t rewards globals : 5.222 \tavg rewards : 6.863,\tavg loss: : 0.075943,\tbuffer size : 22901,\tepsilon : 3.7%, \t r <=40 0.0, \t r > 40 68.62745098039215\n","[Episode 1650]\t rewards globals : 5.245 \tavg rewards : 6.078,\tavg loss: : 0.076453,\tbuffer size : 23662,\tepsilon : 3.5%, \t r <=40 0.0, \t r > 40 60.78431372549019\n","[Episode 1700]\t rewards globals : 5.267 \tavg rewards : 6.078,\tavg loss: : 0.076827,\tbuffer size : 24406,\tepsilon : 3.3%, \t r <=40 0.0, \t r > 40 60.78431372549019\n","[Episode 1750]\t rewards globals : 5.283 \tavg rewards : 5.882,\tavg loss: : 0.076808,\tbuffer size : 25165,\tepsilon : 3.2%, \t r <=40 0.0, \t r > 40 58.82352941176471\n","[Episode 1800]\t rewards globals : 5.347 \tavg rewards : 7.647,\tavg loss: : 0.076945,\tbuffer size : 25971,\tepsilon : 3.0%, \t r <=40 0.0, \t r > 40 76.47058823529412\n","[Episode 1850]\t rewards globals : 5.354 \tavg rewards : 5.686,\tavg loss: : 0.077585,\tbuffer size : 26686,\tepsilon : 2.9%, \t r <=40 0.0, \t r > 40 56.86274509803921\n","[Episode 1900]\t rewards globals : 5.402 \tavg rewards : 7.255,\tavg loss: : 0.077756,\tbuffer size : 27456,\tepsilon : 2.7%, \t r <=40 0.0, \t r > 40 72.54901960784314\n","[Episode 1950]\t rewards globals : 5.464 \tavg rewards : 7.843,\tavg loss: : 0.077869,\tbuffer size : 28244,\tepsilon : 2.6%, \t r <=40 0.0, \t r > 40 78.43137254901961\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 7.8\n","[task_2_tmax40] 100 run(s) avg rewards : 8.1\n","Point: 7.949999999999999\n","[Episode 2000]\t rewards globals : 5.502 \tavg rewards : 7.059,\tavg loss: : 0.078077,\tbuffer size : 28993,\tepsilon : 2.5%, \t r <=40 0.0, \t r > 40 70.58823529411765\n","[Episode 2050]\t rewards globals : 5.573 \tavg rewards : 8.431,\tavg loss: : 0.078526,\tbuffer size : 29804,\tepsilon : 2.4%, \t r <=40 0.0, \t r > 40 84.31372549019608\n","[Episode 2100]\t rewards globals : 5.645 \tavg rewards : 8.627,\tavg loss: : 0.078877,\tbuffer size : 30626,\tepsilon : 2.3%, \t r <=40 0.0, \t r > 40 86.27450980392157\n","[Episode 2150]\t rewards globals : 5.695 \tavg rewards : 7.843,\tavg loss: : 0.078779,\tbuffer size : 31379,\tepsilon : 2.2%, \t r <=40 0.0, \t r > 40 78.43137254901961\n","[Episode 2200]\t rewards globals : 5.738 \tavg rewards : 7.647,\tavg loss: : 0.078651,\tbuffer size : 32164,\tepsilon : 2.1%, \t r <=40 0.0, \t r > 40 76.47058823529412\n","[Episode 2250]\t rewards globals : 5.757 \tavg rewards : 6.667,\tavg loss: : 0.079067,\tbuffer size : 32896,\tepsilon : 2.0%, \t r <=40 0.0, \t r > 40 66.66666666666666\n","[Episode 2300]\t rewards globals : 5.771 \tavg rewards : 6.275,\tavg loss: : 0.079682,\tbuffer size : 33642,\tepsilon : 2.0%, \t r <=40 0.0, \t r > 40 62.745098039215684\n","[Episode 2350]\t rewards globals : 5.810 \tavg rewards : 7.647,\tavg loss: : 0.079545,\tbuffer size : 34396,\tepsilon : 1.9%, \t r <=40 0.0, \t r > 40 76.47058823529412\n","[Episode 2400]\t rewards globals : 5.848 \tavg rewards : 7.647,\tavg loss: : 0.079405,\tbuffer size : 35189,\tepsilon : 1.8%, \t r <=40 0.0, \t r > 40 76.47058823529412\n","[Episode 2450]\t rewards globals : 5.859 \tavg rewards : 6.275,\tavg loss: : 0.079911,\tbuffer size : 35901,\tepsilon : 1.8%, \t r <=40 0.0, \t r > 40 62.745098039215684\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 6.9\n","[task_2_tmax40] 100 run(s) avg rewards : 7.8\n","Point: 7.35\n","[Episode 2500]\t rewards globals : 5.878 \tavg rewards : 6.667,\tavg loss: : 0.080263,\tbuffer size : 36000,\tepsilon : 1.7%, \t r <=40 0.0, \t r > 40 66.66666666666666\n","[Episode 2550]\t rewards globals : 5.900 \tavg rewards : 6.863,\tavg loss: : 0.080347,\tbuffer size : 36000,\tepsilon : 1.7%, \t r <=40 0.0, \t r > 40 68.62745098039215\n","[Episode 2600]\t rewards globals : 5.928 \tavg rewards : 7.451,\tavg loss: : 0.080500,\tbuffer size : 36000,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 74.50980392156863\n","[Episode 2650]\t rewards globals : 5.964 \tavg rewards : 7.843,\tavg loss: : 0.080950,\tbuffer size : 36000,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 78.43137254901961\n","error while saving final\n","device is: cuda\n","config : \n"," x debut 35, y_debut 4, max ep 3100, max epsilon 0.3 \n"," Epsilon_decay 775, test_interval 500\n","Save_interval 1000, batch_size 64, buffer_limit 40000 \n"," Methode Mixed Monte Carlo + Atari DQN, gamma_nstep 0.5, nstep 3\n","Start learning from : x_debut 35 y_debut 4\n","[Episode 50]\t rewards globals : 1.176 \tavg rewards : 1.176,\tavg loss: : 0.172595,\tbuffer size : 566,\tepsilon : 28.2%, \t r <=40 0.0, \t r > 40 11.76470588235294\n","[Episode 100]\t rewards globals : 1.782 \tavg rewards : 2.549,\tavg loss: : 0.090559,\tbuffer size : 1103,\tepsilon : 26.5%, \t r <=40 0.0, \t r > 40 25.49019607843137\n","[Episode 150]\t rewards globals : 1.987 \tavg rewards : 2.549,\tavg loss: : 0.078664,\tbuffer size : 1757,\tepsilon : 24.9%, \t r <=40 0.0, \t r > 40 25.49019607843137\n","[Episode 200]\t rewards globals : 2.139 \tavg rewards : 2.745,\tavg loss: : 0.073956,\tbuffer size : 2353,\tepsilon : 23.4%, \t r <=40 0.0, \t r > 40 27.450980392156865\n","[Episode 250]\t rewards globals : 2.231 \tavg rewards : 2.549,\tavg loss: : 0.073379,\tbuffer size : 3052,\tepsilon : 22.0%, \t r <=40 0.0, \t r > 40 25.49019607843137\n","[Episode 300]\t rewards globals : 2.159 \tavg rewards : 1.765,\tavg loss: : 0.069865,\tbuffer size : 3593,\tepsilon : 20.7%, \t r <=40 0.0, \t r > 40 17.647058823529413\n","[Episode 350]\t rewards globals : 2.279 \tavg rewards : 2.941,\tavg loss: : 0.069552,\tbuffer size : 4268,\tepsilon : 19.5%, \t r <=40 0.0, \t r > 40 29.411764705882355\n","[Episode 400]\t rewards globals : 2.319 \tavg rewards : 2.549,\tavg loss: : 0.070218,\tbuffer size : 4903,\tepsilon : 18.3%, \t r <=40 0.0, \t r > 40 25.49019607843137\n","[Episode 450]\t rewards globals : 2.350 \tavg rewards : 2.549,\tavg loss: : 0.072366,\tbuffer size : 5574,\tepsilon : 17.2%, \t r <=40 0.0, \t r > 40 25.49019607843137\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 6.2\n","[task_2_tmax40] 100 run(s) avg rewards : 6.6\n","Point: 6.4\n","[Episode 500]\t rewards globals : 2.375 \tavg rewards : 2.549,\tavg loss: : 0.073157,\tbuffer size : 6234,\tepsilon : 16.2%, \t r <=40 0.0, \t r > 40 25.49019607843137\n","[Episode 550]\t rewards globals : 2.414 \tavg rewards : 2.941,\tavg loss: : 0.072841,\tbuffer size : 6902,\tepsilon : 15.3%, \t r <=40 0.0, \t r > 40 29.411764705882355\n","[Episode 600]\t rewards globals : 2.496 \tavg rewards : 3.333,\tavg loss: : 0.071913,\tbuffer size : 7660,\tepsilon : 14.4%, \t r <=40 0.0, \t r > 40 33.33333333333333\n","[Episode 650]\t rewards globals : 2.581 \tavg rewards : 3.529,\tavg loss: : 0.073185,\tbuffer size : 8383,\tepsilon : 13.5%, \t r <=40 0.0, \t r > 40 35.294117647058826\n","[Episode 700]\t rewards globals : 2.653 \tavg rewards : 3.725,\tavg loss: : 0.072756,\tbuffer size : 8999,\tepsilon : 12.8%, \t r <=40 0.0, \t r > 40 37.254901960784316\n","[Episode 750]\t rewards globals : 2.676 \tavg rewards : 2.941,\tavg loss: : 0.072374,\tbuffer size : 9695,\tepsilon : 12.0%, \t r <=40 0.0, \t r > 40 29.411764705882355\n","[Episode 800]\t rewards globals : 2.734 \tavg rewards : 3.725,\tavg loss: : 0.071811,\tbuffer size : 10396,\tepsilon : 11.3%, \t r <=40 0.0, \t r > 40 37.254901960784316\n","[Episode 850]\t rewards globals : 2.785 \tavg rewards : 3.529,\tavg loss: : 0.072329,\tbuffer size : 11150,\tepsilon : 10.7%, \t r <=40 0.0, \t r > 40 35.294117647058826\n","[Episode 900]\t rewards globals : 2.886 \tavg rewards : 4.510,\tavg loss: : 0.072228,\tbuffer size : 11871,\tepsilon : 10.1%, \t r <=40 0.0, \t r > 40 45.09803921568628\n","[Episode 950]\t rewards globals : 2.944 \tavg rewards : 4.118,\tavg loss: : 0.072275,\tbuffer size : 12578,\tepsilon : 9.5%, \t r <=40 0.0, \t r > 40 41.17647058823529\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 6.8\n","[task_2_tmax40] 100 run(s) avg rewards : 6.7\n","Point: 6.75\n","[Episode 1000]\t rewards globals : 3.027 \tavg rewards : 4.706,\tavg loss: : 0.072128,\tbuffer size : 13346,\tepsilon : 9.0%, \t r <=40 0.0, \t r > 40 47.05882352941176\n","[Episode 1050]\t rewards globals : 3.083 \tavg rewards : 4.118,\tavg loss: : 0.072639,\tbuffer size : 14064,\tepsilon : 8.5%, \t r <=40 0.0, \t r > 40 41.17647058823529\n","[Episode 1100]\t rewards globals : 3.143 \tavg rewards : 4.314,\tavg loss: : 0.073018,\tbuffer size : 14793,\tepsilon : 8.0%, \t r <=40 0.0, \t r > 40 43.13725490196079\n","[Episode 1150]\t rewards globals : 3.223 \tavg rewards : 5.098,\tavg loss: : 0.072941,\tbuffer size : 15520,\tepsilon : 7.6%, \t r <=40 0.0, \t r > 40 50.98039215686274\n","[Episode 1200]\t rewards globals : 3.231 \tavg rewards : 3.333,\tavg loss: : 0.072943,\tbuffer size : 16225,\tepsilon : 7.2%, \t r <=40 0.0, \t r > 40 33.33333333333333\n","[Episode 1250]\t rewards globals : 3.325 \tavg rewards : 5.490,\tavg loss: : 0.073633,\tbuffer size : 16965,\tepsilon : 6.8%, \t r <=40 0.0, \t r > 40 54.90196078431373\n","[Episode 1300]\t rewards globals : 3.436 \tavg rewards : 6.078,\tavg loss: : 0.073934,\tbuffer size : 17748,\tepsilon : 6.4%, \t r <=40 0.0, \t r > 40 60.78431372549019\n","[Episode 1350]\t rewards globals : 3.501 \tavg rewards : 5.098,\tavg loss: : 0.073874,\tbuffer size : 18531,\tepsilon : 6.1%, \t r <=40 0.0, \t r > 40 50.98039215686274\n","[Episode 1400]\t rewards globals : 3.562 \tavg rewards : 5.294,\tavg loss: : 0.073559,\tbuffer size : 19293,\tepsilon : 5.8%, \t r <=40 0.0, \t r > 40 52.94117647058824\n","[Episode 1450]\t rewards globals : 3.618 \tavg rewards : 5.098,\tavg loss: : 0.074321,\tbuffer size : 20025,\tepsilon : 5.5%, \t r <=40 0.0, \t r > 40 50.98039215686274\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 6.7\n","[task_2_tmax40] 100 run(s) avg rewards : 6.8\n","Point: 6.75\n","[Episode 1500]\t rewards globals : 3.698 \tavg rewards : 6.078,\tavg loss: : 0.074422,\tbuffer size : 20783,\tepsilon : 5.2%, \t r <=40 0.0, \t r > 40 60.78431372549019\n","[Episode 1550]\t rewards globals : 3.720 \tavg rewards : 4.314,\tavg loss: : 0.074266,\tbuffer size : 21485,\tepsilon : 4.9%, \t r <=40 0.0, \t r > 40 43.13725490196079\n","[Episode 1600]\t rewards globals : 3.773 \tavg rewards : 5.294,\tavg loss: : 0.074246,\tbuffer size : 22244,\tepsilon : 4.7%, \t r <=40 0.0, \t r > 40 52.94117647058824\n","[Episode 1650]\t rewards globals : 3.834 \tavg rewards : 5.882,\tavg loss: : 0.074842,\tbuffer size : 23033,\tepsilon : 4.4%, \t r <=40 0.0, \t r > 40 58.82352941176471\n","[Episode 1700]\t rewards globals : 3.874 \tavg rewards : 5.098,\tavg loss: : 0.075103,\tbuffer size : 23761,\tepsilon : 4.2%, \t r <=40 0.0, \t r > 40 50.98039215686274\n","[Episode 1750]\t rewards globals : 3.918 \tavg rewards : 5.490,\tavg loss: : 0.075255,\tbuffer size : 24477,\tepsilon : 4.0%, \t r <=40 0.0, \t r > 40 54.90196078431373\n","[Episode 1800]\t rewards globals : 3.976 \tavg rewards : 6.078,\tavg loss: : 0.075180,\tbuffer size : 25275,\tepsilon : 3.8%, \t r <=40 0.0, \t r > 40 60.78431372549019\n","[Episode 1850]\t rewards globals : 4.046 \tavg rewards : 6.471,\tavg loss: : 0.075716,\tbuffer size : 26062,\tepsilon : 3.7%, \t r <=40 0.0, \t r > 40 64.70588235294117\n","[Episode 1900]\t rewards globals : 4.087 \tavg rewards : 5.686,\tavg loss: : 0.075951,\tbuffer size : 26841,\tepsilon : 3.5%, \t r <=40 0.0, \t r > 40 56.86274509803921\n","[Episode 1950]\t rewards globals : 4.152 \tavg rewards : 6.471,\tavg loss: : 0.075829,\tbuffer size : 27609,\tepsilon : 3.3%, \t r <=40 0.0, \t r > 40 64.70588235294117\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 6.6\n","[task_2_tmax40] 100 run(s) avg rewards : 6.6\n","Point: 6.6\n","[Episode 2000]\t rewards globals : 4.208 \tavg rewards : 6.275,\tavg loss: : 0.075982,\tbuffer size : 28418,\tepsilon : 3.2%, \t r <=40 0.0, \t r > 40 62.745098039215684\n","[Episode 2050]\t rewards globals : 4.237 \tavg rewards : 5.490,\tavg loss: : 0.076217,\tbuffer size : 29170,\tepsilon : 3.1%, \t r <=40 0.0, \t r > 40 54.90196078431373\n","[Episode 2100]\t rewards globals : 4.269 \tavg rewards : 5.686,\tavg loss: : 0.076379,\tbuffer size : 29975,\tepsilon : 2.9%, \t r <=40 0.0, \t r > 40 56.86274509803921\n","[Episode 2150]\t rewards globals : 4.319 \tavg rewards : 6.471,\tavg loss: : 0.076369,\tbuffer size : 30775,\tepsilon : 2.8%, \t r <=40 0.0, \t r > 40 64.70588235294117\n","[Episode 2200]\t rewards globals : 4.348 \tavg rewards : 5.686,\tavg loss: : 0.076243,\tbuffer size : 31520,\tepsilon : 2.7%, \t r <=40 0.0, \t r > 40 56.86274509803921\n","[Episode 2250]\t rewards globals : 4.402 \tavg rewards : 6.863,\tavg loss: : 0.076598,\tbuffer size : 32322,\tepsilon : 2.6%, \t r <=40 0.0, \t r > 40 68.62745098039215\n","[Episode 2300]\t rewards globals : 4.437 \tavg rewards : 5.882,\tavg loss: : 0.076789,\tbuffer size : 33143,\tepsilon : 2.5%, \t r <=40 0.0, \t r > 40 58.82352941176471\n","[Episode 2350]\t rewards globals : 4.479 \tavg rewards : 6.471,\tavg loss: : 0.076790,\tbuffer size : 33943,\tepsilon : 2.4%, \t r <=40 0.0, \t r > 40 64.70588235294117\n","[Episode 2400]\t rewards globals : 4.519 \tavg rewards : 6.471,\tavg loss: : 0.076960,\tbuffer size : 34716,\tepsilon : 2.3%, \t r <=40 0.0, \t r > 40 64.70588235294117\n","[Episode 2450]\t rewards globals : 4.574 \tavg rewards : 7.255,\tavg loss: : 0.077392,\tbuffer size : 35555,\tepsilon : 2.2%, \t r <=40 0.0, \t r > 40 72.54901960784314\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 6.3\n","[task_2_tmax40] 100 run(s) avg rewards : 6.4\n","Point: 6.35\n","[Episode 2500]\t rewards globals : 4.590 \tavg rewards : 5.294,\tavg loss: : 0.077630,\tbuffer size : 36338,\tepsilon : 2.2%, \t r <=40 0.0, \t r > 40 52.94117647058824\n","[Episode 2550]\t rewards globals : 4.626 \tavg rewards : 6.275,\tavg loss: : 0.077589,\tbuffer size : 37138,\tepsilon : 2.1%, \t r <=40 0.0, \t r > 40 62.745098039215684\n","[Episode 2600]\t rewards globals : 4.667 \tavg rewards : 6.667,\tavg loss: : 0.077732,\tbuffer size : 37912,\tepsilon : 2.0%, \t r <=40 0.0, \t r > 40 66.66666666666666\n","[Episode 2650]\t rewards globals : 4.693 \tavg rewards : 5.882,\tavg loss: : 0.077850,\tbuffer size : 38708,\tepsilon : 1.9%, \t r <=40 0.0, \t r > 40 58.82352941176471\n","[Episode 2700]\t rewards globals : 4.732 \tavg rewards : 6.863,\tavg loss: : 0.078235,\tbuffer size : 39518,\tepsilon : 1.9%, \t r <=40 0.0, \t r > 40 68.62745098039215\n","[Episode 2750]\t rewards globals : 4.780 \tavg rewards : 7.255,\tavg loss: : 0.078219,\tbuffer size : 40000,\tepsilon : 1.8%, \t r <=40 0.0, \t r > 40 72.54901960784314\n","[Episode 2800]\t rewards globals : 4.802 \tavg rewards : 6.078,\tavg loss: : 0.078309,\tbuffer size : 40000,\tepsilon : 1.8%, \t r <=40 0.0, \t r > 40 60.78431372549019\n","[Episode 2850]\t rewards globals : 4.837 \tavg rewards : 6.863,\tavg loss: : 0.078607,\tbuffer size : 40000,\tepsilon : 1.7%, \t r <=40 0.0, \t r > 40 68.62745098039215\n","[Episode 2900]\t rewards globals : 4.860 \tavg rewards : 6.275,\tavg loss: : 0.078909,\tbuffer size : 40000,\tepsilon : 1.7%, \t r <=40 0.0, \t r > 40 62.745098039215684\n","[Episode 2950]\t rewards globals : 4.893 \tavg rewards : 6.863,\tavg loss: : 0.079139,\tbuffer size : 40000,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 68.62745098039215\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 6.5\n","[task_2_tmax40] 100 run(s) avg rewards : 6.8\n","Point: 6.65\n","[Episode 3000]\t rewards globals : 4.915 \tavg rewards : 6.078,\tavg loss: : 0.079255,\tbuffer size : 40000,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 60.78431372549019\n","[Episode 3050]\t rewards globals : 4.930 \tavg rewards : 5.686,\tavg loss: : 0.079904,\tbuffer size : 40000,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 56.86274509803921\n","error while saving final\n","device is: cuda\n","config : \n"," x debut 35, y_debut 5, max ep 3500, max epsilon 0.3 \n"," Epsilon_decay 875, test_interval 500\n","Save_interval 1000, batch_size 64, buffer_limit 40000 \n"," Methode Mixed Monte Carlo + Atari DQN, gamma_nstep 0.5, nstep 3\n","Start learning from : x_debut 35 y_debut 5\n","[Episode 50]\t rewards globals : 1.176 \tavg rewards : 1.176,\tavg loss: : 0.140205,\tbuffer size : 580,\tepsilon : 28.4%, \t r <=40 0.0, \t r > 40 11.76470588235294\n","[Episode 100]\t rewards globals : 1.386 \tavg rewards : 1.569,\tavg loss: : 0.085498,\tbuffer size : 1188,\tepsilon : 26.9%, \t r <=40 0.0, \t r > 40 15.686274509803921\n","[Episode 150]\t rewards globals : 1.192 \tavg rewards : 0.784,\tavg loss: : 0.074269,\tbuffer size : 1694,\tepsilon : 25.4%, \t r <=40 0.0, \t r > 40 7.8431372549019605\n","[Episode 200]\t rewards globals : 1.294 \tavg rewards : 1.569,\tavg loss: : 0.070134,\tbuffer size : 2348,\tepsilon : 24.1%, \t r <=40 0.0, \t r > 40 15.686274509803921\n","[Episode 250]\t rewards globals : 1.235 \tavg rewards : 0.980,\tavg loss: : 0.072803,\tbuffer size : 2961,\tepsilon : 22.8%, \t r <=40 0.0, \t r > 40 9.803921568627452\n","[Episode 300]\t rewards globals : 1.395 \tavg rewards : 2.157,\tavg loss: : 0.069565,\tbuffer size : 3634,\tepsilon : 21.6%, \t r <=40 0.0, \t r > 40 21.568627450980394\n","[Episode 350]\t rewards globals : 1.453 \tavg rewards : 1.765,\tavg loss: : 0.067467,\tbuffer size : 4288,\tepsilon : 20.4%, \t r <=40 0.0, \t r > 40 17.647058823529413\n","[Episode 400]\t rewards globals : 1.596 \tavg rewards : 2.549,\tavg loss: : 0.065916,\tbuffer size : 5006,\tepsilon : 19.4%, \t r <=40 0.0, \t r > 40 25.49019607843137\n","[Episode 450]\t rewards globals : 1.752 \tavg rewards : 2.941,\tavg loss: : 0.067983,\tbuffer size : 5788,\tepsilon : 18.3%, \t r <=40 0.0, \t r > 40 29.411764705882355\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 6.1\n","[task_2_tmax40] 100 run(s) avg rewards : 4.4\n","Point: 5.25\n","[Episode 500]\t rewards globals : 1.816 \tavg rewards : 2.549,\tavg loss: : 0.067216,\tbuffer size : 6399,\tepsilon : 17.4%, \t r <=40 0.0, \t r > 40 25.49019607843137\n","[Episode 550]\t rewards globals : 1.869 \tavg rewards : 2.353,\tavg loss: : 0.066809,\tbuffer size : 7059,\tepsilon : 16.5%, \t r <=40 0.0, \t r > 40 23.52941176470588\n","[Episode 600]\t rewards globals : 1.830 \tavg rewards : 1.569,\tavg loss: : 0.065885,\tbuffer size : 7739,\tepsilon : 15.6%, \t r <=40 0.0, \t r > 40 15.686274509803921\n","[Episode 650]\t rewards globals : 1.920 \tavg rewards : 2.941,\tavg loss: : 0.067645,\tbuffer size : 8500,\tepsilon : 14.8%, \t r <=40 0.0, \t r > 40 29.411764705882355\n","[Episode 700]\t rewards globals : 1.897 \tavg rewards : 1.765,\tavg loss: : 0.067266,\tbuffer size : 9156,\tepsilon : 14.0%, \t r <=40 0.0, \t r > 40 17.647058823529413\n","[Episode 750]\t rewards globals : 1.917 \tavg rewards : 2.157,\tavg loss: : 0.067070,\tbuffer size : 9866,\tepsilon : 13.3%, \t r <=40 0.0, \t r > 40 21.568627450980394\n","[Episode 800]\t rewards globals : 1.998 \tavg rewards : 3.137,\tavg loss: : 0.066266,\tbuffer size : 10591,\tepsilon : 12.6%, \t r <=40 0.0, \t r > 40 31.372549019607842\n","[Episode 850]\t rewards globals : 2.009 \tavg rewards : 2.157,\tavg loss: : 0.067386,\tbuffer size : 11299,\tepsilon : 12.0%, \t r <=40 0.0, \t r > 40 21.568627450980394\n","[Episode 900]\t rewards globals : 2.031 \tavg rewards : 2.353,\tavg loss: : 0.067128,\tbuffer size : 12025,\tepsilon : 11.4%, \t r <=40 0.0, \t r > 40 23.52941176470588\n","[Episode 950]\t rewards globals : 2.103 \tavg rewards : 3.529,\tavg loss: : 0.066836,\tbuffer size : 12752,\tepsilon : 10.8%, \t r <=40 0.0, \t r > 40 35.294117647058826\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 4.6\n","[task_2_tmax40] 100 run(s) avg rewards : 5.1\n","Point: 4.85\n","[Episode 1000]\t rewards globals : 2.158 \tavg rewards : 3.333,\tavg loss: : 0.066489,\tbuffer size : 13491,\tepsilon : 10.2%, \t r <=40 0.0, \t r > 40 33.33333333333333\n","[Episode 1050]\t rewards globals : 2.217 \tavg rewards : 3.529,\tavg loss: : 0.067371,\tbuffer size : 14217,\tepsilon : 9.7%, \t r <=40 0.0, \t r > 40 35.294117647058826\n","[Episode 1100]\t rewards globals : 2.316 \tavg rewards : 4.510,\tavg loss: : 0.068018,\tbuffer size : 14949,\tepsilon : 9.2%, \t r <=40 0.0, \t r > 40 45.09803921568628\n","[Episode 1150]\t rewards globals : 2.346 \tavg rewards : 2.941,\tavg loss: : 0.068132,\tbuffer size : 15651,\tepsilon : 8.8%, \t r <=40 0.0, \t r > 40 29.411764705882355\n","[Episode 1200]\t rewards globals : 2.365 \tavg rewards : 2.941,\tavg loss: : 0.067962,\tbuffer size : 16401,\tepsilon : 8.4%, \t r <=40 0.0, \t r > 40 29.411764705882355\n","[Episode 1250]\t rewards globals : 2.430 \tavg rewards : 3.922,\tavg loss: : 0.068711,\tbuffer size : 17152,\tepsilon : 7.9%, \t r <=40 0.0, \t r > 40 39.21568627450981\n","[Episode 1300]\t rewards globals : 2.483 \tavg rewards : 3.922,\tavg loss: : 0.068709,\tbuffer size : 17907,\tepsilon : 7.6%, \t r <=40 0.0, \t r > 40 39.21568627450981\n","[Episode 1350]\t rewards globals : 2.539 \tavg rewards : 3.922,\tavg loss: : 0.068761,\tbuffer size : 18706,\tepsilon : 7.2%, \t r <=40 0.0, \t r > 40 39.21568627450981\n","[Episode 1400]\t rewards globals : 2.598 \tavg rewards : 4.118,\tavg loss: : 0.068338,\tbuffer size : 19531,\tepsilon : 6.9%, \t r <=40 0.0, \t r > 40 41.17647058823529\n","[Episode 1450]\t rewards globals : 2.619 \tavg rewards : 3.333,\tavg loss: : 0.069227,\tbuffer size : 20252,\tepsilon : 6.5%, \t r <=40 0.0, \t r > 40 33.33333333333333\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 4.9\n","[task_2_tmax40] 100 run(s) avg rewards : 4.7\n","Point: 4.800000000000001\n","[Episode 1500]\t rewards globals : 2.685 \tavg rewards : 4.510,\tavg loss: : 0.069441,\tbuffer size : 21044,\tepsilon : 6.2%, \t r <=40 0.0, \t r > 40 45.09803921568628\n","[Episode 1550]\t rewards globals : 2.727 \tavg rewards : 4.118,\tavg loss: : 0.069713,\tbuffer size : 21789,\tepsilon : 5.9%, \t r <=40 0.0, \t r > 40 41.17647058823529\n","[Episode 1600]\t rewards globals : 2.755 \tavg rewards : 3.529,\tavg loss: : 0.069743,\tbuffer size : 22634,\tepsilon : 5.7%, \t r <=40 0.0, \t r > 40 35.294117647058826\n","[Episode 1650]\t rewards globals : 2.750 \tavg rewards : 2.745,\tavg loss: : 0.070325,\tbuffer size : 23383,\tepsilon : 5.4%, \t r <=40 0.0, \t r > 40 27.450980392156865\n","[Episode 1700]\t rewards globals : 2.840 \tavg rewards : 5.882,\tavg loss: : 0.070469,\tbuffer size : 24191,\tepsilon : 5.2%, \t r <=40 0.0, \t r > 40 58.82352941176471\n","[Episode 1750]\t rewards globals : 2.907 \tavg rewards : 5.098,\tavg loss: : 0.070557,\tbuffer size : 25005,\tepsilon : 4.9%, \t r <=40 0.0, \t r > 40 50.98039215686274\n","[Episode 1800]\t rewards globals : 2.959 \tavg rewards : 4.902,\tavg loss: : 0.070688,\tbuffer size : 25838,\tepsilon : 4.7%, \t r <=40 0.0, \t r > 40 49.01960784313725\n","[Episode 1850]\t rewards globals : 2.988 \tavg rewards : 4.118,\tavg loss: : 0.071133,\tbuffer size : 26679,\tepsilon : 4.5%, \t r <=40 0.0, \t r > 40 41.17647058823529\n","[Episode 1900]\t rewards globals : 2.972 \tavg rewards : 2.353,\tavg loss: : 0.071123,\tbuffer size : 27426,\tepsilon : 4.3%, \t r <=40 0.0, \t r > 40 23.52941176470588\n","[Episode 1950]\t rewards globals : 2.983 \tavg rewards : 3.529,\tavg loss: : 0.070904,\tbuffer size : 28218,\tepsilon : 4.1%, \t r <=40 0.0, \t r > 40 35.294117647058826\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 3.8\n","[task_2_tmax40] 100 run(s) avg rewards : 4.4\n","Point: 4.1\n","[Episode 2000]\t rewards globals : 2.974 \tavg rewards : 2.745,\tavg loss: : 0.070857,\tbuffer size : 28972,\tepsilon : 3.9%, \t r <=40 0.0, \t r > 40 27.450980392156865\n","[Episode 2050]\t rewards globals : 2.999 \tavg rewards : 3.922,\tavg loss: : 0.071532,\tbuffer size : 29795,\tepsilon : 3.8%, \t r <=40 0.0, \t r > 40 39.21568627450981\n","[Episode 2100]\t rewards globals : 3.032 \tavg rewards : 4.510,\tavg loss: : 0.071927,\tbuffer size : 30611,\tepsilon : 3.6%, \t r <=40 0.0, \t r > 40 45.09803921568628\n","[Episode 2150]\t rewards globals : 3.054 \tavg rewards : 3.922,\tavg loss: : 0.072484,\tbuffer size : 31390,\tepsilon : 3.5%, \t r <=40 0.0, \t r > 40 39.21568627450981\n","[Episode 2200]\t rewards globals : 3.062 \tavg rewards : 3.333,\tavg loss: : 0.072520,\tbuffer size : 32221,\tepsilon : 3.3%, \t r <=40 0.0, \t r > 40 33.33333333333333\n","[Episode 2250]\t rewards globals : 3.088 \tavg rewards : 4.314,\tavg loss: : 0.072751,\tbuffer size : 32967,\tepsilon : 3.2%, \t r <=40 0.0, \t r > 40 43.13725490196079\n","[Episode 2300]\t rewards globals : 3.103 \tavg rewards : 3.725,\tavg loss: : 0.072795,\tbuffer size : 33765,\tepsilon : 3.1%, \t r <=40 0.0, \t r > 40 37.254901960784316\n","[Episode 2350]\t rewards globals : 3.122 \tavg rewards : 4.118,\tavg loss: : 0.072771,\tbuffer size : 34595,\tepsilon : 3.0%, \t r <=40 0.0, \t r > 40 41.17647058823529\n","[Episode 2400]\t rewards globals : 3.128 \tavg rewards : 3.529,\tavg loss: : 0.072999,\tbuffer size : 35348,\tepsilon : 2.9%, \t r <=40 0.0, \t r > 40 35.294117647058826\n","[Episode 2450]\t rewards globals : 3.142 \tavg rewards : 3.922,\tavg loss: : 0.073877,\tbuffer size : 36149,\tepsilon : 2.8%, \t r <=40 0.0, \t r > 40 39.21568627450981\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 4.4\n","[task_2_tmax40] 100 run(s) avg rewards : 4.2\n","Point: 4.300000000000001\n","[Episode 2500]\t rewards globals : 3.175 \tavg rewards : 4.902,\tavg loss: : 0.074482,\tbuffer size : 36936,\tepsilon : 2.7%, \t r <=40 0.0, \t r > 40 49.01960784313725\n","[Episode 2550]\t rewards globals : 3.207 \tavg rewards : 4.706,\tavg loss: : 0.074748,\tbuffer size : 37690,\tepsilon : 2.6%, \t r <=40 0.0, \t r > 40 47.05882352941176\n","[Episode 2600]\t rewards globals : 3.237 \tavg rewards : 4.902,\tavg loss: : 0.074889,\tbuffer size : 38498,\tepsilon : 2.5%, \t r <=40 0.0, \t r > 40 49.01960784313725\n","[Episode 2650]\t rewards globals : 3.240 \tavg rewards : 3.529,\tavg loss: : 0.075194,\tbuffer size : 39292,\tepsilon : 2.4%, \t r <=40 0.0, \t r > 40 35.294117647058826\n","[Episode 2700]\t rewards globals : 3.247 \tavg rewards : 3.529,\tavg loss: : 0.075362,\tbuffer size : 40000,\tepsilon : 2.3%, \t r <=40 0.0, \t r > 40 35.294117647058826\n","[Episode 2750]\t rewards globals : 3.261 \tavg rewards : 3.922,\tavg loss: : 0.075473,\tbuffer size : 40000,\tepsilon : 2.3%, \t r <=40 0.0, \t r > 40 39.21568627450981\n","[Episode 2800]\t rewards globals : 3.302 \tavg rewards : 5.490,\tavg loss: : 0.075537,\tbuffer size : 40000,\tepsilon : 2.2%, \t r <=40 0.0, \t r > 40 54.90196078431373\n","[Episode 2850]\t rewards globals : 3.322 \tavg rewards : 4.314,\tavg loss: : 0.075642,\tbuffer size : 40000,\tepsilon : 2.1%, \t r <=40 0.0, \t r > 40 43.13725490196079\n","[Episode 2900]\t rewards globals : 3.316 \tavg rewards : 3.137,\tavg loss: : 0.075821,\tbuffer size : 40000,\tepsilon : 2.1%, \t r <=40 0.0, \t r > 40 31.372549019607842\n","[Episode 2950]\t rewards globals : 3.311 \tavg rewards : 2.941,\tavg loss: : 0.075819,\tbuffer size : 40000,\tepsilon : 2.0%, \t r <=40 0.0, \t r > 40 29.411764705882355\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 5.0\n","[task_2_tmax40] 100 run(s) avg rewards : 4.4\n","Point: 4.7\n","[Episode 3000]\t rewards globals : 3.329 \tavg rewards : 4.510,\tavg loss: : 0.075845,\tbuffer size : 40000,\tepsilon : 1.9%, \t r <=40 0.0, \t r > 40 45.09803921568628\n","[Episode 3050]\t rewards globals : 3.330 \tavg rewards : 3.529,\tavg loss: : 0.076145,\tbuffer size : 40000,\tepsilon : 1.9%, \t r <=40 0.0, \t r > 40 35.294117647058826\n","[Episode 3100]\t rewards globals : 3.357 \tavg rewards : 4.902,\tavg loss: : 0.076466,\tbuffer size : 40000,\tepsilon : 1.8%, \t r <=40 0.0, \t r > 40 49.01960784313725\n","[Episode 3150]\t rewards globals : 3.386 \tavg rewards : 5.294,\tavg loss: : 0.076560,\tbuffer size : 40000,\tepsilon : 1.8%, \t r <=40 0.0, \t r > 40 52.94117647058824\n","[Episode 3200]\t rewards globals : 3.399 \tavg rewards : 4.314,\tavg loss: : 0.076671,\tbuffer size : 40000,\tepsilon : 1.7%, \t r <=40 0.0, \t r > 40 43.13725490196079\n","[Episode 3250]\t rewards globals : 3.408 \tavg rewards : 4.118,\tavg loss: : 0.076917,\tbuffer size : 40000,\tepsilon : 1.7%, \t r <=40 0.0, \t r > 40 41.17647058823529\n","[Episode 3300]\t rewards globals : 3.417 \tavg rewards : 3.922,\tavg loss: : 0.077118,\tbuffer size : 40000,\tepsilon : 1.7%, \t r <=40 0.0, \t r > 40 39.21568627450981\n","[Episode 3350]\t rewards globals : 3.441 \tavg rewards : 4.902,\tavg loss: : 0.077255,\tbuffer size : 40000,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 49.01960784313725\n","[Episode 3400]\t rewards globals : 3.464 \tavg rewards : 5.098,\tavg loss: : 0.077392,\tbuffer size : 40000,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 50.98039215686274\n","[Episode 3450]\t rewards globals : 3.474 \tavg rewards : 4.314,\tavg loss: : 0.077571,\tbuffer size : 40000,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 43.13725490196079\n","error while saving final\n","device is: cuda\n","config : \n"," x debut 35, y_debut 6, max ep 3900, max epsilon 0.3 \n"," Epsilon_decay 975, test_interval 500\n","Save_interval 1000, batch_size 64, buffer_limit 40000 \n"," Methode Mixed Monte Carlo + Atari DQN, gamma_nstep 0.5, nstep 3\n","Start learning from : x_debut 35 y_debut 6\n","[Episode 50]\t rewards globals : 0.000 \tavg rewards : 0.000,\tavg loss: : 0.117698,\tbuffer size : 613,\tepsilon : 28.6%, \t r <=40 0.0, \t r > 40 0.0\n","[Episode 100]\t rewards globals : 0.099 \tavg rewards : 0.196,\tavg loss: : 0.077987,\tbuffer size : 1252,\tepsilon : 27.2%, \t r <=40 0.0, \t r > 40 1.9607843137254901\n","[Episode 150]\t rewards globals : 0.265 \tavg rewards : 0.588,\tavg loss: : 0.064502,\tbuffer size : 1901,\tepsilon : 25.9%, \t r <=40 0.0, \t r > 40 5.88235294117647\n","[Episode 200]\t rewards globals : 0.249 \tavg rewards : 0.196,\tavg loss: : 0.064048,\tbuffer size : 2526,\tepsilon : 24.6%, \t r <=40 0.0, \t r > 40 1.9607843137254901\n","[Episode 250]\t rewards globals : 0.199 \tavg rewards : 0.000,\tavg loss: : 0.064881,\tbuffer size : 3123,\tepsilon : 23.4%, \t r <=40 0.0, \t r > 40 0.0\n","[Episode 300]\t rewards globals : 0.299 \tavg rewards : 0.784,\tavg loss: : 0.064119,\tbuffer size : 3773,\tepsilon : 22.3%, \t r <=40 0.0, \t r > 40 7.8431372549019605\n","[Episode 350]\t rewards globals : 0.370 \tavg rewards : 0.784,\tavg loss: : 0.063470,\tbuffer size : 4466,\tepsilon : 21.3%, \t r <=40 0.0, \t r > 40 7.8431372549019605\n","[Episode 400]\t rewards globals : 0.399 \tavg rewards : 0.588,\tavg loss: : 0.062742,\tbuffer size : 5147,\tepsilon : 20.2%, \t r <=40 0.0, \t r > 40 5.88235294117647\n","[Episode 450]\t rewards globals : 0.443 \tavg rewards : 0.980,\tavg loss: : 0.063556,\tbuffer size : 5844,\tepsilon : 19.3%, \t r <=40 0.0, \t r > 40 9.803921568627452\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 0.6\n","[task_2_tmax40] 100 run(s) avg rewards : 0.8\n","Point: 0.7\n","[Episode 500]\t rewards globals : 0.419 \tavg rewards : 0.196,\tavg loss: : 0.061774,\tbuffer size : 6579,\tepsilon : 18.4%, \t r <=40 0.0, \t r > 40 1.9607843137254901\n","[Episode 550]\t rewards globals : 0.454 \tavg rewards : 0.784,\tavg loss: : 0.060392,\tbuffer size : 7277,\tepsilon : 17.5%, \t r <=40 0.0, \t r > 40 7.8431372549019605\n","[Episode 600]\t rewards globals : 0.449 \tavg rewards : 0.392,\tavg loss: : 0.059357,\tbuffer size : 7985,\tepsilon : 16.7%, \t r <=40 0.0, \t r > 40 3.9215686274509802\n","[Episode 650]\t rewards globals : 0.445 \tavg rewards : 0.392,\tavg loss: : 0.059291,\tbuffer size : 8728,\tepsilon : 15.9%, \t r <=40 0.0, \t r > 40 3.9215686274509802\n","[Episode 700]\t rewards globals : 0.485 \tavg rewards : 0.980,\tavg loss: : 0.058676,\tbuffer size : 9493,\tepsilon : 15.1%, \t r <=40 0.0, \t r > 40 9.803921568627452\n","[Episode 750]\t rewards globals : 0.453 \tavg rewards : 0.000,\tavg loss: : 0.057863,\tbuffer size : 10235,\tepsilon : 14.4%, \t r <=40 0.0, \t r > 40 0.0\n","[Episode 800]\t rewards globals : 0.449 \tavg rewards : 0.392,\tavg loss: : 0.057391,\tbuffer size : 10977,\tepsilon : 13.8%, \t r <=40 0.0, \t r > 40 3.9215686274509802\n","[Episode 850]\t rewards globals : 0.447 \tavg rewards : 0.392,\tavg loss: : 0.057680,\tbuffer size : 11670,\tepsilon : 13.1%, \t r <=40 0.0, \t r > 40 3.9215686274509802\n","[Episode 900]\t rewards globals : 0.433 \tavg rewards : 0.196,\tavg loss: : 0.057644,\tbuffer size : 12478,\tepsilon : 12.5%, \t r <=40 0.0, \t r > 40 1.9607843137254901\n","[Episode 950]\t rewards globals : 0.431 \tavg rewards : 0.392,\tavg loss: : 0.056986,\tbuffer size : 13204,\tepsilon : 11.9%, \t r <=40 0.0, \t r > 40 3.9215686274509802\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 0.3\n","[task_2_tmax40] 100 run(s) avg rewards : 0.8\n","Point: 0.55\n","[Episode 1000]\t rewards globals : 0.430 \tavg rewards : 0.392,\tavg loss: : 0.056524,\tbuffer size : 13886,\tepsilon : 11.4%, \t r <=40 0.0, \t r > 40 3.9215686274509802\n","[Episode 1050]\t rewards globals : 0.419 \tavg rewards : 0.196,\tavg loss: : 0.056352,\tbuffer size : 14645,\tepsilon : 10.9%, \t r <=40 0.0, \t r > 40 1.9607843137254901\n","[Episode 1100]\t rewards globals : 0.427 \tavg rewards : 0.588,\tavg loss: : 0.056065,\tbuffer size : 15401,\tepsilon : 10.4%, \t r <=40 0.0, \t r > 40 5.88235294117647\n","[Episode 1150]\t rewards globals : 0.469 \tavg rewards : 1.373,\tavg loss: : 0.055526,\tbuffer size : 16160,\tepsilon : 9.9%, \t r <=40 0.0, \t r > 40 13.725490196078432\n","[Episode 1200]\t rewards globals : 0.475 \tavg rewards : 0.588,\tavg loss: : 0.055067,\tbuffer size : 16923,\tepsilon : 9.5%, \t r <=40 0.0, \t r > 40 5.88235294117647\n","[Episode 1250]\t rewards globals : 0.472 \tavg rewards : 0.588,\tavg loss: : 0.054837,\tbuffer size : 17681,\tepsilon : 9.0%, \t r <=40 0.0, \t r > 40 5.88235294117647\n","[Episode 1300]\t rewards globals : 0.477 \tavg rewards : 0.588,\tavg loss: : 0.054655,\tbuffer size : 18473,\tepsilon : 8.6%, \t r <=40 0.0, \t r > 40 5.88235294117647\n","[Episode 1350]\t rewards globals : 0.474 \tavg rewards : 0.392,\tavg loss: : 0.054193,\tbuffer size : 19227,\tepsilon : 8.3%, \t r <=40 0.0, \t r > 40 3.9215686274509802\n","[Episode 1400]\t rewards globals : 0.485 \tavg rewards : 0.784,\tavg loss: : 0.054052,\tbuffer size : 19991,\tepsilon : 7.9%, \t r <=40 0.0, \t r > 40 7.8431372549019605\n","[Episode 1450]\t rewards globals : 0.489 \tavg rewards : 0.588,\tavg loss: : 0.054423,\tbuffer size : 20771,\tepsilon : 7.6%, \t r <=40 0.0, \t r > 40 5.88235294117647\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 0.8\n","[task_2_tmax40] 100 run(s) avg rewards : 0.5\n","Point: 0.65\n","[Episode 1500]\t rewards globals : 0.493 \tavg rewards : 0.588,\tavg loss: : 0.054181,\tbuffer size : 21536,\tepsilon : 7.2%, \t r <=40 0.0, \t r > 40 5.88235294117647\n","[Episode 1550]\t rewards globals : 0.496 \tavg rewards : 0.588,\tavg loss: : 0.054163,\tbuffer size : 22316,\tepsilon : 6.9%, \t r <=40 0.0, \t r > 40 5.88235294117647\n","[Episode 1600]\t rewards globals : 0.518 \tavg rewards : 1.176,\tavg loss: : 0.054126,\tbuffer size : 23123,\tepsilon : 6.6%, \t r <=40 0.0, \t r > 40 11.76470588235294\n","[Episode 1650]\t rewards globals : 0.509 \tavg rewards : 0.196,\tavg loss: : 0.054061,\tbuffer size : 23935,\tepsilon : 6.3%, \t r <=40 0.0, \t r > 40 1.9607843137254901\n","[Episode 1700]\t rewards globals : 0.529 \tavg rewards : 1.176,\tavg loss: : 0.053849,\tbuffer size : 24707,\tepsilon : 6.1%, \t r <=40 0.0, \t r > 40 11.76470588235294\n","[Episode 1750]\t rewards globals : 0.531 \tavg rewards : 0.588,\tavg loss: : 0.053686,\tbuffer size : 25424,\tepsilon : 5.8%, \t r <=40 0.0, \t r > 40 5.88235294117647\n","[Episode 1800]\t rewards globals : 0.533 \tavg rewards : 0.588,\tavg loss: : 0.053266,\tbuffer size : 26242,\tepsilon : 5.6%, \t r <=40 0.0, \t r > 40 5.88235294117647\n","[Episode 1850]\t rewards globals : 0.546 \tavg rewards : 0.980,\tavg loss: : 0.053269,\tbuffer size : 27013,\tepsilon : 5.3%, \t r <=40 0.0, \t r > 40 9.803921568627452\n","[Episode 1900]\t rewards globals : 0.552 \tavg rewards : 0.784,\tavg loss: : 0.053125,\tbuffer size : 27808,\tepsilon : 5.1%, \t r <=40 0.0, \t r > 40 7.8431372549019605\n","[Episode 1950]\t rewards globals : 0.554 \tavg rewards : 0.588,\tavg loss: : 0.052781,\tbuffer size : 28565,\tepsilon : 4.9%, \t r <=40 0.0, \t r > 40 5.88235294117647\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 1.4\n","[task_2_tmax40] 100 run(s) avg rewards : 0.9\n","Point: 1.15\n","[Episode 2000]\t rewards globals : 0.550 \tavg rewards : 0.392,\tavg loss: : 0.052584,\tbuffer size : 29375,\tepsilon : 4.7%, \t r <=40 0.0, \t r > 40 3.9215686274509802\n","[Episode 2050]\t rewards globals : 0.551 \tavg rewards : 0.588,\tavg loss: : 0.052455,\tbuffer size : 30160,\tepsilon : 4.5%, \t r <=40 0.0, \t r > 40 5.88235294117647\n","[Episode 2100]\t rewards globals : 0.562 \tavg rewards : 0.980,\tavg loss: : 0.052222,\tbuffer size : 30951,\tepsilon : 4.4%, \t r <=40 0.0, \t r > 40 9.803921568627452\n","[Episode 2150]\t rewards globals : 0.567 \tavg rewards : 0.784,\tavg loss: : 0.052191,\tbuffer size : 31765,\tepsilon : 4.2%, \t r <=40 0.0, \t r > 40 7.8431372549019605\n","[Episode 2200]\t rewards globals : 0.563 \tavg rewards : 0.392,\tavg loss: : 0.051938,\tbuffer size : 32499,\tepsilon : 4.0%, \t r <=40 0.0, \t r > 40 3.9215686274509802\n","[Episode 2250]\t rewards globals : 0.586 \tavg rewards : 1.569,\tavg loss: : 0.051908,\tbuffer size : 33279,\tepsilon : 3.9%, \t r <=40 0.0, \t r > 40 15.686274509803921\n","[Episode 2300]\t rewards globals : 0.578 \tavg rewards : 0.196,\tavg loss: : 0.051794,\tbuffer size : 34083,\tepsilon : 3.7%, \t r <=40 0.0, \t r > 40 1.9607843137254901\n","[Episode 2350]\t rewards globals : 0.587 \tavg rewards : 0.980,\tavg loss: : 0.051778,\tbuffer size : 34906,\tepsilon : 3.6%, \t r <=40 0.0, \t r > 40 9.803921568627452\n","[Episode 2400]\t rewards globals : 0.583 \tavg rewards : 0.392,\tavg loss: : 0.051614,\tbuffer size : 35700,\tepsilon : 3.5%, \t r <=40 0.0, \t r > 40 3.9215686274509802\n","[Episode 2450]\t rewards globals : 0.592 \tavg rewards : 0.980,\tavg loss: : 0.051626,\tbuffer size : 36506,\tepsilon : 3.4%, \t r <=40 0.0, \t r > 40 9.803921568627452\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 0.6\n","[task_2_tmax40] 100 run(s) avg rewards : 0.8\n","Point: 0.7\n","[Episode 2500]\t rewards globals : 0.612 \tavg rewards : 1.569,\tavg loss: : 0.051497,\tbuffer size : 37349,\tepsilon : 3.2%, \t r <=40 0.0, \t r > 40 15.686274509803921\n","[Episode 2550]\t rewards globals : 0.608 \tavg rewards : 0.392,\tavg loss: : 0.051359,\tbuffer size : 38162,\tepsilon : 3.1%, \t r <=40 0.0, \t r > 40 3.9215686274509802\n","[Episode 2600]\t rewards globals : 0.604 \tavg rewards : 0.392,\tavg loss: : 0.051220,\tbuffer size : 38912,\tepsilon : 3.0%, \t r <=40 0.0, \t r > 40 3.9215686274509802\n","[Episode 2650]\t rewards globals : 0.607 \tavg rewards : 0.784,\tavg loss: : 0.051139,\tbuffer size : 39714,\tepsilon : 2.9%, \t r <=40 0.0, \t r > 40 7.8431372549019605\n","[Episode 2700]\t rewards globals : 0.607 \tavg rewards : 0.588,\tavg loss: : 0.050960,\tbuffer size : 40000,\tepsilon : 2.8%, \t r <=40 0.0, \t r > 40 5.88235294117647\n","[Episode 2750]\t rewards globals : 0.629 \tavg rewards : 1.765,\tavg loss: : 0.050742,\tbuffer size : 40000,\tepsilon : 2.7%, \t r <=40 0.0, \t r > 40 17.647058823529413\n","[Episode 2800]\t rewards globals : 0.628 \tavg rewards : 0.588,\tavg loss: : 0.050555,\tbuffer size : 40000,\tepsilon : 2.6%, \t r <=40 0.0, \t r > 40 5.88235294117647\n","[Episode 2850]\t rewards globals : 0.621 \tavg rewards : 0.196,\tavg loss: : 0.050483,\tbuffer size : 40000,\tepsilon : 2.6%, \t r <=40 0.0, \t r > 40 1.9607843137254901\n","[Episode 2900]\t rewards globals : 0.614 \tavg rewards : 0.196,\tavg loss: : 0.050238,\tbuffer size : 40000,\tepsilon : 2.5%, \t r <=40 0.0, \t r > 40 1.9607843137254901\n","[Episode 2950]\t rewards globals : 0.610 \tavg rewards : 0.392,\tavg loss: : 0.049994,\tbuffer size : 40000,\tepsilon : 2.4%, \t r <=40 0.0, \t r > 40 3.9215686274509802\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 0.2\n","[task_2_tmax40] 100 run(s) avg rewards : 0.6\n","Point: 0.4\n","[Episode 3000]\t rewards globals : 0.603 \tavg rewards : 0.196,\tavg loss: : 0.049798,\tbuffer size : 40000,\tepsilon : 2.3%, \t r <=40 0.0, \t r > 40 1.9607843137254901\n","[Episode 3050]\t rewards globals : 0.597 \tavg rewards : 0.196,\tavg loss: : 0.049746,\tbuffer size : 40000,\tepsilon : 2.3%, \t r <=40 0.0, \t r > 40 1.9607843137254901\n","[Episode 3100]\t rewards globals : 0.597 \tavg rewards : 0.588,\tavg loss: : 0.049659,\tbuffer size : 40000,\tepsilon : 2.2%, \t r <=40 0.0, \t r > 40 5.88235294117647\n","[Episode 3150]\t rewards globals : 0.593 \tavg rewards : 0.392,\tavg loss: : 0.049645,\tbuffer size : 40000,\tepsilon : 2.1%, \t r <=40 0.0, \t r > 40 3.9215686274509802\n","[Episode 3200]\t rewards globals : 0.597 \tavg rewards : 0.784,\tavg loss: : 0.049492,\tbuffer size : 40000,\tepsilon : 2.1%, \t r <=40 0.0, \t r > 40 7.8431372549019605\n","[Episode 3250]\t rewards globals : 0.606 \tavg rewards : 1.176,\tavg loss: : 0.049484,\tbuffer size : 40000,\tepsilon : 2.0%, \t r <=40 0.0, \t r > 40 11.76470588235294\n","[Episode 3300]\t rewards globals : 0.609 \tavg rewards : 0.784,\tavg loss: : 0.049448,\tbuffer size : 40000,\tepsilon : 2.0%, \t r <=40 0.0, \t r > 40 7.8431372549019605\n","[Episode 3350]\t rewards globals : 0.606 \tavg rewards : 0.392,\tavg loss: : 0.049322,\tbuffer size : 40000,\tepsilon : 1.9%, \t r <=40 0.0, \t r > 40 3.9215686274509802\n","[Episode 3400]\t rewards globals : 0.603 \tavg rewards : 0.392,\tavg loss: : 0.049122,\tbuffer size : 40000,\tepsilon : 1.9%, \t r <=40 0.0, \t r > 40 3.9215686274509802\n","[Episode 3450]\t rewards globals : 0.603 \tavg rewards : 0.588,\tavg loss: : 0.048973,\tbuffer size : 40000,\tepsilon : 1.8%, \t r <=40 0.0, \t r > 40 5.88235294117647\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 0.3\n","[task_2_tmax40] 100 run(s) avg rewards : 0.4\n","Point: 0.35\n","[Episode 3500]\t rewards globals : 0.600 \tavg rewards : 0.392,\tavg loss: : 0.048921,\tbuffer size : 40000,\tepsilon : 1.8%, \t r <=40 0.0, \t r > 40 3.9215686274509802\n","[Episode 3550]\t rewards globals : 0.597 \tavg rewards : 0.392,\tavg loss: : 0.048713,\tbuffer size : 40000,\tepsilon : 1.8%, \t r <=40 0.0, \t r > 40 3.9215686274509802\n","[Episode 3600]\t rewards globals : 0.600 \tavg rewards : 0.784,\tavg loss: : 0.048457,\tbuffer size : 40000,\tepsilon : 1.7%, \t r <=40 0.0, \t r > 40 7.8431372549019605\n","[Episode 3650]\t rewards globals : 0.600 \tavg rewards : 0.588,\tavg loss: : 0.048397,\tbuffer size : 40000,\tepsilon : 1.7%, \t r <=40 0.0, \t r > 40 5.88235294117647\n","[Episode 3700]\t rewards globals : 0.597 \tavg rewards : 0.392,\tavg loss: : 0.048378,\tbuffer size : 40000,\tepsilon : 1.7%, \t r <=40 0.0, \t r > 40 3.9215686274509802\n","[Episode 3750]\t rewards globals : 0.603 \tavg rewards : 0.980,\tavg loss: : 0.048232,\tbuffer size : 40000,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 9.803921568627452\n","[Episode 3800]\t rewards globals : 0.605 \tavg rewards : 0.784,\tavg loss: : 0.048029,\tbuffer size : 40000,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 7.8431372549019605\n","[Episode 3850]\t rewards globals : 0.602 \tavg rewards : 0.392,\tavg loss: : 0.047815,\tbuffer size : 40000,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 3.9215686274509802\n","error while saving final\n","device is: cuda\n","config : \n"," x debut 40, y_debut 0, max ep 1750, max epsilon 0.3 \n"," Epsilon_decay 437, test_interval 500\n","Save_interval 1000, batch_size 64, buffer_limit 28000 \n"," Methode Mixed Monte Carlo + Atari DQN, gamma_nstep 0.5, nstep 3\n","Start learning from : x_debut 40 y_debut 0\n","[Episode 50]\t rewards globals : 2.941 \tavg rewards : 2.941,\tavg loss: : 0.288697,\tbuffer size : 728,\tepsilon : 26.9%, \t r <=40 0.0, \t r > 40 29.411764705882355\n","[Episode 100]\t rewards globals : 3.069 \tavg rewards : 3.137,\tavg loss: : 0.156679,\tbuffer size : 1500,\tepsilon : 24.1%, \t r <=40 0.0, \t r > 40 31.372549019607842\n","[Episode 150]\t rewards globals : 2.848 \tavg rewards : 2.353,\tavg loss: : 0.124223,\tbuffer size : 2151,\tepsilon : 21.6%, \t r <=40 0.0, \t r > 40 23.52941176470588\n","[Episode 200]\t rewards globals : 3.035 \tavg rewards : 3.529,\tavg loss: : 0.109067,\tbuffer size : 2934,\tepsilon : 19.4%, \t r <=40 0.0, \t r > 40 35.294117647058826\n","[Episode 250]\t rewards globals : 3.068 \tavg rewards : 3.137,\tavg loss: : 0.106712,\tbuffer size : 3626,\tepsilon : 17.4%, \t r <=40 0.0, \t r > 40 31.372549019607842\n","[Episode 300]\t rewards globals : 3.422 \tavg rewards : 5.098,\tavg loss: : 0.098598,\tbuffer size : 4439,\tepsilon : 15.6%, \t r <=40 0.0, \t r > 40 50.98039215686274\n","[Episode 350]\t rewards globals : 3.561 \tavg rewards : 4.510,\tavg loss: : 0.093878,\tbuffer size : 5227,\tepsilon : 14.0%, \t r <=40 0.0, \t r > 40 45.09803921568628\n","[Episode 400]\t rewards globals : 3.840 \tavg rewards : 5.686,\tavg loss: : 0.090309,\tbuffer size : 6096,\tepsilon : 12.6%, \t r <=40 0.0, \t r > 40 56.86274509803921\n","[Episode 450]\t rewards globals : 3.969 \tavg rewards : 5.098,\tavg loss: : 0.089390,\tbuffer size : 6921,\tepsilon : 11.4%, \t r <=40 0.0, \t r > 40 50.98039215686274\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 8.9\n","[task_2_tmax40] 100 run(s) avg rewards : 8.0\n","Point: 8.45\n","[Episode 500]\t rewards globals : 4.112 \tavg rewards : 5.490,\tavg loss: : 0.087420,\tbuffer size : 7844,\tepsilon : 10.2%, \t r <=40 0.0, \t r > 40 54.90196078431373\n","[Episode 550]\t rewards globals : 4.319 \tavg rewards : 6.471,\tavg loss: : 0.086607,\tbuffer size : 8796,\tepsilon : 9.2%, \t r <=40 0.0, \t r > 40 64.70588235294117\n","[Episode 600]\t rewards globals : 4.376 \tavg rewards : 5.098,\tavg loss: : 0.084736,\tbuffer size : 9628,\tepsilon : 8.3%, \t r <=40 0.0, \t r > 40 50.98039215686274\n","[Episode 650]\t rewards globals : 4.455 \tavg rewards : 5.294,\tavg loss: : 0.085730,\tbuffer size : 10464,\tepsilon : 7.6%, \t r <=40 0.0, \t r > 40 52.94117647058824\n","[Episode 700]\t rewards globals : 4.536 \tavg rewards : 5.686,\tavg loss: : 0.084547,\tbuffer size : 11358,\tepsilon : 6.8%, \t r <=40 0.0, \t r > 40 56.86274509803921\n","[Episode 750]\t rewards globals : 4.660 \tavg rewards : 6.275,\tavg loss: : 0.084447,\tbuffer size : 12274,\tepsilon : 6.2%, \t r <=40 0.0, \t r > 40 62.745098039215684\n","[Episode 800]\t rewards globals : 4.719 \tavg rewards : 5.686,\tavg loss: : 0.083840,\tbuffer size : 13200,\tepsilon : 5.6%, \t r <=40 0.0, \t r > 40 56.86274509803921\n","[Episode 850]\t rewards globals : 4.841 \tavg rewards : 6.863,\tavg loss: : 0.083885,\tbuffer size : 14119,\tepsilon : 5.1%, \t r <=40 0.0, \t r > 40 68.62745098039215\n","[Episode 900]\t rewards globals : 5.039 \tavg rewards : 8.431,\tavg loss: : 0.083579,\tbuffer size : 15149,\tepsilon : 4.7%, \t r <=40 0.0, \t r > 40 84.31372549019608\n","[Episode 950]\t rewards globals : 5.121 \tavg rewards : 6.471,\tavg loss: : 0.083398,\tbuffer size : 16147,\tepsilon : 4.3%, \t r <=40 0.0, \t r > 40 64.70588235294117\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 8.2\n","[task_2_tmax40] 100 run(s) avg rewards : 8.6\n","Point: 8.399999999999999\n","[Episode 1000]\t rewards globals : 5.195 \tavg rewards : 6.667,\tavg loss: : 0.082499,\tbuffer size : 17140,\tepsilon : 3.9%, \t r <=40 0.0, \t r > 40 66.66666666666666\n","[Episode 1050]\t rewards globals : 5.233 \tavg rewards : 6.078,\tavg loss: : 0.083471,\tbuffer size : 18086,\tepsilon : 3.6%, \t r <=40 0.0, \t r > 40 60.78431372549019\n","[Episode 1100]\t rewards globals : 5.313 \tavg rewards : 7.059,\tavg loss: : 0.083070,\tbuffer size : 19093,\tepsilon : 3.3%, \t r <=40 0.0, \t r > 40 70.58823529411765\n","[Episode 1150]\t rewards globals : 5.404 \tavg rewards : 7.451,\tavg loss: : 0.082396,\tbuffer size : 20117,\tepsilon : 3.1%, \t r <=40 0.0, \t r > 40 74.50980392156863\n","[Episode 1200]\t rewards globals : 5.470 \tavg rewards : 7.059,\tavg loss: : 0.081972,\tbuffer size : 21104,\tepsilon : 2.9%, \t r <=40 0.0, \t r > 40 70.58823529411765\n","[Episode 1250]\t rewards globals : 5.572 \tavg rewards : 7.843,\tavg loss: : 0.082496,\tbuffer size : 22167,\tepsilon : 2.7%, \t r <=40 0.0, \t r > 40 78.43137254901961\n","[Episode 1300]\t rewards globals : 5.688 \tavg rewards : 8.627,\tavg loss: : 0.082140,\tbuffer size : 23191,\tepsilon : 2.5%, \t r <=40 0.0, \t r > 40 86.27450980392157\n","[Episode 1350]\t rewards globals : 5.736 \tavg rewards : 7.059,\tavg loss: : 0.081593,\tbuffer size : 24220,\tepsilon : 2.3%, \t r <=40 0.0, \t r > 40 70.58823529411765\n","[Episode 1400]\t rewards globals : 5.789 \tavg rewards : 7.255,\tavg loss: : 0.081401,\tbuffer size : 25215,\tepsilon : 2.2%, \t r <=40 0.0, \t r > 40 72.54901960784314\n","[Episode 1450]\t rewards globals : 5.837 \tavg rewards : 7.255,\tavg loss: : 0.081796,\tbuffer size : 26269,\tepsilon : 2.1%, \t r <=40 0.0, \t r > 40 72.54901960784314\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 8.6\n","[task_2_tmax40] 100 run(s) avg rewards : 8.3\n","Point: 8.45\n","[Episode 1500]\t rewards globals : 5.929 \tavg rewards : 8.431,\tavg loss: : 0.081708,\tbuffer size : 27340,\tepsilon : 1.9%, \t r <=40 0.0, \t r > 40 84.31372549019608\n","[Episode 1550]\t rewards globals : 5.957 \tavg rewards : 6.863,\tavg loss: : 0.081617,\tbuffer size : 28000,\tepsilon : 1.8%, \t r <=40 0.0, \t r > 40 68.62745098039215\n","[Episode 1600]\t rewards globals : 6.027 \tavg rewards : 8.235,\tavg loss: : 0.081747,\tbuffer size : 28000,\tepsilon : 1.7%, \t r <=40 0.0, \t r > 40 82.35294117647058\n","[Episode 1650]\t rewards globals : 6.081 \tavg rewards : 7.843,\tavg loss: : 0.082065,\tbuffer size : 28000,\tepsilon : 1.7%, \t r <=40 0.0, \t r > 40 78.43137254901961\n","[Episode 1700]\t rewards globals : 6.149 \tavg rewards : 8.235,\tavg loss: : 0.081894,\tbuffer size : 28000,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 82.35294117647058\n","error while saving final\n","device is: cuda\n","config : \n"," x debut 40, y_debut 1, max ep 2150, max epsilon 0.3 \n"," Epsilon_decay 537, test_interval 500\n","Save_interval 1000, batch_size 64, buffer_limit 32000 \n"," Methode Mixed Monte Carlo + Atari DQN, gamma_nstep 0.5, nstep 3\n","Start learning from : x_debut 40 y_debut 1\n","[Episode 50]\t rewards globals : 2.941 \tavg rewards : 2.941,\tavg loss: : 0.266695,\tbuffer size : 597,\tepsilon : 27.4%, \t r <=40 0.0, \t r > 40 29.411764705882355\n","[Episode 100]\t rewards globals : 2.772 \tavg rewards : 2.549,\tavg loss: : 0.123294,\tbuffer size : 1192,\tepsilon : 25.1%, \t r <=40 0.0, \t r > 40 25.49019607843137\n","[Episode 150]\t rewards globals : 2.980 \tavg rewards : 3.529,\tavg loss: : 0.096668,\tbuffer size : 1890,\tepsilon : 22.9%, \t r <=40 0.0, \t r > 40 35.294117647058826\n","[Episode 200]\t rewards globals : 3.184 \tavg rewards : 3.922,\tavg loss: : 0.084232,\tbuffer size : 2610,\tepsilon : 21.0%, \t r <=40 0.0, \t r > 40 39.21568627450981\n","[Episode 250]\t rewards globals : 3.466 \tavg rewards : 4.706,\tavg loss: : 0.082642,\tbuffer size : 3380,\tepsilon : 19.2%, \t r <=40 0.0, \t r > 40 47.05882352941176\n","[Episode 300]\t rewards globals : 3.621 \tavg rewards : 4.510,\tavg loss: : 0.079113,\tbuffer size : 4157,\tepsilon : 17.6%, \t r <=40 0.0, \t r > 40 45.09803921568628\n","[Episode 350]\t rewards globals : 3.647 \tavg rewards : 3.922,\tavg loss: : 0.079212,\tbuffer size : 4991,\tepsilon : 16.1%, \t r <=40 0.0, \t r > 40 39.21568627450981\n","[Episode 400]\t rewards globals : 3.691 \tavg rewards : 3.922,\tavg loss: : 0.077108,\tbuffer size : 5690,\tepsilon : 14.8%, \t r <=40 0.0, \t r > 40 39.21568627450981\n","[Episode 450]\t rewards globals : 3.814 \tavg rewards : 4.706,\tavg loss: : 0.079342,\tbuffer size : 6538,\tepsilon : 13.5%, \t r <=40 0.0, \t r > 40 47.05882352941176\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 8.0\n","[task_2_tmax40] 100 run(s) avg rewards : 7.8\n","Point: 7.9\n","[Episode 500]\t rewards globals : 3.872 \tavg rewards : 4.510,\tavg loss: : 0.078045,\tbuffer size : 7334,\tepsilon : 12.4%, \t r <=40 0.0, \t r > 40 45.09803921568628\n","[Episode 550]\t rewards globals : 3.938 \tavg rewards : 4.706,\tavg loss: : 0.076595,\tbuffer size : 8098,\tepsilon : 11.4%, \t r <=40 0.0, \t r > 40 47.05882352941176\n","[Episode 600]\t rewards globals : 4.110 \tavg rewards : 5.882,\tavg loss: : 0.076627,\tbuffer size : 9001,\tepsilon : 10.5%, \t r <=40 0.0, \t r > 40 58.82352941176471\n","[Episode 650]\t rewards globals : 4.224 \tavg rewards : 5.686,\tavg loss: : 0.077620,\tbuffer size : 9865,\tepsilon : 9.6%, \t r <=40 0.0, \t r > 40 56.86274509803921\n","[Episode 700]\t rewards globals : 4.223 \tavg rewards : 4.118,\tavg loss: : 0.076736,\tbuffer size : 10622,\tepsilon : 8.9%, \t r <=40 0.0, \t r > 40 41.17647058823529\n","[Episode 750]\t rewards globals : 4.234 \tavg rewards : 4.314,\tavg loss: : 0.077280,\tbuffer size : 11472,\tepsilon : 8.2%, \t r <=40 0.0, \t r > 40 43.13725490196079\n","[Episode 800]\t rewards globals : 4.382 \tavg rewards : 6.471,\tavg loss: : 0.077088,\tbuffer size : 12387,\tepsilon : 7.5%, \t r <=40 0.0, \t r > 40 64.70588235294117\n","[Episode 850]\t rewards globals : 4.465 \tavg rewards : 5.686,\tavg loss: : 0.077816,\tbuffer size : 13258,\tepsilon : 7.0%, \t r <=40 0.0, \t r > 40 56.86274509803921\n","[Episode 900]\t rewards globals : 4.528 \tavg rewards : 5.686,\tavg loss: : 0.077621,\tbuffer size : 14162,\tepsilon : 6.4%, \t r <=40 0.0, \t r > 40 56.86274509803921\n","[Episode 950]\t rewards globals : 4.606 \tavg rewards : 6.078,\tavg loss: : 0.077433,\tbuffer size : 15006,\tepsilon : 5.9%, \t r <=40 0.0, \t r > 40 60.78431372549019\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 7.9\n","[task_2_tmax40] 100 run(s) avg rewards : 7.7\n","Point: 7.800000000000001\n","[Episode 1000]\t rewards globals : 4.605 \tavg rewards : 4.706,\tavg loss: : 0.077625,\tbuffer size : 15820,\tepsilon : 5.5%, \t r <=40 0.0, \t r > 40 47.05882352941176\n","[Episode 1050]\t rewards globals : 4.719 \tavg rewards : 7.059,\tavg loss: : 0.078567,\tbuffer size : 16708,\tepsilon : 5.1%, \t r <=40 0.0, \t r > 40 70.58823529411765\n","[Episode 1100]\t rewards globals : 4.741 \tavg rewards : 5.294,\tavg loss: : 0.078634,\tbuffer size : 17563,\tepsilon : 4.7%, \t r <=40 0.0, \t r > 40 52.94117647058824\n","[Episode 1150]\t rewards globals : 4.848 \tavg rewards : 7.059,\tavg loss: : 0.078937,\tbuffer size : 18491,\tepsilon : 4.4%, \t r <=40 0.0, \t r > 40 70.58823529411765\n","[Episode 1200]\t rewards globals : 4.954 \tavg rewards : 7.255,\tavg loss: : 0.078505,\tbuffer size : 19482,\tepsilon : 4.1%, \t r <=40 0.0, \t r > 40 72.54901960784314\n","[Episode 1250]\t rewards globals : 5.068 \tavg rewards : 7.843,\tavg loss: : 0.078805,\tbuffer size : 20454,\tepsilon : 3.8%, \t r <=40 0.0, \t r > 40 78.43137254901961\n","[Episode 1300]\t rewards globals : 5.111 \tavg rewards : 6.275,\tavg loss: : 0.078986,\tbuffer size : 21348,\tepsilon : 3.6%, \t r <=40 0.0, \t r > 40 62.745098039215684\n","[Episode 1350]\t rewards globals : 5.167 \tavg rewards : 6.667,\tavg loss: : 0.078830,\tbuffer size : 22278,\tepsilon : 3.3%, \t r <=40 0.0, \t r > 40 66.66666666666666\n","[Episode 1400]\t rewards globals : 5.218 \tavg rewards : 6.471,\tavg loss: : 0.079074,\tbuffer size : 23198,\tepsilon : 3.1%, \t r <=40 0.0, \t r > 40 64.70588235294117\n","[Episode 1450]\t rewards globals : 5.320 \tavg rewards : 8.235,\tavg loss: : 0.079803,\tbuffer size : 24180,\tepsilon : 2.9%, \t r <=40 0.0, \t r > 40 82.35294117647058\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 8.4\n","[task_2_tmax40] 100 run(s) avg rewards : 7.8\n","Point: 8.1\n","[Episode 1500]\t rewards globals : 5.390 \tavg rewards : 7.451,\tavg loss: : 0.080096,\tbuffer size : 25145,\tepsilon : 2.8%, \t r <=40 0.0, \t r > 40 74.50980392156863\n","[Episode 1550]\t rewards globals : 5.461 \tavg rewards : 7.647,\tavg loss: : 0.079963,\tbuffer size : 26094,\tepsilon : 2.6%, \t r <=40 0.0, \t r > 40 76.47058823529412\n","[Episode 1600]\t rewards globals : 5.490 \tavg rewards : 6.471,\tavg loss: : 0.080271,\tbuffer size : 27035,\tepsilon : 2.5%, \t r <=40 0.0, \t r > 40 64.70588235294117\n","[Episode 1650]\t rewards globals : 5.518 \tavg rewards : 6.471,\tavg loss: : 0.081182,\tbuffer size : 27933,\tepsilon : 2.3%, \t r <=40 0.0, \t r > 40 64.70588235294117\n","[Episode 1700]\t rewards globals : 5.573 \tavg rewards : 7.451,\tavg loss: : 0.081564,\tbuffer size : 28932,\tepsilon : 2.2%, \t r <=40 0.0, \t r > 40 74.50980392156863\n","[Episode 1750]\t rewards globals : 5.625 \tavg rewards : 7.451,\tavg loss: : 0.081635,\tbuffer size : 29964,\tepsilon : 2.1%, \t r <=40 0.0, \t r > 40 74.50980392156863\n","[Episode 1800]\t rewards globals : 5.647 \tavg rewards : 6.275,\tavg loss: : 0.081833,\tbuffer size : 30911,\tepsilon : 2.0%, \t r <=40 0.0, \t r > 40 62.745098039215684\n","[Episode 1850]\t rewards globals : 5.667 \tavg rewards : 6.275,\tavg loss: : 0.082585,\tbuffer size : 31799,\tepsilon : 1.9%, \t r <=40 0.0, \t r > 40 62.745098039215684\n","[Episode 1900]\t rewards globals : 5.708 \tavg rewards : 7.059,\tavg loss: : 0.082644,\tbuffer size : 32000,\tepsilon : 1.8%, \t r <=40 0.0, \t r > 40 70.58823529411765\n","[Episode 1950]\t rewards globals : 5.751 \tavg rewards : 7.255,\tavg loss: : 0.082845,\tbuffer size : 32000,\tepsilon : 1.8%, \t r <=40 0.0, \t r > 40 72.54901960784314\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 8.6\n","[task_2_tmax40] 100 run(s) avg rewards : 8.3\n","Point: 8.45\n","[Episode 2000]\t rewards globals : 5.802 \tavg rewards : 7.843,\tavg loss: : 0.083224,\tbuffer size : 32000,\tepsilon : 1.7%, \t r <=40 0.0, \t r > 40 78.43137254901961\n","[Episode 2050]\t rewards globals : 5.836 \tavg rewards : 7.255,\tavg loss: : 0.083629,\tbuffer size : 32000,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 72.54901960784314\n","[Episode 2100]\t rewards globals : 5.883 \tavg rewards : 7.843,\tavg loss: : 0.083951,\tbuffer size : 32000,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 78.43137254901961\n","error while saving final\n","device is: cuda\n","config : \n"," x debut 40, y_debut 2, max ep 2550, max epsilon 0.3 \n"," Epsilon_decay 637, test_interval 500\n","Save_interval 1000, batch_size 64, buffer_limit 36000 \n"," Methode Mixed Monte Carlo + Atari DQN, gamma_nstep 0.5, nstep 3\n","Start learning from : x_debut 40 y_debut 2\n","[Episode 50]\t rewards globals : 2.549 \tavg rewards : 2.549,\tavg loss: : 0.212898,\tbuffer size : 673,\tepsilon : 27.8%, \t r <=40 0.0, \t r > 40 25.49019607843137\n","[Episode 100]\t rewards globals : 2.574 \tavg rewards : 2.745,\tavg loss: : 0.122219,\tbuffer size : 1320,\tepsilon : 25.8%, \t r <=40 0.0, \t r > 40 27.450980392156865\n","[Episode 150]\t rewards globals : 2.384 \tavg rewards : 1.961,\tavg loss: : 0.099065,\tbuffer size : 1940,\tepsilon : 23.9%, \t r <=40 0.0, \t r > 40 19.607843137254903\n","[Episode 200]\t rewards globals : 2.438 \tavg rewards : 2.745,\tavg loss: : 0.089701,\tbuffer size : 2520,\tepsilon : 22.2%, \t r <=40 0.0, \t r > 40 27.450980392156865\n","[Episode 250]\t rewards globals : 2.550 \tavg rewards : 2.941,\tavg loss: : 0.086736,\tbuffer size : 3225,\tepsilon : 20.6%, \t r <=40 0.0, \t r > 40 29.411764705882355\n","[Episode 300]\t rewards globals : 2.791 \tavg rewards : 3.922,\tavg loss: : 0.084156,\tbuffer size : 3930,\tepsilon : 19.1%, \t r <=40 0.0, \t r > 40 39.21568627450981\n","[Episode 350]\t rewards globals : 2.991 \tavg rewards : 4.118,\tavg loss: : 0.082809,\tbuffer size : 4709,\tepsilon : 17.7%, \t r <=40 0.0, \t r > 40 41.17647058823529\n","[Episode 400]\t rewards globals : 3.167 \tavg rewards : 4.314,\tavg loss: : 0.081231,\tbuffer size : 5457,\tepsilon : 16.5%, \t r <=40 0.0, \t r > 40 43.13725490196079\n","[Episode 450]\t rewards globals : 3.215 \tavg rewards : 3.529,\tavg loss: : 0.083588,\tbuffer size : 6185,\tepsilon : 15.3%, \t r <=40 0.0, \t r > 40 35.294117647058826\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 6.9\n","[task_2_tmax40] 100 run(s) avg rewards : 7.9\n","Point: 7.4\n","[Episode 500]\t rewards globals : 3.373 \tavg rewards : 4.902,\tavg loss: : 0.082380,\tbuffer size : 6955,\tepsilon : 14.2%, \t r <=40 0.0, \t r > 40 49.01960784313725\n","[Episode 550]\t rewards globals : 3.557 \tavg rewards : 5.490,\tavg loss: : 0.080519,\tbuffer size : 7761,\tepsilon : 13.2%, \t r <=40 0.0, \t r > 40 54.90196078431373\n","[Episode 600]\t rewards globals : 3.644 \tavg rewards : 4.510,\tavg loss: : 0.079741,\tbuffer size : 8555,\tepsilon : 12.3%, \t r <=40 0.0, \t r > 40 45.09803921568628\n","[Episode 650]\t rewards globals : 3.733 \tavg rewards : 4.706,\tavg loss: : 0.080700,\tbuffer size : 9311,\tepsilon : 11.5%, \t r <=40 0.0, \t r > 40 47.05882352941176\n","[Episode 700]\t rewards globals : 3.752 \tavg rewards : 3.922,\tavg loss: : 0.081028,\tbuffer size : 10054,\tepsilon : 10.7%, \t r <=40 0.0, \t r > 40 39.21568627450981\n","[Episode 750]\t rewards globals : 3.808 \tavg rewards : 4.706,\tavg loss: : 0.080934,\tbuffer size : 10885,\tepsilon : 9.9%, \t r <=40 0.0, \t r > 40 47.05882352941176\n","[Episode 800]\t rewards globals : 3.895 \tavg rewards : 5.294,\tavg loss: : 0.080288,\tbuffer size : 11621,\tepsilon : 9.3%, \t r <=40 0.0, \t r > 40 52.94117647058824\n","[Episode 850]\t rewards globals : 4.031 \tavg rewards : 6.275,\tavg loss: : 0.081502,\tbuffer size : 12404,\tepsilon : 8.6%, \t r <=40 0.0, \t r > 40 62.745098039215684\n","[Episode 900]\t rewards globals : 4.073 \tavg rewards : 4.706,\tavg loss: : 0.081301,\tbuffer size : 13069,\tepsilon : 8.1%, \t r <=40 0.0, \t r > 40 47.05882352941176\n","[Episode 950]\t rewards globals : 4.154 \tavg rewards : 5.686,\tavg loss: : 0.081003,\tbuffer size : 13869,\tepsilon : 7.5%, \t r <=40 0.0, \t r > 40 56.86274509803921\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 7.8\n","[task_2_tmax40] 100 run(s) avg rewards : 7.8\n","Point: 7.8\n","[Episode 1000]\t rewards globals : 4.236 \tavg rewards : 5.686,\tavg loss: : 0.080406,\tbuffer size : 14729,\tepsilon : 7.0%, \t r <=40 0.0, \t r > 40 56.86274509803921\n","[Episode 1050]\t rewards globals : 4.310 \tavg rewards : 5.882,\tavg loss: : 0.081117,\tbuffer size : 15537,\tepsilon : 6.6%, \t r <=40 0.0, \t r > 40 58.82352941176471\n","[Episode 1100]\t rewards globals : 4.360 \tavg rewards : 5.294,\tavg loss: : 0.081573,\tbuffer size : 16367,\tepsilon : 6.2%, \t r <=40 0.0, \t r > 40 52.94117647058824\n","[Episode 1150]\t rewards globals : 4.431 \tavg rewards : 6.078,\tavg loss: : 0.081318,\tbuffer size : 17163,\tepsilon : 5.8%, \t r <=40 0.0, \t r > 40 60.78431372549019\n","[Episode 1200]\t rewards globals : 4.505 \tavg rewards : 6.275,\tavg loss: : 0.081188,\tbuffer size : 18056,\tepsilon : 5.4%, \t r <=40 0.0, \t r > 40 62.745098039215684\n","[Episode 1250]\t rewards globals : 4.524 \tavg rewards : 4.902,\tavg loss: : 0.081892,\tbuffer size : 18880,\tepsilon : 5.1%, \t r <=40 0.0, \t r > 40 49.01960784313725\n","[Episode 1300]\t rewards globals : 4.612 \tavg rewards : 6.667,\tavg loss: : 0.081844,\tbuffer size : 19705,\tepsilon : 4.8%, \t r <=40 0.0, \t r > 40 66.66666666666666\n","[Episode 1350]\t rewards globals : 4.648 \tavg rewards : 5.490,\tavg loss: : 0.081970,\tbuffer size : 20569,\tepsilon : 4.5%, \t r <=40 0.0, \t r > 40 54.90196078431373\n","[Episode 1400]\t rewards globals : 4.718 \tavg rewards : 6.471,\tavg loss: : 0.082259,\tbuffer size : 21440,\tepsilon : 4.2%, \t r <=40 0.0, \t r > 40 64.70588235294117\n","[Episode 1450]\t rewards globals : 4.797 \tavg rewards : 7.059,\tavg loss: : 0.082958,\tbuffer size : 22367,\tepsilon : 4.0%, \t r <=40 0.0, \t r > 40 70.58823529411765\n","device is: cuda\n","[task_2_tmax50] 100 run(s) avg rewards : 7.6\n","[task_2_tmax40] 100 run(s) avg rewards : 8.5\n","Point: 8.05\n","[Episode 1500]\t rewards globals : 4.857 \tavg rewards : 6.667,\tavg loss: : 0.083288,\tbuffer size : 23275,\tepsilon : 3.8%, \t r <=40 0.0, \t r > 40 66.66666666666666\n","[Episode 1550]\t rewards globals : 4.932 \tavg rewards : 7.255,\tavg loss: : 0.083524,\tbuffer size : 24210,\tepsilon : 3.5%, \t r <=40 0.0, \t r > 40 72.54901960784314\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cjrdeWB-JA1x","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":437},"outputId":"69282a57-b505-47f2-9cc9-f015b81066e8","executionInfo":{"status":"ok","timestamp":1587497035461,"user_tz":-480,"elapsed":100945,"user":{"displayName":"N","photoUrl":"","userId":"08619141603155082384"}}},"source":["# Authentification / Initialization of workspace \n","\n","import os\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)\n","\n","# Create repo folders \n","\n","task_1 = False\n","# If False task_2 workplace will be downloaded\n","\n","if task_1:\n","  number_task = 1\n","else:\n","  number_task = 2\n","\n","root = '/content'\n","local_download = os.path.join(root,'HW3_Task{}'.format(number_task))\n","\n","if not(os.path.exists(local_download)): \n","  os.mkdir(local_download)\n","\n","local_download_agent = os.path.join(local_download,'agent')\n","\n","if not(os.path.exists(local_download_agent)): \n","  os.mkdir(local_download_agent)\n","\n","  # Download files \n","\n","\n","def download_list(file_list,path_name):\n","    \n","  error_l = [];\n","    \n","  for f in file_list:\n","    # 3. Create & download by id.\n","  \n","    print('file found : title: %s, id: %s' % (f['title'], f['id']))\n","    try:\n","      #print('title: %s, id: %s' % (f['title'], f['id']))\n","      fname = os.path.join(path_name, f['title'])\n","      \n","      # Download only .py files\n","      if fname[-3:] == \".py\":\n","        print('downloading to {}'.format(fname))\n","        f_ = drive.CreateFile({'id': f['id']})\n","        f_.GetContentFile(fname)\n","      \n","    except:\n","      print(\"there is an error\")          \n","      error_l.append(fname)\n","\n","\n","if task_1:\n","  # Initial folder\n","  local_download_path = local_download_agent\n","\n","  # Agent files \n","  file_list = drive.ListFile(\n","        {'q': \"'1ktZR8KIIWEi8Cre92SFomSEchWleHtSu' in parents\"}).GetList()\n","\n","  download_list(file_list,local_download_path)\n","\n","\n","  # Initial files\n","  local_download_path = local_download\n","\n","  file_list = drive.ListFile(\n","        {'q': \"'1x4sJIKHA6NZ5y78AnOJeCZP8abI4gLLO' in parents\"}).GetList()\n","\n","  download_list(file_list,local_download_path)\n","\n","else:\n","\n","  # Initial folder\n","  local_download_path = local_download_agent\n","\n","  # Agent files \n","  file_list = drive.ListFile(\n","        {'q': \"'1PUrHkG6ki4JsiXtKOtvc2vAuR6IJMnCa' in parents\"}).GetList()\n","\n","  download_list(file_list,local_download_path)\n","\n","\n","  # Initial files\n","  local_download_path = local_download\n","\n","  file_list = drive.ListFile(\n","        {'q': \"'17wsroYnRmoTt-OIzDJRrUNvVFmohBVFR' in parents\"}).GetList()\n","\n","  download_list(file_list,local_download_path)\n","\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["file found : title: state_change.py, id: 1NUFRvldgt6dRkjycttkz11dmY56cRIEz\n","downloading to /content/HW3_Task2/agent/state_change.py\n","file found : title: models.py, id: 1BW6dbcbWvmC7JbqE78269K1IXIPwtlEi\n","downloading to /content/HW3_Task2/agent/models.py\n","file found : title: __pycache__, id: 1OfJrkmLCYqUS8vVjJnxgeZCvQEnQSQ4J\n","file found : title: env.py, id: 1bYr6NdzVOXe77n-gUac3VdGA8tnpS8HJ\n","downloading to /content/HW3_Task2/agent/env.py\n","file found : title: __init__.py, id: 1WO1O3RwqCpk7XdN07UUZbF6YTaj5mGGg\n","downloading to /content/HW3_Task2/agent/__init__.py\n","file found : title: model_last_20_05_actionR_cb_.textClipping, id: 1aESmsKWtCbxwgAAqKV9wP1eE4xDOCBFp\n","file found : title: model_last_20_05_actionR_cb_8_5000.pt, id: 1FdLXoWHZQz8l5hS76ygQJXe2HWa-wLJI\n","file found : title: model.pt, id: 1_hr_fysoSC6AXix1Qjqq0bBYk0KiSIcq\n","file found : title: .ipynb_checkpoints, id: 14rimDklDDBO4_OqcqfbiybhzNMxQQFSm\n","file found : title: ZZZ.zip, id: 1CVWs9pxeQ6FieIRMszIIfNK1my32N3Wl\n","file found : title: ZZZ, id: 10kHEkPFSjAINnDqi2s5KF7PxQN9bImk3\n","file found : title: try2 2, id: 1Ka1yYoo2aGsxPITkqKfBYB4AAHln0aMZ\n","file found : title: try2.zip, id: 1o5hCQ_s6UkmcbRNTkR8crcNcftp1IhZ7\n","file found : title: try2 2.zip, id: 1R8lmVqVxgvfuQ150WK4ZckNmfjvWmvW4\n","file found : title: try2, id: 1T1A0dN0enM4yy8DmA6A7HJOFyGQS0IlX\n","file found : title: A0212190W_Task2.zip, id: 1XJ9tRmPvxju3IjD76WkmINyUlYTIJvVm\n","file found : title: agent, id: 1PUrHkG6ki4JsiXtKOtvc2vAuR6IJMnCa\n","file found : title: setup.py, id: 1GrxqesqYqNJit3lNQABi9n3Wisa5zuir\n","downloading to /content/HW3_Task2/setup.py\n","file found : title: MANIFEST.in, id: 16_Iy5dYGYMUP2T9hl-OZdODIhV7dbgCf\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kM9mre8d4VL-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"29d721b9-e96f-4987-baab-3009f1442b43","executionInfo":{"status":"ok","timestamp":1587461190711,"user_tz":-480,"elapsed":6406630,"user":{"displayName":"N","photoUrl":"","userId":"08619141603155082384"}}},"source":["# TASK 2\n","\n","import sys\n","import time\n","\n","import torch\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n","\n","date_save = time.strftime(\"%d %H %M\")\n","model_path = os.path.join('HW3_Task2','agent', 'model_{}_{}.pt'.format(date_save,2))\n","save_path = os.path.join('HW3_Task2','agent', 'model_{}_{}.pt'.format(date_save,3))\n","\n","duree = 0 \n","nb_bad_init = 0 \n","# Try for 10 diferent inits\n","\n","for i in range(0,1):\n","  debut = time.time()\n","  # Try an init \n","  #!python3 HW3_Task2/agent/models.py --test --path=HW3_Task2/agent/model_last1_20_05_cb_8_1000.pt --savepath=HW3_Task2/agent/model_last1_20_05\n","  !python3 HW3_Task2/agent/models.py --pretrain --path=HW3_Task2/agent/model_last_21_05_cb_final_8.pt --savepath=HW3_Task2/agent/model_last2_21_05\n","  #!python3 HW3_Task2/agent/models.py --test --path=HW3_Task2/agent/model_last_20_05_actionR_cb_8_19000.pt --savepath=HW3_Task2/agent/model_last_20_05\n","  #!python3 HW3_Task2/agent/testenv.py\n","  duree = (time.time()-debut)\n","  # If good init exit loop\n","  print(duree)\n","  if duree > 10000:\n","    break\n"],"execution_count":8,"outputs":[{"output_type":"stream","text":["cuda\n","pretrain mode\n","model loaded from HW3_Task2/agent/model_last_21_05_cb_final_8.pt\n","ConvDQN(\n","  (features): Sequential(\n","    (0): Conv2d(4, 32, kernel_size=(2, 2), stride=(1, 1))\n","    (1): ReLU()\n","    (2): Conv2d(32, 64, kernel_size=(2, 2), stride=(1, 1))\n","    (3): ReLU()\n","  )\n","  (layers): Sequential(\n","    (0): Linear(in_features=24576, out_features=256, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=256, out_features=5, bias=True)\n","  )\n",")\n","range change : x_min 49 x_max 49\n","environment change : i_1 49 j_1 9\n","[Episode 100]\t rewards globals : 1.307 \tavg rewards : 1.307,\tavg loss: : 0.487006,\tbuffer size : 1954,\tepsilon : 27.1%, \t r <=40 1.9801980198019802, \t r > 40 0.0\n","[Episode 200]\t rewards globals : 2.320 \tavg rewards : 3.314,\tavg loss: : 0.485724,\tbuffer size : 4000,\tepsilon : 10.6%, \t r <=40 11.881188118811881, \t r > 40 1.9801980198019802\n","[Episode 300]\t rewards globals : 3.740 \tavg rewards : 6.726,\tavg loss: : 0.488967,\tbuffer size : 4000,\tepsilon : 4.5%, \t r <=40 29.7029702970297, \t r > 40 1.9801980198019802\n","[Episode 400]\t rewards globals : 4.945 \tavg rewards : 8.503,\tavg loss: : 0.468531,\tbuffer size : 4000,\tepsilon : 2.3%, \t r <=40 29.7029702970297, \t r > 40 20.792079207920793\n","Test mode, runs: 100, i: 49, j: 9, final: True\n","[task_2_tmax50] 100 run(s) avg rewards : 6.0\n","[task_2_tmax40] 100 run(s) avg rewards : 4.3\n","Point: 5.15\n","Local runtime: 68.14006280899048 seconds --- fast\n","WARNING: do note that this might not reflect the runtime on the server.\n","[Episode 500]\t rewards globals : 5.972 \tavg rewards : 9.995,\tavg loss: : 0.447532,\tbuffer size : 4000,\tepsilon : 1.5%, \t r <=40 42.57425742574257, \t r > 40 9.900990099009901\n","[Episode 600]\t rewards globals : 6.709 \tavg rewards : 10.500,\tavg loss: : 0.433261,\tbuffer size : 4000,\tepsilon : 1.2%, \t r <=40 41.584158415841586, \t r > 40 17.82178217821782\n","[Episode 700]\t rewards globals : 7.312 \tavg rewards : 11.023,\tavg loss: : 0.426262,\tbuffer size : 4000,\tepsilon : 1.1%, \t r <=40 49.504950495049506, \t r > 40 6.9306930693069315\n","[Episode 800]\t rewards globals : 7.738 \tavg rewards : 10.818,\tavg loss: : 0.415519,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 45.54455445544555, \t r > 40 12.871287128712872\n","[Episode 900]\t rewards globals : 8.145 \tavg rewards : 11.493,\tavg loss: : 0.406827,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 50.495049504950494, \t r > 40 9.900990099009901\n","environment change : i_1 49 j_1 9\n","Test mode, runs: 100, i: 49, j: 9, final: True\n","[task_2_tmax50] 100 run(s) avg rewards : 6.5\n","[task_2_tmax40] 100 run(s) avg rewards : 5.1\n","Point: 5.8\n","Local runtime: 68.94641470909119 seconds --- fast\n","WARNING: do note that this might not reflect the runtime on the server.\n","[Episode 1000]\t rewards globals : 8.322 \tavg rewards : 9.828,\tavg loss: : 0.401587,\tbuffer size : 4000,\tepsilon : 71.8%, \t r <=40 38.613861386138616, \t r > 40 16.831683168316832\n","[Episode 1100]\t rewards globals : 7.699 \tavg rewards : 1.450,\tavg loss: : 0.396805,\tbuffer size : 4000,\tepsilon : 27.1%, \t r <=40 3.9603960396039604, \t r > 40 0.0\n","[Episode 1200]\t rewards globals : 7.445 \tavg rewards : 4.601,\tavg loss: : 0.398412,\tbuffer size : 4000,\tepsilon : 10.6%, \t r <=40 18.81188118811881, \t r > 40 0.0\n","[Episode 1300]\t rewards globals : 7.619 \tavg rewards : 9.806,\tavg loss: : 0.398826,\tbuffer size : 4000,\tepsilon : 4.5%, \t r <=40 42.57425742574257, \t r > 40 7.920792079207921\n","[Episode 1400]\t rewards globals : 7.686 \tavg rewards : 8.667,\tavg loss: : 0.397236,\tbuffer size : 4000,\tepsilon : 2.3%, \t r <=40 31.683168316831683, \t r > 40 18.81188118811881\n","Test mode, runs: 100, i: 49, j: 9, final: True\n","[task_2_tmax50] 100 run(s) avg rewards : 6.4\n","[task_2_tmax40] 100 run(s) avg rewards : 4.9\n","Point: 5.65\n","Local runtime: 70.49300003051758 seconds --- fast\n","WARNING: do note that this might not reflect the runtime on the server.\n","[Episode 1500]\t rewards globals : 7.869 \tavg rewards : 10.535,\tavg loss: : 0.392010,\tbuffer size : 4000,\tepsilon : 1.5%, \t r <=40 42.57425742574257, \t r > 40 15.841584158415841\n","[Episode 1600]\t rewards globals : 8.100 \tavg rewards : 11.468,\tavg loss: : 0.388609,\tbuffer size : 4000,\tepsilon : 1.2%, \t r <=40 48.51485148514851, \t r > 40 13.861386138613863\n","[Episode 1700]\t rewards globals : 8.348 \tavg rewards : 12.397,\tavg loss: : 0.383431,\tbuffer size : 4000,\tepsilon : 1.1%, \t r <=40 50.495049504950494, \t r > 40 20.792079207920793\n","[Episode 1800]\t rewards globals : 8.454 \tavg rewards : 10.355,\tavg loss: : 0.379069,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 37.62376237623762, \t r > 40 24.752475247524753\n","[Episode 1900]\t rewards globals : 8.616 \tavg rewards : 11.511,\tavg loss: : 0.375020,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 44.554455445544555, \t r > 40 22.772277227722775\n","environment change : i_1 49 j_1 9\n","Test mode, runs: 100, i: 49, j: 9, final: True\n","[task_2_tmax50] 100 run(s) avg rewards : 6.1\n","[task_2_tmax40] 100 run(s) avg rewards : 3.2\n","Point: 4.65\n","Local runtime: 73.01526832580566 seconds --- fast\n","WARNING: do note that this might not reflect the runtime on the server.\n","[Episode 2000]\t rewards globals : 8.717 \tavg rewards : 10.733,\tavg loss: : 0.370258,\tbuffer size : 4000,\tepsilon : 71.8%, \t r <=40 40.5940594059406, \t r > 40 22.772277227722775\n","[Episode 2100]\t rewards globals : 8.362 \tavg rewards : 1.242,\tavg loss: : 0.368516,\tbuffer size : 4000,\tepsilon : 27.1%, \t r <=40 2.9702970297029703, \t r > 40 0.9900990099009901\n","[Episode 2200]\t rewards globals : 8.177 \tavg rewards : 4.254,\tavg loss: : 0.370929,\tbuffer size : 4000,\tepsilon : 10.6%, \t r <=40 16.831683168316832, \t r > 40 1.9801980198019802\n","[Episode 2300]\t rewards globals : 8.104 \tavg rewards : 6.431,\tavg loss: : 0.372289,\tbuffer size : 4000,\tepsilon : 4.5%, \t r <=40 25.742574257425744, \t r > 40 6.9306930693069315\n","[Episode 2400]\t rewards globals : 8.113 \tavg rewards : 8.259,\tavg loss: : 0.372595,\tbuffer size : 4000,\tepsilon : 2.3%, \t r <=40 34.65346534653465, \t r > 40 7.920792079207921\n","Test mode, runs: 100, i: 49, j: 9, final: True\n","[task_2_tmax50] 100 run(s) avg rewards : 6.3\n","[task_2_tmax40] 100 run(s) avg rewards : 2.2\n","Point: 4.25\n","Local runtime: 75.30415272712708 seconds --- fast\n","WARNING: do note that this might not reflect the runtime on the server.\n","[Episode 2500]\t rewards globals : 8.111 \tavg rewards : 8.185,\tavg loss: : 0.372474,\tbuffer size : 4000,\tepsilon : 1.5%, \t r <=40 30.693069306930692, \t r > 40 15.841584158415841\n","[Episode 2600]\t rewards globals : 8.093 \tavg rewards : 7.658,\tavg loss: : 0.371592,\tbuffer size : 4000,\tepsilon : 1.2%, \t r <=40 25.742574257425744, \t r > 40 20.792079207920793\n","[Episode 2700]\t rewards globals : 8.098 \tavg rewards : 8.164,\tavg loss: : 0.370188,\tbuffer size : 4000,\tepsilon : 1.1%, \t r <=40 28.71287128712871, \t r > 40 19.801980198019802\n","[Episode 2800]\t rewards globals : 8.104 \tavg rewards : 8.190,\tavg loss: : 0.369400,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 25.742574257425744, \t r > 40 26.732673267326735\n","[Episode 2900]\t rewards globals : 8.167 \tavg rewards : 10.012,\tavg loss: : 0.368368,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 35.64356435643564, \t r > 40 25.742574257425744\n","environment change : i_1 49 j_1 9\n","Test mode, runs: 100, i: 49, j: 9, final: True\n","[task_2_tmax50] 100 run(s) avg rewards : 6.8\n","[task_2_tmax40] 100 run(s) avg rewards : 4.1\n","Point: 5.449999999999999\n","Local runtime: 70.9747211933136 seconds --- fast\n","WARNING: do note that this might not reflect the runtime on the server.\n","[Episode 3000]\t rewards globals : 8.258 \tavg rewards : 10.885,\tavg loss: : 0.365928,\tbuffer size : 4000,\tepsilon : 71.8%, \t r <=40 40.5940594059406, \t r > 40 24.752475247524753\n","[Episode 3100]\t rewards globals : 8.026 \tavg rewards : 1.062,\tavg loss: : 0.365514,\tbuffer size : 4000,\tepsilon : 27.1%, \t r <=40 1.9801980198019802, \t r > 40 0.0\n","[Episode 3200]\t rewards globals : 7.918 \tavg rewards : 4.742,\tavg loss: : 0.367400,\tbuffer size : 4000,\tepsilon : 10.6%, \t r <=40 18.81188118811881, \t r > 40 1.9801980198019802\n","[Episode 3300]\t rewards globals : 7.891 \tavg rewards : 6.954,\tavg loss: : 0.369418,\tbuffer size : 4000,\tepsilon : 4.5%, \t r <=40 29.7029702970297, \t r > 40 3.9603960396039604\n","[Episode 3400]\t rewards globals : 7.898 \tavg rewards : 8.049,\tavg loss: : 0.369848,\tbuffer size : 4000,\tepsilon : 2.3%, \t r <=40 33.663366336633665, \t r > 40 6.9306930693069315\n","Test mode, runs: 100, i: 49, j: 9, final: True\n","[task_2_tmax50] 100 run(s) avg rewards : 5.8\n","[task_2_tmax40] 100 run(s) avg rewards : 2.2\n","Point: 4.0\n","Local runtime: 75.44797778129578 seconds --- fast\n","WARNING: do note that this might not reflect the runtime on the server.\n","[Episode 3500]\t rewards globals : 7.947 \tavg rewards : 9.547,\tavg loss: : 0.369469,\tbuffer size : 4000,\tepsilon : 1.5%, \t r <=40 38.613861386138616, \t r > 40 13.861386138613863\n","[Episode 3600]\t rewards globals : 7.954 \tavg rewards : 8.106,\tavg loss: : 0.367103,\tbuffer size : 4000,\tepsilon : 1.2%, \t r <=40 29.7029702970297, \t r > 40 16.831683168316832\n","[Episode 3700]\t rewards globals : 7.954 \tavg rewards : 7.895,\tavg loss: : 0.364324,\tbuffer size : 4000,\tepsilon : 1.1%, \t r <=40 27.722772277227726, \t r > 40 18.81188118811881\n","[Episode 3800]\t rewards globals : 7.947 \tavg rewards : 7.702,\tavg loss: : 0.363024,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 30.693069306930692, \t r > 40 9.900990099009901\n","[Episode 3900]\t rewards globals : 7.992 \tavg rewards : 9.808,\tavg loss: : 0.361860,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 37.62376237623762, \t r > 40 18.81188118811881\n","environment change : i_1 49 j_1 9\n","Test mode, runs: 100, i: 49, j: 9, final: True\n","[task_2_tmax50] 100 run(s) avg rewards : 6.2\n","[task_2_tmax40] 100 run(s) avg rewards : 4.6\n","Point: 5.4\n","Local runtime: 68.9856345653534 seconds --- fast\n","WARNING: do note that this might not reflect the runtime on the server.\n","[Episode 4000]\t rewards globals : 8.044 \tavg rewards : 9.976,\tavg loss: : 0.359824,\tbuffer size : 4000,\tepsilon : 71.8%, \t r <=40 37.62376237623762, \t r > 40 20.792079207920793\n","[Episode 4100]\t rewards globals : 7.875 \tavg rewards : 1.107,\tavg loss: : 0.359076,\tbuffer size : 4000,\tepsilon : 27.1%, \t r <=40 1.9801980198019802, \t r > 40 0.0\n","[Episode 4200]\t rewards globals : 7.787 \tavg rewards : 4.139,\tavg loss: : 0.359668,\tbuffer size : 4000,\tepsilon : 10.6%, \t r <=40 15.841584158415841, \t r > 40 2.9702970297029703\n","[Episode 4300]\t rewards globals : 7.791 \tavg rewards : 7.909,\tavg loss: : 0.360164,\tbuffer size : 4000,\tepsilon : 4.5%, \t r <=40 32.67326732673268, \t r > 40 7.920792079207921\n","[Episode 4400]\t rewards globals : 7.837 \tavg rewards : 9.897,\tavg loss: : 0.359834,\tbuffer size : 4000,\tepsilon : 2.3%, \t r <=40 45.54455445544555, \t r > 40 1.9801980198019802\n","Test mode, runs: 100, i: 49, j: 9, final: True\n","[task_2_tmax50] 100 run(s) avg rewards : 6.3\n","[task_2_tmax40] 100 run(s) avg rewards : 4.2\n","Point: 5.25\n","Local runtime: 73.5629563331604 seconds --- fast\n","WARNING: do note that this might not reflect the runtime on the server.\n","[Episode 4500]\t rewards globals : 7.859 \tavg rewards : 8.746,\tavg loss: : 0.358643,\tbuffer size : 4000,\tepsilon : 1.5%, \t r <=40 35.64356435643564, \t r > 40 10.891089108910892\n","[Episode 4600]\t rewards globals : 7.935 \tavg rewards : 11.428,\tavg loss: : 0.357201,\tbuffer size : 4000,\tepsilon : 1.2%, \t r <=40 51.48514851485149, \t r > 40 6.9306930693069315\n","[Episode 4700]\t rewards globals : 8.014 \tavg rewards : 11.771,\tavg loss: : 0.356311,\tbuffer size : 4000,\tepsilon : 1.1%, \t r <=40 52.475247524752476, \t r > 40 8.91089108910891\n","[Episode 4800]\t rewards globals : 8.100 \tavg rewards : 12.210,\tavg loss: : 0.355781,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 56.43564356435643, \t r > 40 4.9504950495049505\n","[Episode 4900]\t rewards globals : 8.174 \tavg rewards : 11.823,\tavg loss: : 0.355468,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 50.495049504950494, \t r > 40 13.861386138613863\n","environment change : i_1 49 j_1 9\n","Test mode, runs: 100, i: 49, j: 9, final: True\n","[task_2_tmax50] 100 run(s) avg rewards : 5.8\n","[task_2_tmax40] 100 run(s) avg rewards : 4.8\n","Point: 5.3\n","Local runtime: 71.10075449943542 seconds --- fast\n","WARNING: do note that this might not reflect the runtime on the server.\n","[Episode 5000]\t rewards globals : 8.204 \tavg rewards : 9.751,\tavg loss: : 0.354907,\tbuffer size : 4000,\tepsilon : 71.8%, \t r <=40 39.603960396039604, \t r > 40 13.861386138613863\n","[Episode 5100]\t rewards globals : 8.066 \tavg rewards : 1.178,\tavg loss: : 0.354237,\tbuffer size : 4000,\tepsilon : 27.1%, \t r <=40 1.9801980198019802, \t r > 40 0.0\n","[Episode 5200]\t rewards globals : 7.998 \tavg rewards : 4.445,\tavg loss: : 0.354616,\tbuffer size : 4000,\tepsilon : 10.6%, \t r <=40 17.82178217821782, \t r > 40 0.0\n","[Episode 5300]\t rewards globals : 7.974 \tavg rewards : 6.679,\tavg loss: : 0.355058,\tbuffer size : 4000,\tepsilon : 4.5%, \t r <=40 27.722772277227726, \t r > 40 3.9603960396039604\n","[Episode 5400]\t rewards globals : 7.998 \tavg rewards : 9.201,\tavg loss: : 0.356351,\tbuffer size : 4000,\tepsilon : 2.3%, \t r <=40 41.584158415841586, \t r > 40 1.9801980198019802\n","Test mode, runs: 100, i: 49, j: 9, final: True\n","[task_2_tmax50] 100 run(s) avg rewards : 5.9\n","[task_2_tmax40] 100 run(s) avg rewards : 5.3\n","Point: 5.6\n","Local runtime: 69.19340896606445 seconds --- fast\n","WARNING: do note that this might not reflect the runtime on the server.\n","[Episode 5500]\t rewards globals : 8.025 \tavg rewards : 9.385,\tavg loss: : 0.356622,\tbuffer size : 4000,\tepsilon : 1.5%, \t r <=40 39.603960396039604, \t r > 40 7.920792079207921\n","[Episode 5600]\t rewards globals : 8.102 \tavg rewards : 12.268,\tavg loss: : 0.356688,\tbuffer size : 4000,\tepsilon : 1.2%, \t r <=40 56.43564356435643, \t r > 40 5.9405940594059405\n","[Episode 5700]\t rewards globals : 8.115 \tavg rewards : 8.937,\tavg loss: : 0.357370,\tbuffer size : 4000,\tepsilon : 1.1%, \t r <=40 38.613861386138616, \t r > 40 5.9405940594059405\n","[Episode 5800]\t rewards globals : 8.155 \tavg rewards : 10.524,\tavg loss: : 0.357379,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 46.53465346534654, \t r > 40 6.9306930693069315\n","[Episode 5900]\t rewards globals : 8.185 \tavg rewards : 10.008,\tavg loss: : 0.358086,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 45.54455445544555, \t r > 40 2.9702970297029703\n","environment change : i_1 49 j_1 9\n","Test mode, runs: 100, i: 49, j: 9, final: True\n","[task_2_tmax50] 100 run(s) avg rewards : 5.3\n","[task_2_tmax40] 100 run(s) avg rewards : 5.7\n","Point: 5.5\n","Local runtime: 61.77952694892883 seconds --- fast\n","WARNING: do note that this might not reflect the runtime on the server.\n","[Episode 6000]\t rewards globals : 8.249 \tavg rewards : 11.913,\tavg loss: : 0.358362,\tbuffer size : 4000,\tepsilon : 71.8%, \t r <=40 56.43564356435643, \t r > 40 0.9900990099009901\n","[Episode 6100]\t rewards globals : 8.134 \tavg rewards : 1.235,\tavg loss: : 0.359214,\tbuffer size : 4000,\tepsilon : 27.1%, \t r <=40 1.9801980198019802, \t r > 40 0.0\n","[Episode 6200]\t rewards globals : 8.070 \tavg rewards : 4.145,\tavg loss: : 0.360747,\tbuffer size : 4000,\tepsilon : 10.6%, \t r <=40 16.831683168316832, \t r > 40 0.9900990099009901\n","[Episode 6300]\t rewards globals : 8.048 \tavg rewards : 6.639,\tavg loss: : 0.363375,\tbuffer size : 4000,\tepsilon : 4.5%, \t r <=40 28.71287128712871, \t r > 40 0.0\n","[Episode 6400]\t rewards globals : 8.078 \tavg rewards : 10.061,\tavg loss: : 0.364702,\tbuffer size : 4000,\tepsilon : 2.3%, \t r <=40 45.54455445544555, \t r > 40 3.9603960396039604\n","Test mode, runs: 100, i: 49, j: 9, final: True\n","[task_2_tmax50] 100 run(s) avg rewards : 5.9\n","[task_2_tmax40] 100 run(s) avg rewards : 5.9\n","Point: 5.9\n","Local runtime: 66.26968693733215 seconds --- fast\n","WARNING: do note that this might not reflect the runtime on the server.\n","[Episode 6500]\t rewards globals : 8.116 \tavg rewards : 10.458,\tavg loss: : 0.364702,\tbuffer size : 4000,\tepsilon : 1.5%, \t r <=40 46.53465346534654, \t r > 40 5.9405940594059405\n","[Episode 6600]\t rewards globals : 8.157 \tavg rewards : 10.898,\tavg loss: : 0.364437,\tbuffer size : 4000,\tepsilon : 1.2%, \t r <=40 49.504950495049506, \t r > 40 4.9504950495049505\n","[Episode 6700]\t rewards globals : 8.195 \tavg rewards : 10.825,\tavg loss: : 0.363360,\tbuffer size : 4000,\tepsilon : 1.1%, \t r <=40 47.524752475247524, \t r > 40 7.920792079207921\n","[Episode 6800]\t rewards globals : 8.237 \tavg rewards : 10.915,\tavg loss: : 0.362636,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 49.504950495049506, \t r > 40 4.9504950495049505\n","[Episode 6900]\t rewards globals : 8.288 \tavg rewards : 11.836,\tavg loss: : 0.361712,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 54.45544554455446, \t r > 40 4.9504950495049505\n","environment change : i_1 49 j_1 9\n","Test mode, runs: 100, i: 49, j: 9, final: True\n","[task_2_tmax50] 100 run(s) avg rewards : 6.1\n","[task_2_tmax40] 100 run(s) avg rewards : 6.1\n","Point: 6.1\n","Local runtime: 62.67900896072388 seconds --- fast\n","WARNING: do note that this might not reflect the runtime on the server.\n","[Episode 7000]\t rewards globals : 8.357 \tavg rewards : 13.182,\tavg loss: : 0.361528,\tbuffer size : 4000,\tepsilon : 71.8%, \t r <=40 63.366336633663366, \t r > 40 0.9900990099009901\n","[Episode 7100]\t rewards globals : 8.258 \tavg rewards : 1.346,\tavg loss: : 0.362219,\tbuffer size : 4000,\tepsilon : 27.1%, \t r <=40 2.9702970297029703, \t r > 40 0.0\n","[Episode 7200]\t rewards globals : 8.198 \tavg rewards : 3.884,\tavg loss: : 0.363280,\tbuffer size : 4000,\tepsilon : 10.6%, \t r <=40 14.85148514851485, \t r > 40 0.0\n","[Episode 7300]\t rewards globals : 8.205 \tavg rewards : 8.636,\tavg loss: : 0.364290,\tbuffer size : 4000,\tepsilon : 4.5%, \t r <=40 37.62376237623762, \t r > 40 4.9504950495049505\n","[Episode 7400]\t rewards globals : 8.234 \tavg rewards : 10.465,\tavg loss: : 0.365095,\tbuffer size : 4000,\tepsilon : 2.3%, \t r <=40 48.51485148514851, \t r > 40 1.9801980198019802\n","Test mode, runs: 100, i: 49, j: 9, final: True\n","[task_2_tmax50] 100 run(s) avg rewards : 5.6\n","[task_2_tmax40] 100 run(s) avg rewards : 5.3\n","Point: 5.449999999999999\n","Local runtime: 69.9328191280365 seconds --- fast\n","WARNING: do note that this might not reflect the runtime on the server.\n","[Episode 7500]\t rewards globals : 8.244 \tavg rewards : 8.889,\tavg loss: : 0.365604,\tbuffer size : 4000,\tepsilon : 1.5%, \t r <=40 38.613861386138616, \t r > 40 5.9405940594059405\n","[Episode 7600]\t rewards globals : 8.241 \tavg rewards : 7.950,\tavg loss: : 0.365794,\tbuffer size : 4000,\tepsilon : 1.2%, \t r <=40 33.663366336633665, \t r > 40 5.9405940594059405\n","[Episode 7700]\t rewards globals : 8.287 \tavg rewards : 11.705,\tavg loss: : 0.366448,\tbuffer size : 4000,\tepsilon : 1.1%, \t r <=40 51.48514851485149, \t r > 40 9.900990099009901\n","[Episode 7800]\t rewards globals : 8.329 \tavg rewards : 11.605,\tavg loss: : 0.367022,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 53.46534653465347, \t r > 40 3.9603960396039604\n","[Episode 7900]\t rewards globals : 8.385 \tavg rewards : 12.877,\tavg loss: : 0.367375,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 60.396039603960396, \t r > 40 3.9603960396039604\n","environment change : i_1 49 j_1 9\n","Test mode, runs: 100, i: 49, j: 9, final: True\n","[task_2_tmax50] 100 run(s) avg rewards : 5.8\n","[task_2_tmax40] 100 run(s) avg rewards : 5.7\n","Point: 5.75\n","Local runtime: 69.9330050945282 seconds --- fast\n","WARNING: do note that this might not reflect the runtime on the server.\n","[Episode 8000]\t rewards globals : 8.421 \tavg rewards : 11.344,\tavg loss: : 0.367578,\tbuffer size : 4000,\tepsilon : 71.8%, \t r <=40 51.48514851485149, \t r > 40 5.9405940594059405\n","[Episode 8100]\t rewards globals : 8.336 \tavg rewards : 1.470,\tavg loss: : 0.367843,\tbuffer size : 4000,\tepsilon : 27.1%, \t r <=40 3.9603960396039604, \t r > 40 0.0\n","[Episode 8200]\t rewards globals : 8.276 \tavg rewards : 3.647,\tavg loss: : 0.368853,\tbuffer size : 4000,\tepsilon : 10.6%, \t r <=40 12.871287128712872, \t r > 40 1.9801980198019802\n","[Episode 8300]\t rewards globals : 8.256 \tavg rewards : 6.542,\tavg loss: : 0.369916,\tbuffer size : 4000,\tepsilon : 4.5%, \t r <=40 27.722772277227726, \t r > 40 0.9900990099009901\n","[Episode 8400]\t rewards globals : 8.273 \tavg rewards : 9.555,\tavg loss: : 0.370177,\tbuffer size : 4000,\tepsilon : 2.3%, \t r <=40 40.5940594059406, \t r > 40 9.900990099009901\n","Test mode, runs: 100, i: 49, j: 9, final: True\n","[task_2_tmax50] 100 run(s) avg rewards : 6.1\n","[task_2_tmax40] 100 run(s) avg rewards : 1.9\n","Point: 4.0\n","Local runtime: 75.99441337585449 seconds --- fast\n","WARNING: do note that this might not reflect the runtime on the server.\n","[Episode 8500]\t rewards globals : 8.265 \tavg rewards : 7.771,\tavg loss: : 0.369323,\tbuffer size : 4000,\tepsilon : 1.5%, \t r <=40 33.663366336633665, \t r > 40 3.9603960396039604\n","[Episode 8600]\t rewards globals : 8.306 \tavg rewards : 11.850,\tavg loss: : 0.368922,\tbuffer size : 4000,\tepsilon : 1.2%, \t r <=40 50.495049504950494, \t r > 40 13.861386138613863\n","[Episode 8700]\t rewards globals : 8.320 \tavg rewards : 9.411,\tavg loss: : 0.368378,\tbuffer size : 4000,\tepsilon : 1.1%, \t r <=40 39.603960396039604, \t r > 40 8.91089108910891\n","[Episode 8800]\t rewards globals : 8.342 \tavg rewards : 10.342,\tavg loss: : 0.367682,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 45.54455445544555, \t r > 40 6.9306930693069315\n","[Episode 8900]\t rewards globals : 8.367 \tavg rewards : 10.631,\tavg loss: : 0.367635,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 47.524752475247524, \t r > 40 5.9405940594059405\n","environment change : i_1 49 j_1 9\n","Test mode, runs: 100, i: 49, j: 9, final: True\n","[task_2_tmax50] 100 run(s) avg rewards : 5.9\n","[task_2_tmax40] 100 run(s) avg rewards : 5.5\n","Point: 5.7\n","Local runtime: 68.52368593215942 seconds --- fast\n","WARNING: do note that this might not reflect the runtime on the server.\n","[Episode 9000]\t rewards globals : 8.396 \tavg rewards : 10.820,\tavg loss: : 0.367640,\tbuffer size : 4000,\tepsilon : 71.8%, \t r <=40 49.504950495049506, \t r > 40 3.9603960396039604\n","[Episode 9100]\t rewards globals : 8.317 \tavg rewards : 1.241,\tavg loss: : 0.368167,\tbuffer size : 4000,\tepsilon : 27.1%, \t r <=40 1.9801980198019802, \t r > 40 0.0\n","[Episode 9200]\t rewards globals : 8.261 \tavg rewards : 3.094,\tavg loss: : 0.369487,\tbuffer size : 4000,\tepsilon : 10.6%, \t r <=40 9.900990099009901, \t r > 40 1.9801980198019802\n","[Episode 9300]\t rewards globals : 8.246 \tavg rewards : 6.862,\tavg loss: : 0.371021,\tbuffer size : 4000,\tepsilon : 4.5%, \t r <=40 29.7029702970297, \t r > 40 2.9702970297029703\n","[Episode 9400]\t rewards globals : 8.262 \tavg rewards : 9.683,\tavg loss: : 0.371601,\tbuffer size : 4000,\tepsilon : 2.3%, \t r <=40 43.56435643564357, \t r > 40 3.9603960396039604\n","Test mode, runs: 100, i: 49, j: 9, final: True\n","[task_2_tmax50] 100 run(s) avg rewards : 5.5\n","[task_2_tmax40] 100 run(s) avg rewards : 3.2\n","Point: 4.35\n","Local runtime: 73.8154604434967 seconds --- fast\n","WARNING: do note that this might not reflect the runtime on the server.\n","[Episode 9500]\t rewards globals : 8.287 \tavg rewards : 10.538,\tavg loss: : 0.371717,\tbuffer size : 4000,\tepsilon : 1.5%, \t r <=40 45.54455445544555, \t r > 40 9.900990099009901\n","[Episode 9600]\t rewards globals : 8.303 \tavg rewards : 9.783,\tavg loss: : 0.371612,\tbuffer size : 4000,\tepsilon : 1.2%, \t r <=40 41.584158415841586, \t r > 40 9.900990099009901\n","[Episode 9700]\t rewards globals : 8.337 \tavg rewards : 11.487,\tavg loss: : 0.371035,\tbuffer size : 4000,\tepsilon : 1.1%, \t r <=40 50.495049504950494, \t r > 40 9.900990099009901\n","[Episode 9800]\t rewards globals : 8.358 \tavg rewards : 10.330,\tavg loss: : 0.370283,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 44.554455445544555, \t r > 40 8.91089108910891\n","[Episode 9900]\t rewards globals : 8.403 \tavg rewards : 12.919,\tavg loss: : 0.370000,\tbuffer size : 4000,\tepsilon : 1.0%, \t r <=40 58.415841584158414, \t r > 40 8.91089108910891\n","6404.733753442764\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"atNXnxM_NcTN","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oEhO5nj6eZz8","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"695142d7-8369-4bd6-b8aa-8cf94801024e"},"source":["# TASK 2\n","\n","import sys\n","import time\n","\n","import torch\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n","\n","date_save = time.strftime(\"%d %H %M\")\n","model_path = os.path.join('HW3_Task2','agent', 'model_{}_{}.pt'.format(date_save,2))\n","save_path = os.path.join('HW3_Task2','agent', 'model_{}_{}.pt'.format(date_save,3))\n","\n","duree = 0 \n","nb_bad_init = 0 \n","# Try for 10 diferent inits\n","\n","for i in range(0,1):\n","  debut = time.time()\n","  # Try an init \n","  #!python3 HW3_Task2/agent/models.py --test --path=HW3_Task2/agent/model_last1_20_05_cb_8_1000.pt --savepath=HW3_Task2/agent/model_last1_20_05\n","  !python3 HW3_Task2/agent/models.py --pretrain --path=HW3_Task2/agent/model_last_21_05_cb_final_8.pt --savepath=HW3_Task2/agent/model_last2_21_05\n","  #!python3 HW3_Task2/agent/models.py --test --path=HW3_Task2/agent/model_last_20_05_actionR_cb_8_19000.pt --savepath=HW3_Task2/agent/model_last_20_05\n","  #!python3 HW3_Task2/agent/testenv.py\n","  duree = (time.time()-debut)\n","  # If good init exit loop\n","  print(duree)\n","  if duree > 10000:\n","    break\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["cuda\n","pretrain mode\n","model loaded from HW3_Task2/agent/model_last_21_05_cb_final_8.pt\n","<class 'tuple'>\n","<class 'NoneType'>\n","ConvDQN(\n","  (features): Sequential(\n","    (0): Conv2d(4, 32, kernel_size=(2, 2), stride=(1, 1))\n","    (1): ReLU()\n","    (2): Conv2d(32, 64, kernel_size=(2, 2), stride=(1, 1))\n","    (3): ReLU()\n","  )\n","  (layers): Sequential(\n","    (0): Linear(in_features=24576, out_features=256, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=256, out_features=5, bias=True)\n","  )\n",")\n","range change : x_min 49 x_max 49\n","environment change : i_1 49 j_1 9\n","[Episode 50]\t rewards globals : 4.510 \tavg rewards : 4.510,\tavg loss: : 0.661286,\tbuffer size : 1459,\tepsilon : 17.1%, \t r <=40 19.607843137254903, \t r > 40 5.88235294117647\n","[Episode 100]\t rewards globals : 3.861 \tavg rewards : 3.137,\tavg loss: : 0.526456,\tbuffer size : 2921,\tepsilon : 14.6%, \t r <=40 13.725490196078432, \t r > 40 3.9215686274509802\n","[Episode 150]\t rewards globals : 3.576 \tavg rewards : 2.941,\tavg loss: : 0.547683,\tbuffer size : 4463,\tepsilon : 12.5%, \t r <=40 11.76470588235294, \t r > 40 5.88235294117647\n","[Episode 200]\t rewards globals : 3.582 \tavg rewards : 3.529,\tavg loss: : 0.527424,\tbuffer size : 5974,\tepsilon : 10.8%, \t r <=40 13.725490196078432, \t r > 40 7.8431372549019605\n","[Episode 250]\t rewards globals : 3.865 \tavg rewards : 4.902,\tavg loss: : 0.539627,\tbuffer size : 7629,\tepsilon : 9.3%, \t r <=40 19.607843137254903, \t r > 40 9.803921568627452\n","[Episode 300]\t rewards globals : 3.588 \tavg rewards : 2.157,\tavg loss: : 0.522487,\tbuffer size : 9219,\tepsilon : 8.0%, \t r <=40 7.8431372549019605, \t r > 40 5.88235294117647\n","[Episode 350]\t rewards globals : 3.447 \tavg rewards : 2.549,\tavg loss: : 0.537449,\tbuffer size : 10000,\tepsilon : 6.9%, \t r <=40 7.8431372549019605, \t r > 40 9.803921568627452\n","[Episode 400]\t rewards globals : 3.566 \tavg rewards : 4.314,\tavg loss: : 0.538590,\tbuffer size : 10000,\tepsilon : 6.0%, \t r <=40 15.686274509803921, \t r > 40 11.76470588235294\n","[Episode 450]\t rewards globals : 3.614 \tavg rewards : 3.922,\tavg loss: : 0.555247,\tbuffer size : 10000,\tepsilon : 5.2%, \t r <=40 15.686274509803921, \t r > 40 7.8431372549019605\n","Test mode, runs: 100, i: 49, j: 9, final: False\n","[task_2_tmax50] 100 run(s) avg rewards : 3.7\n","[task_2_tmax40] 100 run(s) avg rewards : 0.0\n","Point: 1.85\n","Local runtime: 78.26607465744019 seconds --- fast\n","WARNING: do note that this might not reflect the runtime on the server.\n","[Episode 500]\t rewards globals : 3.673 \tavg rewards : 4.118,\tavg loss: : 0.556907,\tbuffer size : 10000,\tepsilon : 4.6%, \t r <=40 13.725490196078432, \t r > 40 13.725490196078432\n","[Episode 550]\t rewards globals : 3.793 \tavg rewards : 5.294,\tavg loss: : 0.560059,\tbuffer size : 10000,\tepsilon : 4.0%, \t r <=40 19.607843137254903, \t r > 40 13.725490196078432\n","[Episode 600]\t rewards globals : 3.727 \tavg rewards : 3.137,\tavg loss: : 0.550965,\tbuffer size : 10000,\tepsilon : 3.6%, \t r <=40 11.76470588235294, \t r > 40 7.8431372549019605\n","[Episode 650]\t rewards globals : 3.948 \tavg rewards : 6.471,\tavg loss: : 0.557319,\tbuffer size : 10000,\tepsilon : 3.2%, \t r <=40 15.686274509803921, \t r > 40 33.33333333333333\n","[Episode 700]\t rewards globals : 3.966 \tavg rewards : 4.314,\tavg loss: : 0.550897,\tbuffer size : 10000,\tepsilon : 2.8%, \t r <=40 11.76470588235294, \t r > 40 19.607843137254903\n","[Episode 750]\t rewards globals : 4.075 \tavg rewards : 5.490,\tavg loss: : 0.552121,\tbuffer size : 10000,\tepsilon : 2.6%, \t r <=40 19.607843137254903, \t r > 40 15.686274509803921\n","[Episode 800]\t rewards globals : 4.182 \tavg rewards : 5.686,\tavg loss: : 0.545547,\tbuffer size : 10000,\tepsilon : 2.3%, \t r <=40 15.686274509803921, \t r > 40 25.49019607843137\n","[Episode 850]\t rewards globals : 4.371 \tavg rewards : 7.451,\tavg loss: : 0.548125,\tbuffer size : 10000,\tepsilon : 2.1%, \t r <=40 17.647058823529413, \t r > 40 39.21568627450981\n","[Episode 900]\t rewards globals : 4.517 \tavg rewards : 6.863,\tavg loss: : 0.541533,\tbuffer size : 10000,\tepsilon : 1.9%, \t r <=40 9.803921568627452, \t r > 40 49.01960784313725\n","[Episode 950]\t rewards globals : 4.648 \tavg rewards : 7.255,\tavg loss: : 0.543573,\tbuffer size : 10000,\tepsilon : 1.8%, \t r <=40 23.52941176470588, \t r > 40 25.49019607843137\n","Test mode, runs: 100, i: 49, j: 9, final: False\n","[task_2_tmax50] 100 run(s) avg rewards : 6.3\n","[task_2_tmax40] 100 run(s) avg rewards : 0.3\n","Point: 3.3\n","Local runtime: 77.66324496269226 seconds --- fast\n","WARNING: do note that this might not reflect the runtime on the server.\n","[Episode 1000]\t rewards globals : 4.745 \tavg rewards : 6.471,\tavg loss: : 0.537770,\tbuffer size : 10000,\tepsilon : 1.7%, \t r <=40 15.686274509803921, \t r > 40 33.33333333333333\n","[Episode 1050]\t rewards globals : 4.853 \tavg rewards : 6.863,\tavg loss: : 0.539622,\tbuffer size : 10000,\tepsilon : 1.6%, \t r <=40 15.686274509803921, \t r > 40 37.254901960784316\n","[Episode 1100]\t rewards globals : 4.914 \tavg rewards : 6.471,\tavg loss: : 0.534116,\tbuffer size : 10000,\tepsilon : 1.5%, \t r <=40 19.607843137254903, \t r > 40 25.49019607843137\n","[Episode 1150]\t rewards globals : 5.030 \tavg rewards : 7.843,\tavg loss: : 0.539234,\tbuffer size : 10000,\tepsilon : 1.4%, \t r <=40 27.450980392156865, \t r > 40 23.52941176470588\n","[Episode 1200]\t rewards globals : 5.121 \tavg rewards : 7.451,\tavg loss: : 0.537963,\tbuffer size : 10000,\tepsilon : 1.3%, \t r <=40 21.568627450980394, \t r > 40 31.372549019607842\n","[Episode 1250]\t rewards globals : 5.180 \tavg rewards : 6.667,\tavg loss: : 0.543616,\tbuffer size : 10000,\tepsilon : 1.3%, \t r <=40 15.686274509803921, \t r > 40 35.294117647058826\n","[Episode 1300]\t rewards globals : 5.211 \tavg rewards : 5.882,\tavg loss: : 0.542491,\tbuffer size : 10000,\tepsilon : 1.2%, \t r <=40 13.725490196078432, \t r > 40 31.372549019607842\n","[Episode 1350]\t rewards globals : 5.300 \tavg rewards : 7.647,\tavg loss: : 0.547045,\tbuffer size : 10000,\tepsilon : 1.2%, \t r <=40 15.686274509803921, \t r > 40 45.09803921568628\n","[Episode 1400]\t rewards globals : 5.396 \tavg rewards : 8.039,\tavg loss: : 0.545420,\tbuffer size : 10000,\tepsilon : 1.2%, \t r <=40 21.568627450980394, \t r > 40 37.254901960784316\n","[Episode 1450]\t rewards globals : 5.369 \tavg rewards : 4.706,\tavg loss: : 0.552152,\tbuffer size : 10000,\tepsilon : 1.2%, \t r <=40 5.88235294117647, \t r > 40 35.294117647058826\n","Test mode, runs: 100, i: 49, j: 9, final: False\n","[task_2_tmax50] 100 run(s) avg rewards : 6.2\n","[task_2_tmax40] 100 run(s) avg rewards : 0.0\n","Point: 3.1\n","Local runtime: 81.11203503608704 seconds --- fast\n","WARNING: do note that this might not reflect the runtime on the server.\n","[Episode 1500]\t rewards globals : 5.383 \tavg rewards : 5.882,\tavg loss: : 0.552177,\tbuffer size : 10000,\tepsilon : 1.1%, \t r <=40 9.803921568627452, \t r > 40 39.21568627450981\n","[Episode 1550]\t rewards globals : 5.461 \tavg rewards : 7.843,\tavg loss: : 0.555929,\tbuffer size : 10000,\tepsilon : 1.1%, \t r <=40 13.725490196078432, \t r > 40 50.98039215686274\n","[Episode 1600]\t rewards globals : 5.453 \tavg rewards : 5.294,\tavg loss: : 0.554381,\tbuffer size : 10000,\tepsilon : 1.1%, \t r <=40 5.88235294117647, \t r > 40 41.17647058823529\n","[Episode 1650]\t rewards globals : 5.475 \tavg rewards : 6.078,\tavg loss: : 0.558338,\tbuffer size : 10000,\tepsilon : 1.1%, \t r <=40 7.8431372549019605, \t r > 40 45.09803921568628\n","[Episode 1700]\t rewards globals : 5.473 \tavg rewards : 5.490,\tavg loss: : 0.558080,\tbuffer size : 10000,\tepsilon : 1.1%, \t r <=40 5.88235294117647, \t r > 40 43.13725490196079\n","[Episode 1750]\t rewards globals : 5.488 \tavg rewards : 6.078,\tavg loss: : 0.561048,\tbuffer size : 10000,\tepsilon : 1.1%, \t r <=40 7.8431372549019605, \t r > 40 45.09803921568628\n","[Episode 1800]\t rewards globals : 5.530 \tavg rewards : 6.863,\tavg loss: : 0.559370,\tbuffer size : 10000,\tepsilon : 1.0%, \t r <=40 7.8431372549019605, \t r > 40 52.94117647058824\n","[Episode 1850]\t rewards globals : 5.538 \tavg rewards : 5.882,\tavg loss: : 0.561215,\tbuffer size : 10000,\tepsilon : 1.0%, \t r <=40 7.8431372549019605, \t r > 40 43.13725490196079\n","[Episode 1900]\t rewards globals : 5.576 \tavg rewards : 7.059,\tavg loss: : 0.558513,\tbuffer size : 10000,\tepsilon : 1.0%, \t r <=40 7.8431372549019605, \t r > 40 54.90196078431373\n","[Episode 1950]\t rewards globals : 5.613 \tavg rewards : 7.059,\tavg loss: : 0.559852,\tbuffer size : 10000,\tepsilon : 1.0%, \t r <=40 3.9215686274509802, \t r > 40 62.745098039215684\n","Test mode, runs: 100, i: 49, j: 9, final: False\n","[task_2_tmax50] 100 run(s) avg rewards : 6.0\n","[task_2_tmax40] 100 run(s) avg rewards : 0.0\n","Point: 3.0\n","Local runtime: 79.12546586990356 seconds --- fast\n","WARNING: do note that this might not reflect the runtime on the server.\n","[Episode 2000]\t rewards globals : 5.622 \tavg rewards : 6.078,\tavg loss: : 0.557019,\tbuffer size : 10000,\tepsilon : 1.0%, \t r <=40 5.88235294117647, \t r > 40 49.01960784313725\n","[Episode 2050]\t rewards globals : 5.670 \tavg rewards : 7.647,\tavg loss: : 0.558439,\tbuffer size : 10000,\tepsilon : 1.0%, \t r <=40 11.76470588235294, \t r > 40 52.94117647058824\n","[Episode 2100]\t rewards globals : 5.693 \tavg rewards : 6.667,\tavg loss: : 0.557015,\tbuffer size : 10000,\tepsilon : 1.0%, \t r <=40 17.647058823529413, \t r > 40 31.372549019607842\n","[Episode 2150]\t rewards globals : 5.695 \tavg rewards : 5.686,\tavg loss: : 0.559567,\tbuffer size : 10000,\tepsilon : 1.0%, \t r <=40 7.8431372549019605, \t r > 40 41.17647058823529\n","[Episode 2200]\t rewards globals : 5.697 \tavg rewards : 5.686,\tavg loss: : 0.557597,\tbuffer size : 10000,\tepsilon : 1.0%, \t r <=40 3.9215686274509802, \t r > 40 49.01960784313725\n","[Episode 2250]\t rewards globals : 5.686 \tavg rewards : 5.294,\tavg loss: : 0.559720,\tbuffer size : 10000,\tepsilon : 1.0%, \t r <=40 3.9215686274509802, \t r > 40 45.09803921568628\n","[Episode 2300]\t rewards globals : 5.706 \tavg rewards : 6.471,\tavg loss: : 0.558115,\tbuffer size : 10000,\tepsilon : 1.0%, \t r <=40 5.88235294117647, \t r > 40 52.94117647058824\n","[Episode 2350]\t rewards globals : 5.738 \tavg rewards : 7.255,\tavg loss: : 0.558797,\tbuffer size : 10000,\tepsilon : 1.0%, \t r <=40 5.88235294117647, \t r > 40 60.78431372549019\n","[Episode 2400]\t rewards globals : 5.718 \tavg rewards : 4.706,\tavg loss: : 0.557735,\tbuffer size : 10000,\tepsilon : 1.0%, \t r <=40 3.9215686274509802, \t r > 40 39.21568627450981\n","[Episode 2450]\t rewards globals : 5.745 \tavg rewards : 6.863,\tavg loss: : 0.559108,\tbuffer size : 10000,\tepsilon : 1.0%, \t r <=40 5.88235294117647, \t r > 40 56.86274509803921\n","environment change : i_1 49 j_1 9\n","Test mode, runs: 100, i: 49, j: 9, final: False\n","[task_2_tmax50] 100 run(s) avg rewards : 6.4\n","[task_2_tmax40] 100 run(s) avg rewards : 0.0\n","Point: 3.2\n","Local runtime: 79.95110702514648 seconds --- fast\n","WARNING: do note that this might not reflect the runtime on the server.\n","[Episode 2500]\t rewards globals : 5.742 \tavg rewards : 5.686,\tavg loss: : 0.558103,\tbuffer size : 10000,\tepsilon : 1.0%, \t r <=40 5.88235294117647, \t r > 40 45.09803921568628\n","[Episode 2550]\t rewards globals : 5.735 \tavg rewards : 5.294,\tavg loss: : 0.560932,\tbuffer size : 10000,\tepsilon : 1.0%, \t r <=40 3.9215686274509802, \t r > 40 45.09803921568628\n","[Episode 2600]\t rewards globals : 5.759 \tavg rewards : 6.863,\tavg loss: : 0.559333,\tbuffer size : 10000,\tepsilon : 1.0%, \t r <=40 11.76470588235294, \t r > 40 45.09803921568628\n","[Episode 2650]\t rewards globals : 5.783 \tavg rewards : 7.255,\tavg loss: : 0.561463,\tbuffer size : 10000,\tepsilon : 1.0%, \t r <=40 7.8431372549019605, \t r > 40 56.86274509803921\n","[Episode 2700]\t rewards globals : 5.794 \tavg rewards : 6.275,\tavg loss: : 0.561337,\tbuffer size : 10000,\tepsilon : 1.0%, \t r <=40 3.9215686274509802, \t r > 40 54.90196078431373\n","[Episode 2750]\t rewards globals : 5.772 \tavg rewards : 4.510,\tavg loss: : 0.563719,\tbuffer size : 10000,\tepsilon : 1.0%, \t r <=40 9.803921568627452, \t r > 40 25.49019607843137\n","[Episode 2800]\t rewards globals : 5.794 \tavg rewards : 6.863,\tavg loss: : 0.562873,\tbuffer size : 10000,\tepsilon : 1.0%, \t r <=40 9.803921568627452, \t r > 40 49.01960784313725\n","[Episode 2850]\t rewards globals : 5.798 \tavg rewards : 6.078,\tavg loss: : 0.564432,\tbuffer size : 10000,\tepsilon : 1.0%, \t r <=40 7.8431372549019605, \t r > 40 45.09803921568628\n","[Episode 2900]\t rewards globals : 5.784 \tavg rewards : 4.902,\tavg loss: : 0.563021,\tbuffer size : 10000,\tepsilon : 1.0%, \t r <=40 9.803921568627452, \t r > 40 29.411764705882355\n","[Episode 2950]\t rewards globals : 5.788 \tavg rewards : 6.078,\tavg loss: : 0.564656,\tbuffer size : 10000,\tepsilon : 1.0%, \t r <=40 19.607843137254903, \t r > 40 21.568627450980394\n","Test mode, runs: 100, i: 49, j: 9, final: False\n","[task_2_tmax50] 100 run(s) avg rewards : 6.6\n","[task_2_tmax40] 100 run(s) avg rewards : 0.7\n","Point: 3.65\n","Local runtime: 76.17748880386353 seconds --- fast\n","WARNING: do note that this might not reflect the runtime on the server.\n","[Episode 3000]\t rewards globals : 5.811 \tavg rewards : 7.059,\tavg loss: : 0.564002,\tbuffer size : 10000,\tepsilon : 1.0%, \t r <=40 23.52941176470588, \t r > 40 23.52941176470588\n","[Episode 3050]\t rewards globals : 5.818 \tavg rewards : 6.275,\tavg loss: : 0.567028,\tbuffer size : 10000,\tepsilon : 1.0%, \t r <=40 11.76470588235294, \t r > 40 39.21568627450981\n","[Episode 3100]\t rewards globals : 5.830 \tavg rewards : 6.471,\tavg loss: : 0.566410,\tbuffer size : 10000,\tepsilon : 1.0%, \t r <=40 13.725490196078432, \t r > 40 37.254901960784316\n","[Episode 3150]\t rewards globals : 5.833 \tavg rewards : 6.078,\tavg loss: : 0.569227,\tbuffer size : 10000,\tepsilon : 1.0%, \t r <=40 11.76470588235294, \t r > 40 37.254901960784316\n","[Episode 3200]\t rewards globals : 5.845 \tavg rewards : 6.667,\tavg loss: : 0.568218,\tbuffer size : 10000,\tepsilon : 1.0%, \t r <=40 11.76470588235294, \t r > 40 43.13725490196079\n","[Episode 3250]\t rewards globals : 5.875 \tavg rewards : 7.843,\tavg loss: : 0.570382,\tbuffer size : 10000,\tepsilon : 1.0%, \t r <=40 21.568627450980394, \t r > 40 35.294117647058826\n","[Episode 3300]\t rewards globals : 5.901 \tavg rewards : 7.647,\tavg loss: : 0.570027,\tbuffer size : 10000,\tepsilon : 1.0%, \t r <=40 23.52941176470588, \t r > 40 29.411764705882355\n","[Episode 3350]\t rewards globals : 5.939 \tavg rewards : 8.235,\tavg loss: : 0.571774,\tbuffer size : 10000,\tepsilon : 1.0%, \t r <=40 25.49019607843137, \t r > 40 31.372549019607842\n","[Episode 3400]\t rewards globals : 5.969 \tavg rewards : 8.039,\tavg loss: : 0.570724,\tbuffer size : 10000,\tepsilon : 1.0%, \t r <=40 31.372549019607842, \t r > 40 17.647058823529413\n","[Episode 3450]\t rewards globals : 5.981 \tavg rewards : 7.059,\tavg loss: : 0.572596,\tbuffer size : 10000,\tepsilon : 1.0%, \t r <=40 21.568627450980394, \t r > 40 27.450980392156865\n","Test mode, runs: 100, i: 49, j: 9, final: False\n","[task_2_tmax50] 100 run(s) avg rewards : 6.2\n","[task_2_tmax40] 100 run(s) avg rewards : 3.1\n","Point: 4.65\n","Local runtime: 73.75953197479248 seconds --- fast\n","WARNING: do note that this might not reflect the runtime on the server.\n","[Episode 3500]\t rewards globals : 6.018 \tavg rewards : 8.824,\tavg loss: : 0.572049,\tbuffer size : 10000,\tepsilon : 1.0%, \t r <=40 37.254901960784316, \t r > 40 13.725490196078432\n","[Episode 3550]\t rewards globals : 6.046 \tavg rewards : 8.235,\tavg loss: : 0.573754,\tbuffer size : 10000,\tepsilon : 1.0%, \t r <=40 27.450980392156865, \t r > 40 27.450980392156865\n","[Episode 3600]\t rewards globals : 6.057 \tavg rewards : 6.863,\tavg loss: : 0.573086,\tbuffer size : 10000,\tepsilon : 1.0%, \t r <=40 25.49019607843137, \t r > 40 17.647058823529413\n","[Episode 3650]\t rewards globals : 6.097 \tavg rewards : 9.216,\tavg loss: : 0.575060,\tbuffer size : 10000,\tepsilon : 1.0%, \t r <=40 41.17647058823529, \t r > 40 9.803921568627452\n","[Episode 3700]\t rewards globals : 6.109 \tavg rewards : 6.863,\tavg loss: : 0.575450,\tbuffer size : 10000,\tepsilon : 1.0%, \t r <=40 27.450980392156865, \t r > 40 13.725490196078432\n","[Episode 3750]\t rewards globals : 6.137 \tavg rewards : 8.235,\tavg loss: : 0.576800,\tbuffer size : 10000,\tepsilon : 1.0%, \t r <=40 27.450980392156865, \t r > 40 27.450980392156865\n","[Episode 3800]\t rewards globals : 6.162 \tavg rewards : 8.235,\tavg loss: : 0.575999,\tbuffer size : 10000,\tepsilon : 1.0%, \t r <=40 33.33333333333333, \t r > 40 15.686274509803921\n","[Episode 3850]\t rewards globals : 6.172 \tavg rewards : 7.255,\tavg loss: : 0.577308,\tbuffer size : 10000,\tepsilon : 1.0%, \t r <=40 29.411764705882355, \t r > 40 13.725490196078432\n","[Episode 3900]\t rewards globals : 6.198 \tavg rewards : 8.039,\tavg loss: : 0.577161,\tbuffer size : 10000,\tepsilon : 1.0%, \t r <=40 35.294117647058826, \t r > 40 9.803921568627452\n","[Episode 3950]\t rewards globals : 6.229 \tavg rewards : 8.431,\tavg loss: : 0.580104,\tbuffer size : 10000,\tepsilon : 1.0%, \t r <=40 31.372549019607842, \t r > 40 21.568627450980394\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qS_dXtwjSEFH","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1RIkGEjdJw83","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":401},"outputId":"5a1e1651-31fd-4784-91fa-0d672a113f88","executionInfo":{"status":"error","timestamp":1587229992901,"user_tz":-480,"elapsed":3492,"user":{"displayName":"N","photoUrl":"","userId":"08619141603155082384"}}},"source":["# TASK 1\n","\n","import time\n","duree = 0 \n","nb_bad_init = 0 \n","# Try for 10 diferent inits\n","\n","for i in range(0,10):\n","  debut = time.time()\n","  # Try an init \n","  !python3 HW3_Task1/agent/dqn.py --train\n","  duree = (time.time()-debut)\n","  # If good init exit loop\n","  if duree > 100:\n","    break"],"execution_count":80,"outputs":[{"output_type":"stream","text":["python3: can't open file 'HW3_Task1/agent/dqn.py': [Errno 2] No such file or directory\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-80-c525fe8926e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mdebut\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;31m# Try an init\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m   \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'python3 HW3_Task1/agent/dqn.py --train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m   \u001b[0mduree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdebut\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0;31m# If good init exit loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'also_return_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_system_commands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_system_compat\u001b[0;34m(shell, cmd, also_return_output)\u001b[0m\n\u001b[1;32m    436\u001b[0m   \u001b[0;31m# stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m   result = _run_command(\n\u001b[0;32m--> 438\u001b[0;31m       shell.var_expand(cmd, depth=2), clear_streamed_output=False)\n\u001b[0m\u001b[1;32m    439\u001b[0m   \u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_exit_code'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_INTERRUPTED_SIGNALS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_run_command\u001b[0;34m(cmd, clear_streamed_output)\u001b[0m\n\u001b[1;32m    193\u001b[0m       \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild_pty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0m_monitor_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_pty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_stdin_widget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0mepoll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_display_stdin_widget\u001b[0;34m(delay_millis)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m   \u001b[0mhide_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'cell_remove_stdin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m   \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocking_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mhide_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_header\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    169\u001b[0m   \u001b[0;31m# unique.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m   \u001b[0mrequest_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msend_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     if (reply.get('type') == 'colab_reply' and\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"ru96YQP2MC35","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"4675ff97-28f6-413e-c82b-1b5a632e8450","executionInfo":{"status":"ok","timestamp":1587381061220,"user_tz":-480,"elapsed":4701,"user":{"displayName":"N","photoUrl":"","userId":"08619141603155082384"}}},"source":[""],"execution_count":1,"outputs":[{"output_type":"stream","text":["cuda\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"luZOdkPXSDQ6","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":895},"outputId":"3966c615-f052-4533-8ffc-8ea73c8eda21","executionInfo":{"status":"ok","timestamp":1587473332044,"user_tz":-480,"elapsed":1688,"user":{"displayName":"N","photoUrl":"","userId":"08619141603155082384"}}},"source":["\n","from HW3_Task2.agent.env import construct_task2_env_0\n","env = construct_task2_env_0()\n","\n","state = env.reset()\n","env.render()\n","\n","import numpy as np\n","def pos_from_state(state):\n","  for i in range(0,10):\n","    column = (np.where(state[1,i,:] == 1.))\n","    if len(column[0]) > 0:\n","      column = column[0][0]\n","      row = i \n","      return(column,row)\n","\n","def state_from_pos(pos_x,pos_y,state):\n","  #Crop to keep a window [49,4]\n","  if pos_y > 7:\n","    y_max = 9\n","    y_min = 5\n","  elif pos_y < 2:\n","    y_max = 4\n","    y_min = 0\n","  else:\n","    y_max = pos_y + 2\n","    y_min = pos_y - 2\n","  \n","  if pos_x > 45:\n","    x_max = 49\n","    x_min = 40\n","  elif pos_x < 5:\n","    x_max = 9\n","    x_min = 0\n","  else:\n","    x_max = pos_x + 4\n","    x_min = pos_x - 5\n","\n","  return(state[:,y_min:(y_max+1),x_min:(x_max+1)])\n","\n","state,_,_,_ = env.step(0)\n","env.render()\n","state,_,_,_ = env.step(1)\n","env.render()\n","state,_,_,_ = env.step(2)\n","env.render()\n","state,_,_,_ = env.step(3)\n","env.render()\n","print(pos_from_state(state))\n","x,y = pos_from_state(state)\n","print(x,y)\n","state2 = state_from_pos(x,y,state)\n","print(state2)"],"execution_count":22,"outputs":[{"output_type":"stream","text":["========================================================================================================================================================================================================\n","  F   O   1   -   -   -   -   5   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   3   -   -   -   -   -   -   -   -   -   -   -   -   -   -   4   -   -   -   -   -   -   -   6   -   2\n","  -   -   -   -   -   -  12   -   -  14   -   -   -   -   -   -   -   -   7   -   -  10   -   -   -   8   9  11   -   -   -   -  13   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -\n","  -   -  17   -   -   -   -   -   -   -   -  19   -   -   -   -  15   -   -   -   -   -   -   -   -   -  20   -   -   -   -   -   -   -  16   -   -   -   -   -   -   -   -   -   -   -   -   -   -  18\n","  -   -   -   -   -   -   -   -   -   -   -   -  21   -   -   -   -   -   -  25   -   -   -   -  24   -   -   -   -   -   -   -   -   -   -   -  26   -  22   -   -   -   -   -  23   -   -   -   -   -\n"," 27   -   -   -   -   -   -   -  33   -   -   -   -   -  32  29   -   -   -   -   -   -  30   -   -   -   -   -   -   -   -   -   -   -  31   -   -   -   -   -   -   -  28   -   -   -   -   -   -   -\n"," 38  35  39   -   -   -   -   -   -   -   -   -   -   -  41   -   -   -   -  36   -   -   -   -   -   -   -  34   -   -   -   -  40   -  37   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -\n"," 43   -   -   -   -   -   -   -   -   -   -   -  42   -   -   -   -   -   -   -   -   -   -   -   -  45   -   -   -   -  44   -   -   -   -  47   -   -   -   -   -   -   -   -   -   -   -   -   -  46\n","  -   -   -   -   -   -   -   -   -   -   -  51   -   -   -  49   -   -   -   -   -   -   -   -   -   -  50   -   -   -   -  48   -   -  52   -   -   -   -   -   -   -   -   -   -   -  53  54   -   -\n","  -   -   -   -   -   -  60   -   -   -   -   -   -  55   -   -  58  59   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -  57   -  56   -   -   -   -   -   -   -\n","  -   -  68  63  64   -   -   -   -   -   -   -   -   -   -   -  65   -   -   -   -   -   -   -   -  62   -   -   -   -   -   -   -   -  61   -   -   -   -   -   -   -   -   -  66  67   -   -   -   <\n","========================================================================================================================================================================================================\n","========================================================================================================================================================================================================\n"," 1F   -   -   -   5   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   3   -   -   -   -   -   -   -   -   -   -   -   -   -   -   4   ~   -   -   -   -   -   -   6   2   ~   -   -   O\n","  -   -  12   ~   -   -   -  14   -   -   -   -   -   -   -   7   -   -   -  10   -   -   8   ~   9  11   -   -   -   -  13   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -\n"," 17   -   -   -   -   -   -   -   -  19   -   -   -   -  15   -   -   -   -   -   -   -   -   -  20   -   -   -   -   -   -   -  16   -   -   -   -   -   -   -   -   -   -   -   -   -   -  18   -   -\n","  -   -   -   -   -   -   -   -  21   ~   ~   -   -   -   -   -  25   ~   -   -  24   ~   -   -   -   -   -   -   -   -   -   -  26   -  22   ~   ~   -   -   -  23   -   -   -   -   -   -   -   -   -\n","  -   -   -   -   -   -  33   -   -   -  32   ~  29   ~   -   -   -   -   -  30   -   -   -   -   -   -   -   -   -   -   -  31   -   -   -   -   -   -   -  28   ~   -   -   -   -   -   -  27   ~   -\n"," 39   -   -   -   -   -   -   -   -   -   -  41   ~   -   -   -  36   ~   -   -   -   -   -   -   -  34   -   -   -  40   -   -  37   -   -   -   -   -   -   -   -   -   -   -   -   -   -  38   ~  35\n","  -   -   -   -   -   -  42   ~   ~   -   -   -   -   -   -   -   -   -   -   -   -  45   ~   -   -   -  44   ~   -   -  47   ~   ~   -   -   -   -   -   -   -   -   -   -   -   -  46  43   ~   -   -\n","  -   -   -   -   -   -   -   -   -  51   -   -   -  49   -   -   -   -   -   -   -   -   -   -  50   -   -   -   -  48   -   -  52   -   -   -   -   -   -   -   -   -   -   -  53  54   -   -   -   -\n","  -   -  60   ~   -   -   -   -   -   -  55   -   -  58  59   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -  57  56   ~   -   -   -   -   -   -   -   -   -   -\n"," 64   ~   -   -   -   -   -   -   -   -   -   -  65   ~   -   -   -   -   -   -   -  62   ~   -   -   -   -   -   -   -  61   ~   -   -   -   -   -   -   -   -  66  67   ~   -   -   -   -   <  68  63\n","========================================================================================================================================================================================================\n","(47, 9)\n","47 9\n","[[[0. 0. 0. 0. 0. 0. 0. 1. 0. 1.]\n","  [0. 0. 0. 0. 0. 1. 1. 0. 0. 0.]\n","  [0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","  [1. 1. 0. 0. 0. 0. 0. 0. 1. 1.]]\n","\n"," [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n","\n"," [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n","\n"," [[0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n","  [0. 0. 0. 0. 0. 1. 1. 1. 0. 0.]\n","  [0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","  [1. 1. 1. 0. 0. 0. 0. 0. 1. 1.]]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cYCvYcoNxryt","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}