{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "numero2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCXRy7hMe_eN",
        "colab_type": "text"
      },
      "source": [
        "## 1. Read Me \n",
        "**Run first cell :** install dependencies \n",
        "\n",
        "**Run second cell :** install files and local environment on the VM, choose which task you want to run by setting task_1 or task_2 value to true\n",
        "\n",
        "**Run third cell :** try diferent init of the agent. When one is found -> the agent is trained  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSJ4SDrrzZcH",
        "colab_type": "code",
        "outputId": "ecc91dd1-7009-4e4a-ffb5-d89dc4f14fed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from HW3_Task2.agent import models \n",
        "from HW3_Task2.agent import agent \n",
        "from HW3_Task2.agent import env as env_builder\n",
        "from google.colab import files\n",
        "from HW3_Task2.agent import state_change\n",
        "import torch\n",
        "import importlib \n",
        "import collections\n",
        "importlib.reload(models)\n",
        "importlib.reload(agent)\n",
        "importlib.reload(env_builder)\n",
        "try:\n",
        "  importlib.reload(models)\n",
        "  importlib.reload(agent)\n",
        "  importlib.reload(env)\n",
        "except:pass\n",
        "Config = collections.namedtuple('Config', ('x_debut', 'y_debut', 'max_episode', 'max_epsilon', 'epsilon_decay', 'test_interval','save_interval','batch_size','buffer_limit','methode','gamma_nstep','nstep','largeur','hauteur','min_buffer','x_fin','y_fin','lane_d','lane_f','speedy','devant','derriere'))\n",
        "Config_loaded = collections.namedtuple('Config', ('dim_space'))\n",
        "config_l=[]\n",
        "last_y = 2\n",
        "Cropping_confing = collections.namedtuple('cropping_config', ('x_debut', 'y_debut', 'hauteur','largeur','devant','derriere'))\n",
        "#time = i + j \n",
        "max_episode = 10000\n",
        "# max_epsilon decrease with time \n",
        "max_epsilon = 1. \n",
        "epsilon_decay =1200\n",
        "test_interval = 500\n",
        "save_interval = 3000\n",
        "batch_size = 64\n",
        "#buffer_limit = min(10000,2000*(time+1))\n",
        "\n",
        "buffer_limit = 8000\n",
        "methode = 'DQN'\n",
        "gamma_nstep  = 0.5\n",
        "nstep = 2\n",
        "largeur = 31\n",
        "hauteur = 3\n",
        "min_buffer = 5000\n",
        "\n",
        "\n",
        "x_debut = 10\n",
        "y_debut = 2\n",
        "x_fin = 0\n",
        "y_fin = 0\n",
        "lane_d = 0\n",
        "lane_f = 3\n",
        "speedy = False\n",
        "devant = 10\n",
        "derriere = 10 \n",
        "dim_space = (4,4,31)\n",
        "config_model = Config_loaded(dim_space)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "cropping_config = Cropping_confing(x_debut,y_debut,hauteur,largeur,devant,derriere)\n",
        "config_cur = Config(x_debut,y_debut,max_episode,max_epsilon,epsilon_decay,test_interval,save_interval,batch_size,buffer_limit,methode,gamma_nstep,nstep,largeur,hauteur,min_buffer,x_fin,y_fin,lane_d,lane_f,speedy,devant,derriere)\n",
        "\n",
        "env = env_builder.construct_task2_env_ij_lanes(x_debut,y_debut,x_fin,y_fin,lane_d,lane_f)\n",
        "env_test = env_builder.construct_task2_env_ij_lanes(x_debut,y_debut,x_fin,y_fin,lane_d,lane_f)\n",
        "env.render()\n",
        "state = env.reset()\n",
        "#state = state_change.state_from_pos(state,cropping=cropping_config)\n",
        "#print(state)\n",
        "model = models.train(agent.Conv1DQN, env=env,pretrain=False,model_p= None,savepath='HW3_Task2/saves/michel_c1_dense_',config=config_cur,cropping=True,env_test=env_test)\n",
        "models.mainTest(model,env=env_test,runs=1000,t1=x_debut,t2=int((4/5*x_debut)),cropping=config_cur)\n",
        "#else:\n",
        "#  model = model''s.train(agent.ConvDQN, env=env,pretrain=True,model_p= model,savepath='HW3_Task2/saves/m_',config=config_cur)\n",
        "\n",
        "x_debut = 20\n",
        "y_debut = 2\n",
        "x_fin = 0\n",
        "y_fin = 0\n",
        "lane_d = 0\n",
        "lane_f = 3\n",
        "speedy = False\n",
        "devant = 20\n",
        "derriere = 10 \n",
        "dim_space = (4,4,31)\n",
        "config_model = Config_loaded(dim_space)\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "cropping_config = Cropping_confing(x_debut,y_debut,hauteur,largeur,devant,derriere)\n",
        "config_cur = Config(x_debut,y_debut,max_episode,max_epsilon,epsilon_decay,test_interval,save_interval,batch_size,buffer_limit,methode,gamma_nstep,nstep,largeur,hauteur,min_buffer,x_fin,y_fin,lane_d,lane_f,speedy,devant,derriere)\n",
        "\n",
        "env = env_builder.construct_task2_env_ij_lanes(x_debut,y_debut,x_fin,y_fin,lane_d,lane_f)\n",
        "env_test = env_builder.construct_task2_env_ij_lanes(x_debut,y_debut,x_fin,y_fin,lane_d,lane_f)\n",
        "env.render()\n",
        "state = env.reset()\n",
        "#state = state_change.state_from_pos(state,cropping=cropping_config)\n",
        "#print(state)\n",
        "model = models.train(agent.Conv1DQN, env=env,pretrain=False,model_p= None,savepath='HW3_Task2/saves/michel_c2_dense_',config=config_cur,cropping=True,env_test=env_test)\n",
        "models.mainTest(model,env=env_test,runs=1000,t1=x_debut,t2=int((4/5*x_debut)),cropping=config_cur)\n",
        "#else:\n",
        "\n",
        "x_debut = 15\n",
        "y_debut = 2\n",
        "x_fin = 0\n",
        "y_fin = 0\n",
        "lane_d = 0\n",
        "lane_f = 3\n",
        "speedy = False\n",
        "devant = 15\n",
        "derriere = 10 \n",
        "dim_space = (4,4,31)\n",
        "config_model = Config_loaded(dim_space)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "cropping_config = Cropping_confing(x_debut,y_debut,hauteur,largeur,devant,derriere)\n",
        "config_cur = Config(x_debut,y_debut,max_episode,max_epsilon,epsilon_decay,test_interval,save_interval,batch_size,buffer_limit,methode,gamma_nstep,nstep,largeur,hauteur,min_buffer,x_fin,y_fin,lane_d,lane_f,speedy,devant,derriere)\n",
        "\n",
        "env = env_builder.construct_task2_env_ij_lanes(x_debut,y_debut,x_fin,y_fin,lane_d,lane_f)\n",
        "env_test = env_builder.construct_task2_env_ij_lanes(x_debut,y_debut,x_fin,y_fin,lane_d,lane_f)\n",
        "env.render()\n",
        "state = env.reset()\n",
        "#state = state_change.state_from_pos(state,cropping=cropping_config)\n",
        "#print(state)\n",
        "model = models.train(agent.Conv1DQN, env=env,pretrain=False,model_p= None,savepath='HW3_Task2/saves/michel_c3_dense_',config=config_cur,cropping=True,env_test=env_test)\n",
        "models.mainTest(model,env=env_test,runs=1000,t1=x_debut,t2=int((4/5*x_debut)),cropping=config_cur)\n",
        "#else:\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "========================================================================================================================================================================================================\n",
            "  F   -   -   -   -   4   1   -   O   -   -   -   -   -   -   5   -   -   -   -   -   -   -   -   3   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   6   -   -   -   2   -   -   -\n",
            "  -  12   -   -  10   -   -   -   7   -   8   -   -  14   -   -   -   -   -   -   -   9   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -  13   -   -   -   -  11   -   -   -   -   -   -   -\n",
            "  -  18   -   -   -   -   -  16   -  20   <   -   -  15   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -  17   -   -   -   -  19\n",
            "========================================================================================================================================================================================================\n",
            "device is: cpu\n",
            "(4, 3, 21)\n",
            "(4, 3, 21)\n",
            "config : \n",
            " x debut 10, y_debut 2, max ep 10000, max epsilon 1.0 \n",
            " Epsilon_decay 1200, test_interval 500\n",
            "Save_interval 3000, batch_size 64, buffer_limit 8000 \n",
            " Methode DQN, gamma_nstep 0.5, nstep 2\n",
            "Dim crop largeur,hauteur 21,3 min buffer 5000 \n",
            " lane_d,lane_f 0,3\n",
            "Start learning from : x_debut 10 y_debut 2\n",
            "Testing: t1,t2 : 10,8\n",
            "device is: cpu\n",
            "t1,t2,runs,tmax: 10,8,50,10\n",
            "[task_2_tmax10] "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
            "  out=out, **kwargs)\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "50 run(s) avg rewards : 0.0;nan,0.0\n",
            "[task_2_tmax8] 50 run(s) avg rewards : 0.0;nan,0.0\n",
            "Point: 0.0\n",
            "[Episode 200]\t rewards globals : 0.846 \tavg rewards : 0.846,\t avg timing 5.9 \tavg loss: : nan,\tbuffer size : 1011,\tepsilon : 84.8%, \t r <=40 0.0, \t r > 40 8.45771144278607\n",
            "avg_reaching_0 0.7 avg timing reached 0 : 3.400\n",
            "[Episode 400]\t rewards globals : 0.823 \tavg rewards : 0.796,\t avg timing 5.7 \tavg loss: : nan,\tbuffer size : 2025,\tepsilon : 71.9%, \t r <=40 0.0, \t r > 40 7.960199004975125\n",
            "avg_reaching_0 0.65 avg timing reached 0 : 4.200\n",
            "Testing: t1,t2 : 10,8\n",
            "device is: cpu\n",
            "t1,t2,runs,tmax: 10,8,50,10\n",
            "[task_2_tmax10] 50 run(s) avg rewards : 0.0;nan,0.0\n",
            "[task_2_tmax8] 50 run(s) avg rewards : 0.0;nan,0.0\n",
            "Point: 0.0\n",
            "[Episode 600]\t rewards globals : 0.666 \tavg rewards : 0.348,\t avg timing 5.9 \tavg loss: : nan,\tbuffer size : 2991,\tepsilon : 61.0%, \t r <=40 0.0, \t r > 40 3.482587064676617\n",
            "avg_reaching_0 0.65 avg timing reached 0 : 3.900\n",
            "[Episode 800]\t rewards globals : 0.599 \tavg rewards : 0.398,\t avg timing 5.7 \tavg loss: : nan,\tbuffer size : 3908,\tepsilon : 51.8%, \t r <=40 0.0, \t r > 40 3.9800995024875623\n",
            "avg_reaching_0 0.2 avg timing reached 0 : 3.600\n",
            "Testing: t1,t2 : 10,8\n",
            "device is: cpu\n",
            "t1,t2,runs,tmax: 10,8,50,10\n",
            "[task_2_tmax10] 50 run(s) avg rewards : 0.0;nan,0.0\n",
            "[task_2_tmax8] 50 run(s) avg rewards : 0.0;nan,0.0\n",
            "Point: 0.0\n",
            "[Episode 1000]\t rewards globals : 0.559 \tavg rewards : 0.448,\t avg timing 5.7 \tavg loss: : nan,\tbuffer size : 4872,\tepsilon : 44.0%, \t r <=40 0.0, \t r > 40 4.477611940298507\n",
            "avg_reaching_0 0.55 avg timing reached 0 : 4.600\n",
            "[Episode 1200]\t rewards globals : 0.649 \tavg rewards : 1.095,\t avg timing 5.9 \tavg loss: : 20.736052,\tbuffer size : 6221,\tepsilon : 37.4%, \t r <=40 0.0, \t r > 40 10.945273631840797\n",
            "avg_reaching_0 0.7 avg timing reached 0 : 3.300\n",
            "[Episode 1400]\t rewards globals : 1.056 \tavg rewards : 3.483,\t avg timing 6.8 \tavg loss: : 12.027771,\tbuffer size : 7454,\tepsilon : 31.8%, \t r <=40 0.0, \t r > 40 34.82587064676617\n",
            "avg_reaching_0 2.6 avg timing reached 0 : 3.900\n",
            "Testing: t1,t2 : 10,8\n",
            "device is: cpu\n",
            "t1,t2,runs,tmax: 10,8,50,10\n",
            "[task_2_tmax10] 50 run(s) avg rewards : 8.0;2.7777777777777777,7.2\n",
            "[task_2_tmax8] 50 run(s) avg rewards : 8.4;2.5161290322580645,6.595744680851064\n",
            "Point: 8.2\n",
            "[Episode 1600]\t rewards globals : 1.562 \tavg rewards : 5.075,\t avg timing 6.9 \tavg loss: : 9.170320,\tbuffer size : 8000,\tepsilon : 27.1%, \t r <=40 0.0, \t r > 40 50.74626865671642\n",
            "avg_reaching_0 5.05 avg timing reached 0 : 3.200\n",
            "[Episode 1800]\t rewards globals : 2.027 \tavg rewards : 5.721,\t avg timing 7.2 \tavg loss: : 8.255266,\tbuffer size : 8000,\tepsilon : 23.1%, \t r <=40 0.0, \t r > 40 57.2139303482587\n",
            "avg_reaching_0 5.75 avg timing reached 0 : 3.300\n",
            "Testing: t1,t2 : 10,8\n",
            "device is: cpu\n",
            "t1,t2,runs,tmax: 10,8,50,10\n",
            "[task_2_tmax10] 50 run(s) avg rewards : 9.0;2.5952380952380953,8.4\n",
            "[task_2_tmax8] 50 run(s) avg rewards : 8.0;2.7948717948717947,7.8\n",
            "Point: 8.5\n",
            "[Episode 2000]\t rewards globals : 2.479 \tavg rewards : 6.567,\t avg timing 7.9 \tavg loss: : 8.046206,\tbuffer size : 8000,\tepsilon : 19.7%, \t r <=40 0.0, \t r > 40 65.67164179104478\n",
            "avg_reaching_0 7.25 avg timing reached 0 : 2.600\n",
            "[Episode 2200]\t rewards globals : 2.862 \tavg rewards : 6.667,\t avg timing 7.8 \tavg loss: : 8.128063,\tbuffer size : 8000,\tepsilon : 16.8%, \t r <=40 0.0, \t r > 40 66.66666666666666\n",
            "avg_reaching_0 7.0 avg timing reached 0 : 3.200\n",
            "[Episode 2400]\t rewards globals : 3.211 \tavg rewards : 7.065,\t avg timing 7.8 \tavg loss: : 8.259098,\tbuffer size : 8000,\tepsilon : 14.4%, \t r <=40 0.0, \t r > 40 70.64676616915423\n",
            "avg_reaching_0 7.15 avg timing reached 0 : 3.300\n",
            "Testing: t1,t2 : 10,8\n",
            "device is: cpu\n",
            "t1,t2,runs,tmax: 10,8,50,10\n",
            "[task_2_tmax10] 50 run(s) avg rewards : 8.6;3.051282051282051,8.125\n",
            "[task_2_tmax8] 50 run(s) avg rewards : 8.4;2.857142857142857,9.545454545454545\n",
            "Point: 8.5\n",
            "[Episode 2600]\t rewards globals : 3.545 \tavg rewards : 7.562,\t avg timing 8.1 \tavg loss: : 8.431052,\tbuffer size : 8000,\tepsilon : 12.3%, \t r <=40 0.0, \t r > 40 75.62189054726367\n",
            "avg_reaching_0 7.35 avg timing reached 0 : 2.400\n",
            "[Episode 2800]\t rewards globals : 3.849 \tavg rewards : 7.811,\t avg timing 8.4 \tavg loss: : 8.660326,\tbuffer size : 8000,\tepsilon : 10.6%, \t r <=40 0.0, \t r > 40 78.1094527363184\n",
            "avg_reaching_0 7.9 avg timing reached 0 : 3.300\n",
            "Testing: t1,t2 : 10,8\n",
            "device is: cpu\n",
            "t1,t2,runs,tmax: 10,8,50,10\n",
            "[task_2_tmax10] 50 run(s) avg rewards : 9.6;3.260869565217391,9.2\n",
            "[task_2_tmax8] 50 run(s) avg rewards : 8.6;3.0,9.347826086956522\n",
            "Point: 9.1\n",
            "[Episode 3000]\t rewards globals : 4.079 \tavg rewards : 7.313,\t avg timing 8.1 \tavg loss: : 8.943827,\tbuffer size : 8000,\tepsilon : 9.1%, \t r <=40 0.0, \t r > 40 73.13432835820896\n",
            "avg_reaching_0 7.5 avg timing reached 0 : 3.700\n",
            "[Episode 3200]\t rewards globals : 4.317 \tavg rewards : 7.910,\t avg timing 8.5 \tavg loss: : 9.203062,\tbuffer size : 8000,\tepsilon : 7.9%, \t r <=40 0.0, \t r > 40 79.1044776119403\n",
            "avg_reaching_0 7.8 avg timing reached 0 : 3.000\n",
            "[Episode 3400]\t rewards globals : 4.525 \tavg rewards : 7.861,\t avg timing 8.4 \tavg loss: : 9.474054,\tbuffer size : 8000,\tepsilon : 6.8%, \t r <=40 0.0, \t r > 40 78.60696517412936\n",
            "avg_reaching_0 7.85 avg timing reached 0 : 3.300\n",
            "Testing: t1,t2 : 10,8\n",
            "device is: cpu\n",
            "t1,t2,runs,tmax: 10,8,50,10\n",
            "[task_2_tmax10] 50 run(s) avg rewards : 8.4;3.2,8.333333333333334\n",
            "[task_2_tmax8] 50 run(s) avg rewards : 8.0;3.225,8.88888888888889\n",
            "Point: 8.2\n",
            "[Episode 3600]\t rewards globals : 4.732 \tavg rewards : 8.259,\t avg timing 8.4 \tavg loss: : 9.738111,\tbuffer size : 8000,\tepsilon : 5.9%, \t r <=40 0.0, \t r > 40 82.58706467661692\n",
            "avg_reaching_0 8.25 avg timing reached 0 : 3.100\n",
            "[Episode 3800]\t rewards globals : 4.867 \tavg rewards : 7.313,\t avg timing 8.0 \tavg loss: : 10.025734,\tbuffer size : 8000,\tepsilon : 5.2%, \t r <=40 0.0, \t r > 40 73.13432835820896\n",
            "avg_reaching_0 7.55 avg timing reached 0 : 2.900\n",
            "Testing: t1,t2 : 10,8\n",
            "device is: cpu\n",
            "t1,t2,runs,tmax: 10,8,50,10\n",
            "[task_2_tmax10] 50 run(s) avg rewards : 9.4;3.574468085106383,9.4\n",
            "[task_2_tmax8] 50 run(s) avg rewards : 8.4;3.1951219512195124,9.11111111111111\n",
            "Point: 8.9\n",
            "[Episode 4000]\t rewards globals : 5.029 \tavg rewards : 8.109,\t avg timing 8.8 \tavg loss: : 10.187601,\tbuffer size : 8000,\tepsilon : 4.5%, \t r <=40 0.0, \t r > 40 81.09452736318407\n",
            "avg_reaching_0 7.75 avg timing reached 0 : 3.300\n",
            "[Episode 4200]\t rewards globals : 5.161 \tavg rewards : 7.811,\t avg timing 8.8 \tavg loss: : 10.314792,\tbuffer size : 8000,\tepsilon : 4.0%, \t r <=40 0.0, \t r > 40 78.1094527363184\n",
            "avg_reaching_0 7.8 avg timing reached 0 : 3.000\n",
            "[Episode 4400]\t rewards globals : 5.301 \tavg rewards : 8.209,\t avg timing 8.5 \tavg loss: : 10.371815,\tbuffer size : 8000,\tepsilon : 3.5%, \t r <=40 0.0, \t r > 40 82.08955223880598\n",
            "avg_reaching_0 8.05 avg timing reached 0 : 2.700\n",
            "Testing: t1,t2 : 10,8\n",
            "device is: cpu\n",
            "t1,t2,runs,tmax: 10,8,50,10\n",
            "[task_2_tmax10] 50 run(s) avg rewards : 9.0;3.8,8.16326530612245\n",
            "[task_2_tmax8] 50 run(s) avg rewards : 8.4;3.2142857142857144,10.0\n",
            "Point: 8.7\n",
            "[Episode 4600]\t rewards globals : 5.418 \tavg rewards : 8.010,\t avg timing 8.9 \tavg loss: : 10.394346,\tbuffer size : 8000,\tepsilon : 3.1%, \t r <=40 0.0, \t r > 40 80.09950248756219\n",
            "avg_reaching_0 8.25 avg timing reached 0 : 3.100\n",
            "[Episode 4800]\t rewards globals : 5.553 \tavg rewards : 8.607,\t avg timing 8.8 \tavg loss: : 10.422203,\tbuffer size : 8000,\tepsilon : 2.8%, \t r <=40 0.0, \t r > 40 86.06965174129353\n",
            "avg_reaching_0 8.45 avg timing reached 0 : 2.200\n",
            "Testing: t1,t2 : 10,8\n",
            "device is: cpu\n",
            "t1,t2,runs,tmax: 10,8,50,10\n",
            "[task_2_tmax10] 50 run(s) avg rewards : 8.4;3.3333333333333335,8.571428571428571\n",
            "[task_2_tmax8] 50 run(s) avg rewards : 8.8;3.227272727272727,9.166666666666666\n",
            "Point: 8.600000000000001\n",
            "[Episode 5000]\t rewards globals : 5.685 \tavg rewards : 8.856,\t avg timing 8.5 \tavg loss: : 10.393936,\tbuffer size : 8000,\tepsilon : 2.5%, \t r <=40 0.0, \t r > 40 88.55721393034825\n",
            "avg_reaching_0 8.7 avg timing reached 0 : 2.900\n",
            "[Episode 5200]\t rewards globals : 5.793 \tavg rewards : 8.507,\t avg timing 8.6 \tavg loss: : 10.329530,\tbuffer size : 8000,\tepsilon : 2.3%, \t r <=40 0.0, \t r > 40 85.07462686567165\n",
            "avg_reaching_0 8.15 avg timing reached 0 : 3.500\n",
            "[Episode 5400]\t rewards globals : 5.906 \tavg rewards : 8.856,\t avg timing 9.0 \tavg loss: : 10.258433,\tbuffer size : 8000,\tepsilon : 2.1%, \t r <=40 0.0, \t r > 40 88.55721393034825\n",
            "avg_reaching_0 8.35 avg timing reached 0 : 3.300\n",
            "Testing: t1,t2 : 10,8\n",
            "device is: cpu\n",
            "t1,t2,runs,tmax: 10,8,50,10\n",
            "[task_2_tmax10] 50 run(s) avg rewards : 9.0;3.15,8.16326530612245\n",
            "[task_2_tmax8] 50 run(s) avg rewards : 8.8;3.5,9.130434782608695\n",
            "Point: 8.9\n",
            "[Episode 5600]\t rewards globals : 5.999 \tavg rewards : 8.458,\t avg timing 8.7 \tavg loss: : 10.145780,\tbuffer size : 8000,\tepsilon : 1.9%, \t r <=40 0.0, \t r > 40 84.5771144278607\n",
            "avg_reaching_0 7.65 avg timing reached 0 : 2.900\n",
            "[Episode 5800]\t rewards globals : 6.099 \tavg rewards : 8.856,\t avg timing 8.8 \tavg loss: : 10.014978,\tbuffer size : 8000,\tepsilon : 1.8%, \t r <=40 0.0, \t r > 40 88.55721393034825\n",
            "avg_reaching_0 8.3 avg timing reached 0 : 3.000\n",
            "Testing: t1,t2 : 10,8\n",
            "device is: cpu\n",
            "t1,t2,runs,tmax: 10,8,50,10\n",
            "[task_2_tmax10] 50 run(s) avg rewards : 9.8;3.2888888888888888,9.0\n",
            "[task_2_tmax8] 50 run(s) avg rewards : 7.8;2.9473684210526314,9.047619047619047\n",
            "Point: 8.8\n",
            "[Episode 6000]\t rewards globals : 6.177 \tavg rewards : 8.458,\t avg timing 8.7 \tavg loss: : 9.889826,\tbuffer size : 8000,\tepsilon : 1.7%, \t r <=40 0.0, \t r > 40 84.5771144278607\n",
            "avg_reaching_0 8.2 avg timing reached 0 : 3.100\n",
            "[Episode 6200]\t rewards globals : 6.264 \tavg rewards : 8.856,\t avg timing 8.7 \tavg loss: : 9.756832,\tbuffer size : 8000,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 88.55721393034825\n",
            "avg_reaching_0 8.4 avg timing reached 0 : 3.800\n",
            "[Episode 6400]\t rewards globals : 6.335 \tavg rewards : 8.557,\t avg timing 8.9 \tavg loss: : 9.630077,\tbuffer size : 8000,\tepsilon : 1.5%, \t r <=40 0.0, \t r > 40 85.57213930348259\n",
            "avg_reaching_0 8.05 avg timing reached 0 : 4.400\n",
            "Testing: t1,t2 : 10,8\n",
            "device is: cpu\n",
            "t1,t2,runs,tmax: 10,8,50,10\n",
            "[task_2_tmax10] 50 run(s) avg rewards : 8.8;3.357142857142857,8.571428571428571\n",
            "[task_2_tmax8] 50 run(s) avg rewards : 8.2;4.0,9.11111111111111\n",
            "Point: 8.5\n",
            "[Episode 6600]\t rewards globals : 6.411 \tavg rewards : 8.856,\t avg timing 8.8 \tavg loss: : 9.507890,\tbuffer size : 8000,\tepsilon : 1.4%, \t r <=40 0.0, \t r > 40 88.55721393034825\n",
            "avg_reaching_0 8.55 avg timing reached 0 : 4.300\n",
            "[Episode 6800]\t rewards globals : 6.473 \tavg rewards : 8.458,\t avg timing 9.0 \tavg loss: : 9.403627,\tbuffer size : 8000,\tepsilon : 1.3%, \t r <=40 0.0, \t r > 40 84.5771144278607\n",
            "avg_reaching_0 8.35 avg timing reached 0 : 3.500\n",
            "Testing: t1,t2 : 10,8\n",
            "device is: cpu\n",
            "t1,t2,runs,tmax: 10,8,50,10\n",
            "[task_2_tmax10] 50 run(s) avg rewards : 9.6;3.6666666666666665,9.375\n",
            "[task_2_tmax8] 50 run(s) avg rewards : 8.6;3.7,8.51063829787234\n",
            "Point: 9.1\n",
            "[Episode 7000]\t rewards globals : 6.535 \tavg rewards : 8.657,\t avg timing 9.0 \tavg loss: : 9.310544,\tbuffer size : 8000,\tepsilon : 1.3%, \t r <=40 0.0, \t r > 40 86.56716417910447\n",
            "avg_reaching_0 8.1 avg timing reached 0 : 4.300\n",
            "[Episode 7200]\t rewards globals : 6.594 \tavg rewards : 8.607,\t avg timing 9.0 \tavg loss: : 9.216539,\tbuffer size : 8000,\tepsilon : 1.2%, \t r <=40 0.0, \t r > 40 86.06965174129353\n",
            "avg_reaching_0 7.9 avg timing reached 0 : 3.700\n",
            "[Episode 7400]\t rewards globals : 6.652 \tavg rewards : 8.756,\t avg timing 8.6 \tavg loss: : 9.146900,\tbuffer size : 8000,\tepsilon : 1.2%, \t r <=40 0.0, \t r > 40 87.56218905472637\n",
            "avg_reaching_0 8.35 avg timing reached 0 : 3.300\n",
            "Testing: t1,t2 : 10,8\n",
            "device is: cpu\n",
            "t1,t2,runs,tmax: 10,8,50,10\n",
            "[task_2_tmax10] 50 run(s) avg rewards : 8.4;3.7,8.16326530612245\n",
            "[task_2_tmax8] 50 run(s) avg rewards : 8.4;3.5,8.695652173913043\n",
            "Point: 8.4\n",
            "[Episode 7600]\t rewards globals : 6.708 \tavg rewards : 8.806,\t avg timing 8.6 \tavg loss: : 9.083427,\tbuffer size : 8000,\tepsilon : 1.2%, \t r <=40 0.0, \t r > 40 88.05970149253731\n",
            "avg_reaching_0 8.3 avg timing reached 0 : 2.700\n",
            "[Episode 7800]\t rewards globals : 6.772 \tavg rewards : 9.204,\t avg timing 9.0 \tavg loss: : 9.026083,\tbuffer size : 8000,\tepsilon : 1.1%, \t r <=40 0.0, \t r > 40 92.03980099502488\n",
            "avg_reaching_0 8.8 avg timing reached 0 : 3.400\n",
            "Testing: t1,t2 : 10,8\n",
            "device is: cpu\n",
            "t1,t2,runs,tmax: 10,8,50,10\n",
            "[task_2_tmax10] 50 run(s) avg rewards : 9.4;3.7333333333333334,9.0\n",
            "[task_2_tmax8] 50 run(s) avg rewards : 8.8;3.227272727272727,9.166666666666666\n",
            "Point: 9.100000000000001\n",
            "[Episode 8000]\t rewards globals : 6.812 \tavg rewards : 8.358,\t avg timing 9.0 \tavg loss: : 8.982141,\tbuffer size : 8000,\tepsilon : 1.1%, \t r <=40 0.0, \t r > 40 83.5820895522388\n",
            "avg_reaching_0 8.2 avg timing reached 0 : 2.900\n",
            "[Episode 8200]\t rewards globals : 6.871 \tavg rewards : 9.254,\t avg timing 9.0 \tavg loss: : 8.931517,\tbuffer size : 8000,\tepsilon : 1.1%, \t r <=40 0.0, \t r > 40 92.53731343283582\n",
            "avg_reaching_0 8.85 avg timing reached 0 : 3.500\n",
            "[Episode 8400]\t rewards globals : 6.906 \tavg rewards : 8.358,\t avg timing 9.0 \tavg loss: : 8.871776,\tbuffer size : 8000,\tepsilon : 1.1%, \t r <=40 0.0, \t r > 40 83.5820895522388\n",
            "avg_reaching_0 8.05 avg timing reached 0 : 3.300\n",
            "Testing: t1,t2 : 10,8\n",
            "device is: cpu\n",
            "t1,t2,runs,tmax: 10,8,50,10\n",
            "[task_2_tmax10] 50 run(s) avg rewards : 8.8;3.6136363636363638,8.8\n",
            "[task_2_tmax8] 50 run(s) avg rewards : 7.8;3.6153846153846154,8.666666666666666\n",
            "Point: 8.3\n",
            "[Episode 8600]\t rewards globals : 6.934 \tavg rewards : 8.109,\t avg timing 8.6 \tavg loss: : 8.842591,\tbuffer size : 8000,\tepsilon : 1.1%, \t r <=40 0.0, \t r > 40 81.09452736318407\n",
            "avg_reaching_0 7.7 avg timing reached 0 : 3.200\n",
            "[Episode 8800]\t rewards globals : 6.979 \tavg rewards : 8.856,\t avg timing 9.0 \tavg loss: : 8.820825,\tbuffer size : 8000,\tepsilon : 1.1%, \t r <=40 0.0, \t r > 40 88.55721393034825\n",
            "avg_reaching_0 8.25 avg timing reached 0 : 3.100\n",
            "Testing: t1,t2 : 10,8\n",
            "device is: cpu\n",
            "t1,t2,runs,tmax: 10,8,50,10\n",
            "[task_2_tmax10] 50 run(s) avg rewards : 8.2;3.4444444444444446,7.346938775510204\n",
            "[task_2_tmax8] 50 run(s) avg rewards : 9.2;2.9555555555555557,9.574468085106384\n",
            "Point: 8.7\n",
            "[Episode 9000]\t rewards globals : 7.009 \tavg rewards : 8.358,\t avg timing 9.0 \tavg loss: : 8.811924,\tbuffer size : 8000,\tepsilon : 1.1%, \t r <=40 0.0, \t r > 40 83.5820895522388\n",
            "avg_reaching_0 8.1 avg timing reached 0 : 3.300\n",
            "[Episode 9200]\t rewards globals : 7.044 \tavg rewards : 8.607,\t avg timing 8.7 \tavg loss: : 8.800358,\tbuffer size : 8000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 86.06965174129353\n",
            "avg_reaching_0 7.85 avg timing reached 0 : 3.900\n",
            "[Episode 9400]\t rewards globals : 7.076 \tavg rewards : 8.557,\t avg timing 8.8 \tavg loss: : 8.788867,\tbuffer size : 8000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 85.57213930348259\n",
            "avg_reaching_0 8.25 avg timing reached 0 : 3.200\n",
            "Testing: t1,t2 : 10,8\n",
            "device is: cpu\n",
            "t1,t2,runs,tmax: 10,8,50,10\n",
            "[task_2_tmax10] 50 run(s) avg rewards : 9.0;3.5853658536585367,8.72340425531915\n",
            "[task_2_tmax8] 50 run(s) avg rewards : 9.2;3.2888888888888888,9.183673469387756\n",
            "Point: 9.1\n",
            "[Episode 9600]\t rewards globals : 7.102 \tavg rewards : 8.358,\t avg timing 8.8 \tavg loss: : 8.769753,\tbuffer size : 8000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 83.5820895522388\n",
            "avg_reaching_0 8.25 avg timing reached 0 : 3.400\n",
            "[Episode 9800]\t rewards globals : 7.131 \tavg rewards : 8.507,\t avg timing 8.8 \tavg loss: : 8.739451,\tbuffer size : 8000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 85.07462686567165\n",
            "avg_reaching_0 8.45 avg timing reached 0 : 3.900\n",
            "Testing: t1,t2 : 10,8\n",
            "device is: cpu\n",
            "t1,t2,runs,tmax: 10,8,50,10\n",
            "[task_2_tmax10] 50 run(s) avg rewards : 8.8;3.707317073170732,8.72340425531915\n",
            "[task_2_tmax8] 50 run(s) avg rewards : 8.8;3.4761904761904763,9.333333333333334\n",
            "Point: 8.8\n",
            "device is: cpu\n",
            "t1,t2,runs,tmax: 10,8,1000,10\n",
            "[task_2_tmax10] 1000 run(s) avg rewards : 8.9;3.838709677419355,8.575819672131148\n",
            "[task_2_tmax8] 1000 run(s) avg rewards : 8.4;3.772121212121212,9.187082405345212\n",
            "Point: 8.665\n",
            "========================================================================================================================================================================================================\n",
            "  F   -   -   -   -   -   6   -   -   -   -   -   4   -   -   -   3   -   -   -   O   -   -   -   -   -   2   -   -   -   -   -   -   -   5   -   -   -   -   -   -   -   -   -   -   -   -   -   1   -\n",
            "  -   -   -   -   -   -   -   -   -   9   -   -   -  14   -   -   -   -   -   -   -  13   -   8   -   -   -   -   -   -   -  11  12   -   -  10   -   -   -   -   7   -   -   -   -   -   -   -   -   -\n",
            "  -   -   -   -   -   -   -   -   -   -   -   -   -   -   -  19   -   -  16   -   <   -   -   -   -   -   -   -   -   -   -  17  15   -   -  18   -   -   -   -   -   -   -   -   -   -   -   -   -  20\n",
            "========================================================================================================================================================================================================\n",
            "device is: cpu\n",
            "(4, 3, 31)\n",
            "(4, 3, 31)\n",
            "config : \n",
            " x debut 20, y_debut 2, max ep 10000, max epsilon 1.0 \n",
            " Epsilon_decay 1200, test_interval 500\n",
            "Save_interval 3000, batch_size 64, buffer_limit 8000 \n",
            " Methode DQN, gamma_nstep 0.5, nstep 2\n",
            "Dim crop largeur,hauteur 31,3 min buffer 5000 \n",
            " lane_d,lane_f 0,3\n",
            "Start learning from : x_debut 20 y_debut 2\n",
            "Testing: t1,t2 : 20,16\n",
            "device is: cpu\n",
            "t1,t2,runs,tmax: 20,16,50,20\n",
            "[task_2_tmax20] 50 run(s) avg rewards : 4.4;2.727272727272727,4.4\n",
            "[task_2_tmax16] 50 run(s) avg rewards : 5.0;2.76,5.0\n",
            "Point: 4.7\n",
            "[Episode 200]\t rewards globals : 1.194 \tavg rewards : 1.194,\t avg timing 11.6 \tavg loss: : nan,\tbuffer size : 1324,\tepsilon : 84.8%, \t r <=40 0.0, \t r > 40 11.940298507462686\n",
            "avg_reaching_0 1.95 avg timing reached 0 : 7.300\n",
            "[Episode 400]\t rewards globals : 1.646 \tavg rewards : 2.090,\t avg timing 12.4 \tavg loss: : nan,\tbuffer size : 2827,\tepsilon : 71.9%, \t r <=40 0.0, \t r > 40 20.8955223880597\n",
            "avg_reaching_0 3.6 avg timing reached 0 : 5.000\n",
            "Testing: t1,t2 : 20,16\n",
            "device is: cpu\n",
            "t1,t2,runs,tmax: 20,16,50,20\n",
            "[task_2_tmax20] 50 run(s) avg rewards : 4.8;2.625,4.8\n",
            "[task_2_tmax16] 50 run(s) avg rewards : 3.8;2.789473684210526,3.8\n",
            "Point: 4.3\n",
            "[Episode 600]\t rewards globals : 1.864 \tavg rewards : 2.289,\t avg timing 13.1 \tavg loss: : nan,\tbuffer size : 4348,\tepsilon : 61.0%, \t r <=40 0.0, \t r > 40 22.885572139303484\n",
            "avg_reaching_0 4.2 avg timing reached 0 : 5.100\n",
            "[Episode 800]\t rewards globals : 1.823 \tavg rewards : 1.741,\t avg timing 13.2 \tavg loss: : 33.631895,\tbuffer size : 6019,\tepsilon : 51.8%, \t r <=40 0.0, \t r > 40 17.412935323383085\n",
            "avg_reaching_0 3.05 avg timing reached 0 : 4.800\n",
            "Testing: t1,t2 : 20,16\n",
            "device is: cpu\n",
            "t1,t2,runs,tmax: 20,16,50,20\n",
            "[task_2_tmax20] 50 run(s) avg rewards : 8.6;5.738095238095238,8.4\n",
            "[task_2_tmax16] 50 run(s) avg rewards : 7.2;4.117647058823529,7.083333333333333\n",
            "Point: 7.9\n",
            "[Episode 1000]\t rewards globals : 2.088 \tavg rewards : 3.134,\t avg timing 13.2 \tavg loss: : 19.025218,\tbuffer size : 7988,\tepsilon : 44.0%, \t r <=40 0.0, \t r > 40 31.343283582089555\n",
            "avg_reaching_0 4.95 avg timing reached 0 : 3.800\n",
            "[Episode 1200]\t rewards globals : 2.398 \tavg rewards : 3.930,\t avg timing 13.3 \tavg loss: : 14.376977,\tbuffer size : 8000,\tepsilon : 37.4%, \t r <=40 0.0, \t r > 40 39.30348258706468\n",
            "avg_reaching_0 6.0 avg timing reached 0 : 5.200\n",
            "[Episode 1400]\t rewards globals : 2.684 \tavg rewards : 4.378,\t avg timing 14.5 \tavg loss: : 12.957406,\tbuffer size : 8000,\tepsilon : 31.8%, \t r <=40 0.0, \t r > 40 43.78109452736319\n",
            "avg_reaching_0 6.95 avg timing reached 0 : 3.800\n",
            "Testing: t1,t2 : 20,16\n",
            "device is: cpu\n",
            "t1,t2,runs,tmax: 20,16,50,20\n",
            "[task_2_tmax20] 50 run(s) avg rewards : 8.6;3.1627906976744184,8.6\n",
            "[task_2_tmax16] 50 run(s) avg rewards : 8.4;3.4761904761904763,8.4\n",
            "Point: 8.5\n",
            "[Episode 1600]\t rewards globals : 2.929 \tavg rewards : 4.677,\t avg timing 14.6 \tavg loss: : 12.728982,\tbuffer size : 8000,\tepsilon : 27.1%, \t r <=40 0.0, \t r > 40 46.766169154228855\n",
            "avg_reaching_0 6.9 avg timing reached 0 : 3.900\n",
            "[Episode 1800]\t rewards globals : 3.182 \tavg rewards : 5.224,\t avg timing 15.0 \tavg loss: : 12.825715,\tbuffer size : 8000,\tepsilon : 23.1%, \t r <=40 0.0, \t r > 40 52.23880597014925\n",
            "avg_reaching_0 7.45 avg timing reached 0 : 3.200\n",
            "Testing: t1,t2 : 20,16\n",
            "device is: cpu\n",
            "t1,t2,runs,tmax: 20,16,50,20\n",
            "[task_2_tmax20] 50 run(s) avg rewards : 8.6;3.5348837209302326,8.6\n",
            "[task_2_tmax16] 50 run(s) avg rewards : 8.6;3.7209302325581395,8.6\n",
            "Point: 8.6\n",
            "[Episode 2000]\t rewards globals : 3.388 \tavg rewards : 5.274,\t avg timing 14.7 \tavg loss: : 13.307242,\tbuffer size : 8000,\tepsilon : 19.7%, \t r <=40 0.0, \t r > 40 52.736318407960205\n",
            "avg_reaching_0 7.5 avg timing reached 0 : 3.000\n",
            "[Episode 2200]\t rewards globals : 3.639 \tavg rewards : 6.119,\t avg timing 15.6 \tavg loss: : 14.299871,\tbuffer size : 8000,\tepsilon : 16.8%, \t r <=40 0.0, \t r > 40 61.19402985074627\n",
            "avg_reaching_0 7.95 avg timing reached 0 : 4.200\n",
            "[Episode 2400]\t rewards globals : 3.928 \tavg rewards : 7.114,\t avg timing 16.6 \tavg loss: : 15.236143,\tbuffer size : 8000,\tepsilon : 14.4%, \t r <=40 0.0, \t r > 40 71.14427860696517\n",
            "avg_reaching_0 8.4 avg timing reached 0 : 2.400\n",
            "Testing: t1,t2 : 20,16\n",
            "device is: cpu\n",
            "t1,t2,runs,tmax: 20,16,50,20\n",
            "[task_2_tmax20] 50 run(s) avg rewards : 8.6;3.372093023255814,8.6\n",
            "[task_2_tmax16] 50 run(s) avg rewards : 9.4;3.4893617021276597,9.591836734693878\n",
            "Point: 9.0\n",
            "[Episode 2600]\t rewards globals : 4.118 \tavg rewards : 6.418,\t avg timing 15.6 \tavg loss: : 16.396518,\tbuffer size : 8000,\tepsilon : 12.3%, \t r <=40 0.0, \t r > 40 64.17910447761194\n",
            "avg_reaching_0 8.25 avg timing reached 0 : 4.000\n",
            "[Episode 2800]\t rewards globals : 4.284 \tavg rewards : 6.468,\t avg timing 16.7 \tavg loss: : 17.481755,\tbuffer size : 8000,\tepsilon : 10.6%, \t r <=40 0.0, \t r > 40 64.6766169154229\n",
            "avg_reaching_0 8.0 avg timing reached 0 : 4.400\n",
            "Testing: t1,t2 : 20,16\n",
            "device is: cpu\n",
            "t1,t2,runs,tmax: 20,16,50,20\n",
            "[task_2_tmax20] 50 run(s) avg rewards : 9.0;4.363636363636363,8.8\n",
            "[task_2_tmax16] 50 run(s) avg rewards : 9.0;3.511111111111111,9.183673469387756\n",
            "Point: 9.0\n",
            "[Episode 3000]\t rewards globals : 4.499 \tavg rewards : 7.512,\t avg timing 16.9 \tavg loss: : 18.888037,\tbuffer size : 8000,\tepsilon : 9.1%, \t r <=40 0.0, \t r > 40 75.12437810945273\n",
            "avg_reaching_0 8.55 avg timing reached 0 : 4.100\n",
            "[Episode 3200]\t rewards globals : 4.699 \tavg rewards : 7.711,\t avg timing 17.2 \tavg loss: : 20.409493,\tbuffer size : 8000,\tepsilon : 7.9%, \t r <=40 0.0, \t r > 40 77.11442786069652\n",
            "avg_reaching_0 8.85 avg timing reached 0 : 3.200\n",
            "[Episode 3400]\t rewards globals : 4.872 \tavg rewards : 7.612,\t avg timing 16.7 \tavg loss: : 21.930057,\tbuffer size : 8000,\tepsilon : 6.8%, \t r <=40 0.0, \t r > 40 76.11940298507463\n",
            "avg_reaching_0 8.55 avg timing reached 0 : 6.500\n",
            "Testing: t1,t2 : 20,16\n",
            "device is: cpu\n",
            "t1,t2,runs,tmax: 20,16,50,20\n",
            "[task_2_tmax20] 50 run(s) avg rewards : 9.2;3.4130434782608696,9.2\n",
            "[task_2_tmax16] 50 run(s) avg rewards : 9.4;3.595744680851064,9.591836734693878\n",
            "Point: 9.3\n",
            "[Episode 3600]\t rewards globals : 5.060 \tavg rewards : 8.259,\t avg timing 17.2 \tavg loss: : 23.316498,\tbuffer size : 8000,\tepsilon : 5.9%, \t r <=40 0.0, \t r > 40 82.58706467661692\n",
            "avg_reaching_0 8.85 avg timing reached 0 : 3.400\n",
            "[Episode 3800]\t rewards globals : 5.230 \tavg rewards : 8.308,\t avg timing 17.5 \tavg loss: : 24.655801,\tbuffer size : 8000,\tepsilon : 5.2%, \t r <=40 0.0, \t r > 40 83.08457711442786\n",
            "avg_reaching_0 9.15 avg timing reached 0 : 4.000\n",
            "Testing: t1,t2 : 20,16\n",
            "device is: cpu\n",
            "t1,t2,runs,tmax: 20,16,50,20\n",
            "[task_2_tmax20] 50 run(s) avg rewards : 9.4;4.191489361702128,9.591836734693878\n",
            "[task_2_tmax16] 50 run(s) avg rewards : 9.2;4.543478260869565,9.2\n",
            "Point: 9.3\n",
            "[Episode 4000]\t rewards globals : 5.381 \tavg rewards : 8.259,\t avg timing 17.5 \tavg loss: : 25.708900,\tbuffer size : 8000,\tepsilon : 4.5%, \t r <=40 0.0, \t r > 40 82.58706467661692\n",
            "avg_reaching_0 8.7 avg timing reached 0 : 3.700\n",
            "[Episode 4200]\t rewards globals : 5.532 \tavg rewards : 8.557,\t avg timing 16.7 \tavg loss: : 26.851940,\tbuffer size : 8000,\tepsilon : 4.0%, \t r <=40 0.0, \t r > 40 85.57213930348259\n",
            "avg_reaching_0 9.25 avg timing reached 0 : 2.300\n",
            "[Episode 4400]\t rewards globals : 5.674 \tavg rewards : 8.657,\t avg timing 17.8 \tavg loss: : 27.986403,\tbuffer size : 8000,\tepsilon : 3.5%, \t r <=40 0.0, \t r > 40 86.56716417910447\n",
            "avg_reaching_0 8.95 avg timing reached 0 : 2.900\n",
            "Testing: t1,t2 : 20,16\n",
            "device is: cpu\n",
            "t1,t2,runs,tmax: 20,16,50,20\n",
            "[task_2_tmax20] 50 run(s) avg rewards : 9.2;3.5434782608695654,9.2\n",
            "[task_2_tmax16] 50 run(s) avg rewards : 9.4;3.8260869565217392,9.387755102040817\n",
            "Point: 9.3\n",
            "[Episode 4600]\t rewards globals : 5.777 \tavg rewards : 8.060,\t avg timing 17.2 \tavg loss: : 29.223594,\tbuffer size : 8000,\tepsilon : 3.1%, \t r <=40 0.0, \t r > 40 80.59701492537313\n",
            "avg_reaching_0 8.7 avg timing reached 0 : 4.000\n",
            "[Episode 4800]\t rewards globals : 5.905 \tavg rewards : 8.806,\t avg timing 17.8 \tavg loss: : 30.312353,\tbuffer size : 8000,\tepsilon : 2.8%, \t r <=40 0.0, \t r > 40 88.05970149253731\n",
            "avg_reaching_0 9.1 avg timing reached 0 : 5.800\n",
            "Testing: t1,t2 : 20,16\n",
            "device is: cpu\n",
            "t1,t2,runs,tmax: 20,16,50,20\n",
            "[task_2_tmax20] 50 run(s) avg rewards : 9.2;4.521739130434782,9.2\n",
            "[task_2_tmax16] 50 run(s) avg rewards : 9.2;3.391304347826087,9.2\n",
            "Point: 9.2\n",
            "[Episode 5000]\t rewards globals : 6.033 \tavg rewards : 9.104,\t avg timing 18.5 \tavg loss: : 31.240358,\tbuffer size : 8000,\tepsilon : 2.5%, \t r <=40 0.0, \t r > 40 91.04477611940298\n",
            "avg_reaching_0 9.3 avg timing reached 0 : 3.700\n",
            "[Episode 5200]\t rewards globals : 6.135 \tavg rewards : 8.706,\t avg timing 17.9 \tavg loss: : 32.076738,\tbuffer size : 8000,\tepsilon : 2.3%, \t r <=40 0.0, \t r > 40 87.06467661691542\n",
            "avg_reaching_0 9.25 avg timing reached 0 : 4.100\n",
            "[Episode 5400]\t rewards globals : 6.232 \tavg rewards : 8.706,\t avg timing 18.3 \tavg loss: : 32.885615,\tbuffer size : 8000,\tepsilon : 2.1%, \t r <=40 0.0, \t r > 40 87.06467661691542\n",
            "avg_reaching_0 9.0 avg timing reached 0 : 5.000\n",
            "Testing: t1,t2 : 20,16\n",
            "device is: cpu\n",
            "t1,t2,runs,tmax: 20,16,50,20\n",
            "[task_2_tmax20] 50 run(s) avg rewards : 9.8;4.959183673469388,9.8\n",
            "[task_2_tmax16] 50 run(s) avg rewards : 9.2;3.891304347826087,9.583333333333334\n",
            "Point: 9.5\n",
            "Double check Testing: t1,t2 : 20,16\n",
            "device is: cpu\n",
            "t1,t2,runs,tmax: 20,16,300,20\n",
            "[task_2_tmax20] 300 run(s) avg rewards : 9.2;4.425992779783393,9.233333333333333\n",
            "[task_2_tmax16] 300 run(s) avg rewards : 9.4;4.149466192170818,9.525423728813559\n",
            "Point: 9.316666666666666\n",
            "[Episode 5600]\t rewards globals : 6.315 \tavg rewards : 8.557,\t avg timing 18.5 \tavg loss: : 33.690982,\tbuffer size : 8000,\tepsilon : 1.9%, \t r <=40 0.0, \t r > 40 85.57213930348259\n",
            "avg_reaching_0 9.15 avg timing reached 0 : 4.000\n",
            "[Episode 5800]\t rewards globals : 6.404 \tavg rewards : 8.905,\t avg timing 18.5 \tavg loss: : 34.410173,\tbuffer size : 8000,\tepsilon : 1.8%, \t r <=40 0.0, \t r > 40 89.05472636815921\n",
            "avg_reaching_0 9.2 avg timing reached 0 : 3.500\n",
            "Testing: t1,t2 : 20,16\n",
            "device is: cpu\n",
            "t1,t2,runs,tmax: 20,16,50,20\n",
            "[task_2_tmax20] 50 run(s) avg rewards : 8.8;5.409090909090909,8.8\n",
            "[task_2_tmax16] 50 run(s) avg rewards : 9.4;4.297872340425532,9.4\n",
            "Point: 9.100000000000001\n",
            "[Episode 6000]\t rewards globals : 6.486 \tavg rewards : 8.806,\t avg timing 18.3 \tavg loss: : 35.177703,\tbuffer size : 8000,\tepsilon : 1.7%, \t r <=40 0.0, \t r > 40 88.05970149253731\n",
            "avg_reaching_0 8.95 avg timing reached 0 : 3.900\n",
            "[Episode 6200]\t rewards globals : 6.570 \tavg rewards : 9.055,\t avg timing 18.3 \tavg loss: : 35.934395,\tbuffer size : 8000,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 90.54726368159204\n",
            "avg_reaching_0 9.15 avg timing reached 0 : 6.000\n",
            "[Episode 6400]\t rewards globals : 6.644 \tavg rewards : 8.955,\t avg timing 18.4 \tavg loss: : 36.677738,\tbuffer size : 8000,\tepsilon : 1.5%, \t r <=40 0.0, \t r > 40 89.55223880597015\n",
            "avg_reaching_0 9.15 avg timing reached 0 : 5.300\n",
            "Testing: t1,t2 : 20,16\n",
            "device is: cpu\n",
            "t1,t2,runs,tmax: 20,16,50,20\n",
            "[task_2_tmax20] 50 run(s) avg rewards : 9.4;5.0638297872340425,9.4\n",
            "[task_2_tmax16] 50 run(s) avg rewards : 9.4;4.787234042553192,9.591836734693878\n",
            "Point: 9.4\n",
            "[Episode 6600]\t rewards globals : 6.705 \tavg rewards : 8.657,\t avg timing 18.1 \tavg loss: : 37.541642,\tbuffer size : 8000,\tepsilon : 1.4%, \t r <=40 0.0, \t r > 40 86.56716417910447\n",
            "avg_reaching_0 8.9 avg timing reached 0 : 3.800\n",
            "[Episode 6800]\t rewards globals : 6.770 \tavg rewards : 8.905,\t avg timing 18.8 \tavg loss: : 38.301164,\tbuffer size : 8000,\tepsilon : 1.3%, \t r <=40 0.0, \t r > 40 89.05472636815921\n",
            "avg_reaching_0 9.1 avg timing reached 0 : 5.800\n",
            "Testing: t1,t2 : 20,16\n",
            "device is: cpu\n",
            "t1,t2,runs,tmax: 20,16,50,20\n",
            "[task_2_tmax20] 50 run(s) avg rewards : 9.2;4.130434782608695,9.2\n",
            "[task_2_tmax16] 50 run(s) avg rewards : 8.0;4.55,8.0\n",
            "Point: 8.6\n",
            "[Episode 7000]\t rewards globals : 6.838 \tavg rewards : 9.154,\t avg timing 18.8 \tavg loss: : 39.026723,\tbuffer size : 8000,\tepsilon : 1.3%, \t r <=40 0.0, \t r > 40 91.54228855721394\n",
            "avg_reaching_0 9.3 avg timing reached 0 : 5.500\n",
            "[Episode 7200]\t rewards globals : 6.903 \tavg rewards : 9.204,\t avg timing 18.5 \tavg loss: : 39.707673,\tbuffer size : 8000,\tepsilon : 1.2%, \t r <=40 0.0, \t r > 40 92.03980099502488\n",
            "avg_reaching_0 9.35 avg timing reached 0 : 6.200\n",
            "[Episode 7400]\t rewards globals : 6.965 \tavg rewards : 9.204,\t avg timing 18.7 \tavg loss: : 40.313085,\tbuffer size : 8000,\tepsilon : 1.2%, \t r <=40 0.0, \t r > 40 92.03980099502488\n",
            "avg_reaching_0 9.3 avg timing reached 0 : 2.800\n",
            "Testing: t1,t2 : 20,16\n",
            "device is: cpu\n",
            "t1,t2,runs,tmax: 20,16,50,20\n",
            "[task_2_tmax20] 50 run(s) avg rewards : 9.6;4.479166666666667,9.795918367346939\n",
            "[task_2_tmax16] 50 run(s) avg rewards : 8.6;3.395348837209302,8.958333333333334\n",
            "Point: 9.1\n",
            "[Episode 7600]\t rewards globals : 7.028 \tavg rewards : 9.353,\t avg timing 18.8 \tavg loss: : 40.777737,\tbuffer size : 8000,\tepsilon : 1.2%, \t r <=40 0.0, \t r > 40 93.53233830845771\n",
            "avg_reaching_0 9.6 avg timing reached 0 : 2.900\n",
            "[Episode 7800]\t rewards globals : 7.067 \tavg rewards : 8.557,\t avg timing 18.4 \tavg loss: : 41.289834,\tbuffer size : 8000,\tepsilon : 1.1%, \t r <=40 0.0, \t r > 40 85.57213930348259\n",
            "avg_reaching_0 8.9 avg timing reached 0 : 2.700\n",
            "Testing: t1,t2 : 20,16\n",
            "device is: cpu\n",
            "t1,t2,runs,tmax: 20,16,50,20\n",
            "[task_2_tmax20] 50 run(s) avg rewards : 9.8;4.36734693877551,9.8\n",
            "[task_2_tmax16] 50 run(s) avg rewards : 10.0;4.36,10.0\n",
            "Point: 9.9\n",
            "Double check Testing: t1,t2 : 20,16\n",
            "device is: cpu\n",
            "t1,t2,runs,tmax: 20,16,300,20\n",
            "[task_2_tmax20] 300 run(s) avg rewards : 9.4;4.053571428571429,9.333333333333334\n",
            "[task_2_tmax16] 300 run(s) avg rewards : 9.3;4.428571428571429,9.427609427609427\n",
            "Point: 9.350000000000001\n",
            "[Episode 8000]\t rewards globals : 7.110 \tavg rewards : 8.806,\t avg timing 18.7 \tavg loss: : 41.712155,\tbuffer size : 8000,\tepsilon : 1.1%, \t r <=40 0.0, \t r > 40 88.05970149253731\n",
            "avg_reaching_0 9.15 avg timing reached 0 : 4.800\n",
            "[Episode 8200]\t rewards globals : 7.161 \tavg rewards : 9.204,\t avg timing 18.7 \tavg loss: : 42.259881,\tbuffer size : 8000,\tepsilon : 1.1%, \t r <=40 0.0, \t r > 40 92.03980099502488\n",
            "avg_reaching_0 9.45 avg timing reached 0 : 4.300\n",
            "[Episode 8400]\t rewards globals : 7.211 \tavg rewards : 9.254,\t avg timing 18.4 \tavg loss: : 42.874117,\tbuffer size : 8000,\tepsilon : 1.1%, \t r <=40 0.0, \t r > 40 92.53731343283582\n",
            "avg_reaching_0 9.4 avg timing reached 0 : 5.000\n",
            "Testing: t1,t2 : 20,16\n",
            "device is: cpu\n",
            "t1,t2,runs,tmax: 20,16,50,20\n",
            "[task_2_tmax20] 50 run(s) avg rewards : 8.8;4.0227272727272725,8.8\n",
            "[task_2_tmax16] 50 run(s) avg rewards : 9.8;3.938775510204082,9.8\n",
            "Point: 9.3\n",
            "[Episode 8600]\t rewards globals : 7.254 \tavg rewards : 9.055,\t avg timing 18.8 \tavg loss: : 43.651070,\tbuffer size : 8000,\tepsilon : 1.1%, \t r <=40 0.0, \t r > 40 90.54726368159204\n",
            "avg_reaching_0 9.2 avg timing reached 0 : 3.700\n",
            "[Episode 8800]\t rewards globals : 7.286 \tavg rewards : 8.657,\t avg timing 18.9 \tavg loss: : 44.561362,\tbuffer size : 8000,\tepsilon : 1.1%, \t r <=40 0.0, \t r > 40 86.56716417910447\n",
            "avg_reaching_0 8.85 avg timing reached 0 : 5.500\n",
            "Testing: t1,t2 : 20,16\n",
            "device is: cpu\n",
            "t1,t2,runs,tmax: 20,16,50,20\n",
            "[task_2_tmax20] 50 run(s) avg rewards : 8.6;3.4651162790697674,8.6\n",
            "[task_2_tmax16] 50 run(s) avg rewards : 8.6;3.3488372093023258,8.6\n",
            "Point: 8.6\n",
            "[Episode 9000]\t rewards globals : 7.318 \tavg rewards : 8.756,\t avg timing 18.7 \tavg loss: : 45.777416,\tbuffer size : 8000,\tepsilon : 1.1%, \t r <=40 0.0, \t r > 40 87.56218905472637\n",
            "avg_reaching_0 8.85 avg timing reached 0 : 3.700\n",
            "[Episode 9200]\t rewards globals : 7.350 \tavg rewards : 8.806,\t avg timing 18.6 \tavg loss: : 47.278680,\tbuffer size : 8000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 88.05970149253731\n",
            "avg_reaching_0 8.85 avg timing reached 0 : 2.600\n",
            "[Episode 9400]\t rewards globals : 7.374 \tavg rewards : 8.458,\t avg timing 18.8 \tavg loss: : 49.290384,\tbuffer size : 8000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 84.5771144278607\n",
            "avg_reaching_0 8.75 avg timing reached 0 : 4.200\n",
            "Testing: t1,t2 : 20,16\n",
            "device is: cpu\n",
            "t1,t2,runs,tmax: 20,16,50,20\n",
            "[task_2_tmax20] 50 run(s) avg rewards : 9.2;3.4782608695652173,9.2\n",
            "[task_2_tmax16] 50 run(s) avg rewards : 8.6;4.534883720930233,8.6\n",
            "Point: 8.899999999999999\n",
            "[Episode 9600]\t rewards globals : 7.404 \tavg rewards : 8.856,\t avg timing 18.4 \tavg loss: : 51.238526,\tbuffer size : 8000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 88.55721393034825\n",
            "avg_reaching_0 8.95 avg timing reached 0 : 4.000\n",
            "[Episode 9800]\t rewards globals : 7.439 \tavg rewards : 9.104,\t avg timing 18.4 \tavg loss: : 53.181634,\tbuffer size : 8000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 91.04477611940298\n",
            "avg_reaching_0 9.3 avg timing reached 0 : 3.300\n",
            "Testing: t1,t2 : 20,16\n",
            "device is: cpu\n",
            "t1,t2,runs,tmax: 20,16,50,20\n",
            "[task_2_tmax20] 50 run(s) avg rewards : 9.6;4.520833333333333,9.6\n",
            "[task_2_tmax16] 50 run(s) avg rewards : 9.2;3.891304347826087,9.583333333333334\n",
            "Point: 9.399999999999999\n",
            "device is: cpu\n",
            "t1,t2,runs,tmax: 20,16,1000,20\n",
            "[task_2_tmax20] 1000 run(s) avg rewards : 9.2;3.936750272628135,9.17\n",
            "[task_2_tmax16] 1000 run(s) avg rewards : 9.2;3.616974972796518,9.21765295887663\n",
            "Point: 9.190000000000001\n",
            "========================================================================================================================================================================================================\n",
            "  F   -   -   -   -   3   -   -   5   -   -   -   6   -   -   -   -   -   -   -   -   -   -   -   -   -   4   -   -   -   -   -   -   -   -   -   2   -   -   -   -   -   -   -   1   -   -   -   -   O\n",
            "  -   -   -   -   -   -  14   -   -   -   -   -   -   -   -  10   -   -   7   -   -   -   -   -   -   -  13  11   -   -   -   -   -   -   -   -   -   -  12   -   9   -   -   -   -   -   -   8   -   -\n",
            "  -   -   -   -  20   -   -   -   -   -   -  16   -   -   -   <   -  17   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -  19   -   -   -   -  18   -   -  15   -   -   -   -   -\n",
            "========================================================================================================================================================================================================\n",
            "device is: cpu\n",
            "(4, 3, 26)\n",
            "(4, 3, 26)\n",
            "config : \n",
            " x debut 15, y_debut 2, max ep 10000, max epsilon 1.0 \n",
            " Epsilon_decay 1200, test_interval 500\n",
            "Save_interval 3000, batch_size 64, buffer_limit 8000 \n",
            " Methode DQN, gamma_nstep 0.5, nstep 2\n",
            "Dim crop largeur,hauteur 26,3 min buffer 5000 \n",
            " lane_d,lane_f 0,3\n",
            "Start learning from : x_debut 15 y_debut 2\n",
            "Testing: t1,t2 : 15,12\n",
            "device is: cpu\n",
            "t1,t2,runs,tmax: 15,12,50,15\n",
            "[task_2_tmax15] 50 run(s) avg rewards : 4.6;2.8947368421052633,3.8\n",
            "[task_2_tmax12] 50 run(s) avg rewards : 4.8;3.217391304347826,4.6\n",
            "Point: 4.699999999999999\n",
            "[Episode 200]\t rewards globals : 1.045 \tavg rewards : 1.045,\t avg timing 8.8 \tavg loss: : nan,\tbuffer size : 1273,\tepsilon : 84.8%, \t r <=40 0.0, \t r > 40 10.44776119402985\n",
            "avg_reaching_0 1.7 avg timing reached 0 : 4.800\n",
            "[Episode 400]\t rewards globals : 1.197 \tavg rewards : 1.343,\t avg timing 9.0 \tavg loss: : nan,\tbuffer size : 2416,\tepsilon : 71.9%, \t r <=40 0.0, \t r > 40 13.432835820895523\n",
            "avg_reaching_0 2.35 avg timing reached 0 : 4.600\n",
            "Testing: t1,t2 : 15,12\n",
            "device is: cpu\n",
            "t1,t2,runs,tmax: 15,12,50,15\n",
            "[task_2_tmax15] 50 run(s) avg rewards : 4.2;3.736842105263158,3.8\n",
            "[task_2_tmax12] 50 run(s) avg rewards : 4.6;3.217391304347826,4.6\n",
            "Point: 4.4\n",
            "[Episode 600]\t rewards globals : 1.298 \tavg rewards : 1.542,\t avg timing 9.4 \tavg loss: : nan,\tbuffer size : 3544,\tepsilon : 61.0%, \t r <=40 0.0, \t r > 40 15.422885572139302\n",
            "avg_reaching_0 2.55 avg timing reached 0 : 4.600\n",
            "[Episode 800]\t rewards globals : 1.635 \tavg rewards : 2.687,\t avg timing 9.0 \tavg loss: : nan,\tbuffer size : 4826,\tepsilon : 51.8%, \t r <=40 0.0, \t r > 40 26.865671641791046\n",
            "avg_reaching_0 3.9 avg timing reached 0 : 4.100\n",
            "Testing: t1,t2 : 15,12\n",
            "device is: cpu\n",
            "t1,t2,runs,tmax: 15,12,50,15\n",
            "[task_2_tmax15] 50 run(s) avg rewards : 1.8;6.0,1.3333333333333333\n",
            "[task_2_tmax12] 50 run(s) avg rewards : 2.0;6.0,2.121212121212121\n",
            "Point: 1.9\n",
            "[Episode 1000]\t rewards globals : 1.588 \tavg rewards : 1.393,\t avg timing 9.9 \tavg loss: : 30.580467,\tbuffer size : 6410,\tepsilon : 44.0%, \t r <=40 0.0, \t r > 40 13.930348258706468\n",
            "avg_reaching_0 1.6 avg timing reached 0 : 5.600\n",
            "[Episode 1200]\t rewards globals : 1.957 \tavg rewards : 3.781,\t avg timing 9.6 \tavg loss: : 19.715883,\tbuffer size : 8000,\tepsilon : 37.4%, \t r <=40 0.0, \t r > 40 37.81094527363184\n",
            "avg_reaching_0 5.05 avg timing reached 0 : 5.100\n",
            "[Episode 1400]\t rewards globals : 2.384 \tavg rewards : 4.925,\t avg timing 9.9 \tavg loss: : 15.540526,\tbuffer size : 8000,\tepsilon : 31.8%, \t r <=40 0.0, \t r > 40 49.25373134328358\n",
            "avg_reaching_0 6.1 avg timing reached 0 : 4.100\n",
            "Testing: t1,t2 : 15,12\n",
            "device is: cpu\n",
            "t1,t2,runs,tmax: 15,12,50,15\n",
            "[task_2_tmax15] 50 run(s) avg rewards : 9.0;3.0444444444444443,9.0\n",
            "[task_2_tmax12] 50 run(s) avg rewards : 8.2;2.926829268292683,8.91304347826087\n",
            "Point: 8.6\n",
            "[Episode 1600]\t rewards globals : 2.861 \tavg rewards : 6.169,\t avg timing 11.3 \tavg loss: : 13.539439,\tbuffer size : 8000,\tepsilon : 27.1%, \t r <=40 0.0, \t r > 40 61.69154228855721\n",
            "avg_reaching_0 7.15 avg timing reached 0 : 2.800\n",
            "[Episode 1800]\t rewards globals : 3.159 \tavg rewards : 5.522,\t avg timing 11.1 \tavg loss: : 12.755310,\tbuffer size : 8000,\tepsilon : 23.1%, \t r <=40 0.0, \t r > 40 55.223880597014926\n",
            "avg_reaching_0 7.1 avg timing reached 0 : 3.000\n",
            "Testing: t1,t2 : 15,12\n",
            "device is: cpu\n",
            "t1,t2,runs,tmax: 15,12,50,15\n",
            "[task_2_tmax15] 50 run(s) avg rewards : 9.0;3.3863636363636362,8.8\n",
            "[task_2_tmax12] 50 run(s) avg rewards : 8.8;3.302325581395349,8.775510204081632\n",
            "Point: 8.9\n",
            "[Episode 2000]\t rewards globals : 3.448 \tavg rewards : 6.070,\t avg timing 11.7 \tavg loss: : 12.394892,\tbuffer size : 8000,\tepsilon : 19.7%, \t r <=40 0.0, \t r > 40 60.69651741293532\n",
            "avg_reaching_0 7.0 avg timing reached 0 : 3.500\n",
            "[Episode 2200]\t rewards globals : 3.771 \tavg rewards : 6.965,\t avg timing 12.0 \tavg loss: : 12.186529,\tbuffer size : 8000,\tepsilon : 16.8%, \t r <=40 0.0, \t r > 40 69.65174129353234\n",
            "avg_reaching_0 7.75 avg timing reached 0 : 3.200\n",
            "[Episode 2400]\t rewards globals : 4.077 \tavg rewards : 7.463,\t avg timing 11.3 \tavg loss: : 12.047416,\tbuffer size : 8000,\tepsilon : 14.4%, \t r <=40 0.0, \t r > 40 74.6268656716418\n",
            "avg_reaching_0 8.05 avg timing reached 0 : 3.400\n",
            "Testing: t1,t2 : 15,12\n",
            "device is: cpu\n",
            "t1,t2,runs,tmax: 15,12,50,15\n",
            "[task_2_tmax15] 50 run(s) avg rewards : 9.6;3.4130434782608696,9.2\n",
            "[task_2_tmax12] 50 run(s) avg rewards : 9.0;3.840909090909091,9.777777777777779\n",
            "Point: 9.3\n",
            "[Episode 2600]\t rewards globals : 4.314 \tavg rewards : 7.114,\t avg timing 11.1 \tavg loss: : 12.191910,\tbuffer size : 8000,\tepsilon : 12.3%, \t r <=40 0.0, \t r > 40 71.14427860696517\n",
            "avg_reaching_0 7.7 avg timing reached 0 : 3.200\n",
            "[Episode 2800]\t rewards globals : 4.538 \tavg rewards : 7.463,\t avg timing 12.3 \tavg loss: : 12.419968,\tbuffer size : 8000,\tepsilon : 10.6%, \t r <=40 0.0, \t r > 40 74.6268656716418\n",
            "avg_reaching_0 8.3 avg timing reached 0 : 3.300\n",
            "Testing: t1,t2 : 15,12\n",
            "device is: cpu\n",
            "t1,t2,runs,tmax: 15,12,50,15\n",
            "[task_2_tmax15] 50 run(s) avg rewards : 9.6;3.978723404255319,9.4\n",
            "[task_2_tmax12] 50 run(s) avg rewards : 9.4;3.595744680851064,9.4\n",
            "Point: 9.5\n",
            "Double check Testing: t1,t2 : 15,12\n",
            "device is: cpu\n",
            "t1,t2,runs,tmax: 15,12,300,15\n",
            "[task_2_tmax15] 300 run(s) avg rewards : 9.3;3.7970479704797047,9.093959731543624\n",
            "[task_2_tmax12] 300 run(s) avg rewards : 9.0;3.3533834586466167,9.268292682926829\n",
            "Point: 9.116666666666667\n",
            "[Episode 3000]\t rewards globals : 4.745 \tavg rewards : 7.662,\t avg timing 12.3 \tavg loss: : 12.725792,\tbuffer size : 8000,\tepsilon : 9.1%, \t r <=40 0.0, \t r > 40 76.61691542288557\n",
            "avg_reaching_0 7.95 avg timing reached 0 : 4.500\n",
            "[Episode 3200]\t rewards globals : 4.948 \tavg rewards : 7.960,\t avg timing 13.3 \tavg loss: : 13.070633,\tbuffer size : 8000,\tepsilon : 7.9%, \t r <=40 0.0, \t r > 40 79.60199004975125\n",
            "avg_reaching_0 8.35 avg timing reached 0 : 3.600\n",
            "[Episode 3400]\t rewards globals : 5.181 \tavg rewards : 8.905,\t avg timing 12.5 \tavg loss: : 13.304036,\tbuffer size : 8000,\tepsilon : 6.8%, \t r <=40 0.0, \t r > 40 89.05472636815921\n",
            "avg_reaching_0 9.05 avg timing reached 0 : 5.100\n",
            "Testing: t1,t2 : 15,12\n",
            "device is: cpu\n",
            "t1,t2,runs,tmax: 15,12,50,15\n",
            "[task_2_tmax15] 50 run(s) avg rewards : 9.6;3.2083333333333335,9.6\n",
            "[task_2_tmax12] 50 run(s) avg rewards : 9.8;3.3469387755102042,9.8\n",
            "Point: 9.7\n",
            "Double check Testing: t1,t2 : 15,12\n",
            "device is: cpu\n",
            "t1,t2,runs,tmax: 15,12,300,15\n",
            "[task_2_tmax15] 300 run(s) avg rewards : 9.4;3.795620437956204,9.163879598662207\n",
            "[task_2_tmax12] 300 run(s) avg rewards : 9.1;3.4306569343065694,9.750889679715302\n",
            "Point: 9.266666666666666\n",
            "[Episode 3600]\t rewards globals : 5.357 \tavg rewards : 8.358,\t avg timing 12.8 \tavg loss: : 13.548026,\tbuffer size : 8000,\tepsilon : 5.9%, \t r <=40 0.0, \t r > 40 83.5820895522388\n",
            "avg_reaching_0 8.95 avg timing reached 0 : 3.400\n",
            "[Episode 3800]\t rewards globals : 5.520 \tavg rewards : 8.408,\t avg timing 13.2 \tavg loss: : 13.735489,\tbuffer size : 8000,\tepsilon : 5.2%, \t r <=40 0.0, \t r > 40 84.07960199004975\n",
            "avg_reaching_0 8.85 avg timing reached 0 : 2.600\n",
            "Testing: t1,t2 : 15,12\n",
            "device is: cpu\n",
            "t1,t2,runs,tmax: 15,12,50,15\n",
            "[task_2_tmax15] 50 run(s) avg rewards : 9.6;3.425531914893617,9.591836734693878\n",
            "[task_2_tmax12] 50 run(s) avg rewards : 9.4;4.0,9.591836734693878\n",
            "Point: 9.5\n",
            "Double check Testing: t1,t2 : 15,12\n",
            "device is: cpu\n",
            "t1,t2,runs,tmax: 15,12,300,15\n",
            "[task_2_tmax15] 300 run(s) avg rewards : 9.5;3.7580071174377223,9.493243243243244\n",
            "[task_2_tmax12] 300 run(s) avg rewards : 8.8;3.7471698113207546,9.298245614035087\n",
            "Point: 9.15\n",
            "[Episode 4000]\t rewards globals : 5.649 \tavg rewards : 8.109,\t avg timing 13.8 \tavg loss: : 13.908246,\tbuffer size : 8000,\tepsilon : 4.5%, \t r <=40 0.0, \t r > 40 81.09452736318407\n",
            "avg_reaching_0 8.3 avg timing reached 0 : 3.500\n",
            "[Episode 4200]\t rewards globals : 5.818 \tavg rewards : 9.204,\t avg timing 13.9 \tavg loss: : 14.042648,\tbuffer size : 8000,\tepsilon : 4.0%, \t r <=40 0.0, \t r > 40 92.03980099502488\n",
            "avg_reaching_0 9.1 avg timing reached 0 : 5.200\n",
            "[Episode 4400]\t rewards globals : 5.955 \tavg rewards : 8.856,\t avg timing 13.6 \tavg loss: : 14.253864,\tbuffer size : 8000,\tepsilon : 3.5%, \t r <=40 0.0, \t r > 40 88.55721393034825\n",
            "avg_reaching_0 9.0 avg timing reached 0 : 2.500\n",
            "Testing: t1,t2 : 15,12\n",
            "device is: cpu\n",
            "t1,t2,runs,tmax: 15,12,50,15\n",
            "[task_2_tmax15] 50 run(s) avg rewards : 8.2;3.8536585365853657,8.2\n",
            "[task_2_tmax12] 50 run(s) avg rewards : 9.0;3.2666666666666666,9.183673469387756\n",
            "Point: 8.6\n",
            "[Episode 4600]\t rewards globals : 6.077 \tavg rewards : 8.756,\t avg timing 12.8 \tavg loss: : 14.444011,\tbuffer size : 8000,\tepsilon : 3.1%, \t r <=40 0.0, \t r > 40 87.56218905472637\n",
            "avg_reaching_0 8.85 avg timing reached 0 : 3.400\n",
            "[Episode 4800]\t rewards globals : 6.199 \tavg rewards : 9.005,\t avg timing 13.3 \tavg loss: : 14.673831,\tbuffer size : 8000,\tepsilon : 2.8%, \t r <=40 0.0, \t r > 40 90.04975124378109\n",
            "avg_reaching_0 8.95 avg timing reached 0 : 4.300\n",
            "Testing: t1,t2 : 15,12\n",
            "device is: cpu\n",
            "t1,t2,runs,tmax: 15,12,50,15\n",
            "[task_2_tmax15] 50 run(s) avg rewards : 9.0;3.255813953488372,8.6\n",
            "[task_2_tmax12] 50 run(s) avg rewards : 8.8;3.409090909090909,9.166666666666666\n",
            "Point: 8.9\n",
            "[Episode 5000]\t rewards globals : 6.315 \tavg rewards : 9.104,\t avg timing 13.8 \tavg loss: : 14.919156,\tbuffer size : 8000,\tepsilon : 2.5%, \t r <=40 0.0, \t r > 40 91.04477611940298\n",
            "avg_reaching_0 9.05 avg timing reached 0 : 4.000\n",
            "[Episode 5200]\t rewards globals : 6.416 \tavg rewards : 8.955,\t avg timing 13.3 \tavg loss: : 15.283368,\tbuffer size : 8000,\tepsilon : 2.3%, \t r <=40 0.0, \t r > 40 89.55223880597015\n",
            "avg_reaching_0 8.95 avg timing reached 0 : 4.100\n",
            "[Episode 5400]\t rewards globals : 6.510 \tavg rewards : 8.955,\t avg timing 13.9 \tavg loss: : 15.694984,\tbuffer size : 8000,\tepsilon : 2.1%, \t r <=40 0.0, \t r > 40 89.55223880597015\n",
            "avg_reaching_0 9.0 avg timing reached 0 : 3.400\n",
            "Testing: t1,t2 : 15,12\n",
            "device is: cpu\n",
            "t1,t2,runs,tmax: 15,12,50,15\n",
            "[task_2_tmax15] 50 run(s) avg rewards : 8.8;3.6511627906976742,8.6\n",
            "[task_2_tmax12] 50 run(s) avg rewards : 9.0;3.088888888888889,9.0\n",
            "Point: 8.9\n",
            "[Episode 5600]\t rewards globals : 6.595 \tavg rewards : 8.905,\t avg timing 13.5 \tavg loss: : 16.104008,\tbuffer size : 8000,\tepsilon : 1.9%, \t r <=40 0.0, \t r > 40 89.05472636815921\n",
            "avg_reaching_0 9.0 avg timing reached 0 : 2.500\n",
            "[Episode 5800]\t rewards globals : 6.685 \tavg rewards : 9.204,\t avg timing 13.5 \tavg loss: : 16.518318,\tbuffer size : 8000,\tepsilon : 1.8%, \t r <=40 0.0, \t r > 40 92.03980099502488\n",
            "avg_reaching_0 9.1 avg timing reached 0 : 3.800\n",
            "Testing: t1,t2 : 15,12\n",
            "device is: cpu\n",
            "t1,t2,runs,tmax: 15,12,50,15\n",
            "[task_2_tmax15] 50 run(s) avg rewards : 8.8;3.5454545454545454,8.8\n",
            "[task_2_tmax12] 50 run(s) avg rewards : 9.6;4.1875,9.6\n",
            "Point: 9.2\n",
            "[Episode 6000]\t rewards globals : 6.764 \tavg rewards : 9.055,\t avg timing 13.9 \tavg loss: : 16.908513,\tbuffer size : 8000,\tepsilon : 1.7%, \t r <=40 0.0, \t r > 40 90.54726368159204\n",
            "avg_reaching_0 8.95 avg timing reached 0 : 3.100\n",
            "[Episode 6200]\t rewards globals : 6.834 \tavg rewards : 8.955,\t avg timing 13.6 \tavg loss: : 17.321191,\tbuffer size : 8000,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 89.55223880597015\n",
            "avg_reaching_0 9.25 avg timing reached 0 : 4.700\n",
            "[Episode 6400]\t rewards globals : 6.908 \tavg rewards : 9.204,\t avg timing 13.6 \tavg loss: : 17.700023,\tbuffer size : 8000,\tepsilon : 1.5%, \t r <=40 0.0, \t r > 40 92.03980099502488\n",
            "avg_reaching_0 9.3 avg timing reached 0 : 2.800\n",
            "Testing: t1,t2 : 15,12\n",
            "device is: cpu\n",
            "t1,t2,runs,tmax: 15,12,50,15\n",
            "[task_2_tmax15] 50 run(s) avg rewards : 8.8;3.7954545454545454,8.8\n",
            "[task_2_tmax12] 50 run(s) avg rewards : 8.8;3.522727272727273,8.8\n",
            "Point: 8.8\n",
            "[Episode 6600]\t rewards globals : 6.964 \tavg rewards : 8.756,\t avg timing 13.8 \tavg loss: : 18.126484,\tbuffer size : 8000,\tepsilon : 1.4%, \t r <=40 0.0, \t r > 40 87.56218905472637\n",
            "avg_reaching_0 8.4 avg timing reached 0 : 5.200\n",
            "[Episode 6800]\t rewards globals : 7.027 \tavg rewards : 9.104,\t avg timing 13.7 \tavg loss: : 18.494671,\tbuffer size : 8000,\tepsilon : 1.3%, \t r <=40 0.0, \t r > 40 91.04477611940298\n",
            "avg_reaching_0 9.1 avg timing reached 0 : 3.400\n",
            "Testing: t1,t2 : 15,12\n",
            "device is: cpu\n",
            "t1,t2,runs,tmax: 15,12,50,15\n",
            "[task_2_tmax15] 50 run(s) avg rewards : 9.0;3.7777777777777777,9.0\n",
            "[task_2_tmax12] 50 run(s) avg rewards : 9.6;3.5416666666666665,9.6\n",
            "Point: 9.3\n",
            "[Episode 7000]\t rewards globals : 7.088 \tavg rewards : 9.154,\t avg timing 13.8 \tavg loss: : 18.924310,\tbuffer size : 8000,\tepsilon : 1.3%, \t r <=40 0.0, \t r > 40 91.54228855721394\n",
            "avg_reaching_0 9.05 avg timing reached 0 : 3.400\n",
            "[Episode 7200]\t rewards globals : 7.141 \tavg rewards : 9.005,\t avg timing 13.7 \tavg loss: : 19.455469,\tbuffer size : 8000,\tepsilon : 1.2%, \t r <=40 0.0, \t r > 40 90.04975124378109\n",
            "avg_reaching_0 9.1 avg timing reached 0 : 3.000\n",
            "[Episode 7400]\t rewards globals : 7.179 \tavg rewards : 8.557,\t avg timing 12.9 \tavg loss: : 20.043728,\tbuffer size : 8000,\tepsilon : 1.2%, \t r <=40 0.0, \t r > 40 85.57213930348259\n",
            "avg_reaching_0 8.55 avg timing reached 0 : 4.200\n",
            "Testing: t1,t2 : 15,12\n",
            "device is: cpu\n",
            "t1,t2,runs,tmax: 15,12,50,15\n",
            "[task_2_tmax15] 50 run(s) avg rewards : 8.6;3.142857142857143,8.4\n",
            "[task_2_tmax12] 50 run(s) avg rewards : 9.2;3.6666666666666665,9.0\n",
            "Point: 8.899999999999999\n",
            "[Episode 7600]\t rewards globals : 7.225 \tavg rewards : 8.955,\t avg timing 13.5 \tavg loss: : 20.616262,\tbuffer size : 8000,\tepsilon : 1.2%, \t r <=40 0.0, \t r > 40 89.55223880597015\n",
            "avg_reaching_0 9.2 avg timing reached 0 : 2.800\n",
            "[Episode 7800]\t rewards globals : 7.268 \tavg rewards : 8.905,\t avg timing 13.9 \tavg loss: : 21.290688,\tbuffer size : 8000,\tepsilon : 1.1%, \t r <=40 0.0, \t r > 40 89.05472636815921\n",
            "avg_reaching_0 8.95 avg timing reached 0 : 4.800\n",
            "Testing: t1,t2 : 15,12\n",
            "device is: cpu\n",
            "t1,t2,runs,tmax: 15,12,50,15\n",
            "[task_2_tmax15] 50 run(s) avg rewards : 9.2;3.9782608695652173,9.2\n",
            "[task_2_tmax12] 50 run(s) avg rewards : 9.4;3.3191489361702127,9.591836734693878\n",
            "Point: 9.3\n",
            "[Episode 8000]\t rewards globals : 7.309 \tavg rewards : 8.905,\t avg timing 13.6 \tavg loss: : 21.903003,\tbuffer size : 8000,\tepsilon : 1.1%, \t r <=40 0.0, \t r > 40 89.05472636815921\n",
            "avg_reaching_0 9.0 avg timing reached 0 : 3.400\n",
            "[Episode 8200]\t rewards globals : 7.349 \tavg rewards : 8.955,\t avg timing 13.8 \tavg loss: : 22.467220,\tbuffer size : 8000,\tepsilon : 1.1%, \t r <=40 0.0, \t r > 40 89.55223880597015\n",
            "avg_reaching_0 8.95 avg timing reached 0 : 2.900\n",
            "[Episode 8400]\t rewards globals : 7.390 \tavg rewards : 9.055,\t avg timing 13.3 \tavg loss: : 23.135541,\tbuffer size : 8000,\tepsilon : 1.1%, \t r <=40 0.0, \t r > 40 90.54726368159204\n",
            "avg_reaching_0 9.2 avg timing reached 0 : 3.000\n",
            "Testing: t1,t2 : 15,12\n",
            "device is: cpu\n",
            "t1,t2,runs,tmax: 15,12,50,15\n",
            "[task_2_tmax15] 50 run(s) avg rewards : 9.6;3.404255319148936,9.4\n",
            "[task_2_tmax12] 50 run(s) avg rewards : 9.0;3.4186046511627906,8.6\n",
            "Point: 9.3\n",
            "[Episode 8600]\t rewards globals : 7.425 \tavg rewards : 8.905,\t avg timing 13.3 \tavg loss: : 23.772023,\tbuffer size : 8000,\tepsilon : 1.1%, \t r <=40 0.0, \t r > 40 89.05472636815921\n",
            "avg_reaching_0 9.1 avg timing reached 0 : 4.300\n",
            "[Episode 8800]\t rewards globals : 7.465 \tavg rewards : 9.204,\t avg timing 13.9 \tavg loss: : 24.452119,\tbuffer size : 8000,\tepsilon : 1.1%, \t r <=40 0.0, \t r > 40 92.03980099502488\n",
            "avg_reaching_0 9.3 avg timing reached 0 : 2.800\n",
            "Testing: t1,t2 : 15,12\n",
            "device is: cpu\n",
            "t1,t2,runs,tmax: 15,12,50,15\n",
            "[task_2_tmax15] 50 run(s) avg rewards : 9.2;3.608695652173913,9.2\n",
            "[task_2_tmax12] 50 run(s) avg rewards : 9.8;3.5714285714285716,9.8\n",
            "Point: 9.5\n",
            "Double check Testing: t1,t2 : 15,12\n",
            "device is: cpu\n",
            "t1,t2,runs,tmax: 15,12,300,15\n",
            "[task_2_tmax15] 300 run(s) avg rewards : 8.9;3.988721804511278,8.866666666666667\n",
            "[task_2_tmax12] 300 run(s) avg rewards : 8.9;4.245283018867925,9.075342465753424\n",
            "Point: 8.916666666666668\n",
            "[Episode 9000]\t rewards globals : 7.498 \tavg rewards : 8.905,\t avg timing 13.7 \tavg loss: : 25.250929,\tbuffer size : 8000,\tepsilon : 1.1%, \t r <=40 0.0, \t r > 40 89.05472636815921\n",
            "avg_reaching_0 9.1 avg timing reached 0 : 4.000\n",
            "[Episode 9200]\t rewards globals : 7.525 \tavg rewards : 8.756,\t avg timing 13.6 \tavg loss: : 26.121725,\tbuffer size : 8000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 87.56218905472637\n",
            "avg_reaching_0 8.95 avg timing reached 0 : 3.600\n",
            "[Episode 9400]\t rewards globals : 7.550 \tavg rewards : 8.706,\t avg timing 13.5 \tavg loss: : 26.944136,\tbuffer size : 8000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 87.06467661691542\n",
            "avg_reaching_0 8.85 avg timing reached 0 : 4.900\n",
            "Testing: t1,t2 : 15,12\n",
            "device is: cpu\n",
            "t1,t2,runs,tmax: 15,12,50,15\n",
            "[task_2_tmax15] 50 run(s) avg rewards : 8.2;4.0,8.2\n",
            "[task_2_tmax12] 50 run(s) avg rewards : 9.0;3.466666666666667,9.183673469387756\n",
            "Point: 8.6\n",
            "[Episode 9600]\t rewards globals : 7.572 \tavg rewards : 8.607,\t avg timing 13.6 \tavg loss: : 27.768682,\tbuffer size : 8000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 86.06965174129353\n",
            "avg_reaching_0 8.85 avg timing reached 0 : 4.200\n",
            "[Episode 9800]\t rewards globals : 7.602 \tavg rewards : 9.055,\t avg timing 13.7 \tavg loss: : 28.463154,\tbuffer size : 8000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 90.54726368159204\n",
            "avg_reaching_0 9.05 avg timing reached 0 : 4.000\n",
            "Testing: t1,t2 : 15,12\n",
            "device is: cpu\n",
            "t1,t2,runs,tmax: 15,12,50,15\n",
            "[task_2_tmax15] 50 run(s) avg rewards : 8.6;3.372093023255814,8.6\n",
            "[task_2_tmax12] 50 run(s) avg rewards : 9.4;3.148936170212766,9.4\n",
            "Point: 9.0\n",
            "device is: cpu\n",
            "t1,t2,runs,tmax: 15,12,1000,15\n",
            "[task_2_tmax15] 1000 run(s) avg rewards : 9.0;3.7527964205816553,8.94894894894895\n",
            "[task_2_tmax12] 1000 run(s) avg rewards : 8.9;3.886516853932584,9.090909090909092\n",
            "Point: 8.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8.95"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVl6BA5Kow7b",
        "colab_type": "code",
        "outputId": "78e7f51d-259e-406d-bcb5-f4c9d7d5e75b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        }
      },
      "source": [
        "import importlib \n",
        "import torch\n",
        "from HW3_Task2.agent import state_change\n",
        "importlib.reload(models)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# 15,3 \n",
        "model_0_3 = models.get_model(modelpath='HW3_Task2/saves/m__cb_12_2.pt',device=device)\n",
        "# 18,3 \n",
        "#model_1_4 = models.get_model(modelpath='HW3_Task2/saves/m_DQN_12_2_1_4.pt',device=device)\n",
        "\n",
        "heads = [model_0_3]\n",
        "heads_config = [[15,3],[18,3]]\n",
        "\n",
        "x_debut = 12\n",
        "y_debut = 2\n",
        "x_fin = 0\n",
        "y_fin = 0 \n",
        "lane_d = 0\n",
        "lane_f = 3\n",
        "\n",
        "env = env_builder.construct_task2_env_ij_lanes(x_debut,y_debut,x_fin,y_fin,lane_d,lane_f)\n",
        "\n",
        "state = env.reset()\n",
        "env.render()\n",
        "state = state_change.state_from_pos(x_debut,y_debut,state,largeur=18,hauteur=3)\n",
        "print(state)\n",
        "models.mainTest(model,runs=100,env=env,t1=x_debut,t2=int(4*x_debut/5))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model loaded to device is: cuda\n",
            "========================================================================================================================================================================================================\n",
            "  F   -   4   O   5   -   -   -   6   -   -   -   -   -   -   -   -   -   -   -   -   -   -   2   1   -   -   -   -   -   -   -   -   -   -   -   -   -   -   3   -   -   -   -   -   -   -   -   -   -\n",
            "  9   -   7   -   -  10   -  11  13   -   -   -   -   -  12   -   -   -   -   -   -   -   -   -   -   -   -   8   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -  14   -   -   -   -   -   -\n",
            "  -   -   -   -   -   -  19   -   -   -   -   -   <   -  17   -   -   -   -   -   -  15   -   -   -  16   -   -   -   -   -   -   -   -   -  20   -   -   -  18   -   -   -   -   -   -   -   -   -   -\n",
            "========================================================================================================================================================================================================\n",
            "[[[0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n",
            "device is: cuda\n",
            "t1,t2,runs,tmax: 12,9,100,12\n",
            "[task_2_tmax12] 100 run(s) avg rewards : 9.0\n",
            "[task_2_tmax9] 100 run(s) avg rewards : 8.3\n",
            "Point: 8.65\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8.65"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYM2wcpcTqXm",
        "colab_type": "code",
        "outputId": "fd9c479d-6734-4b23-c2a9-72926238898a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 910
        }
      },
      "source": [
        "from google.colab import files\n",
        "'''try:\n",
        "  !zip -r HW3_Task2_6_9_4_7.zip HW3_Task2\n",
        "except:\n",
        "  print(\"error while zipping\")\n",
        "try:\n",
        "  files.download(\"HW3_Task2autre2.zip\")\n",
        "except:\n",
        "  print(\"error while downloading\")'''\n",
        "!unzip HW3_Task2_exportoulala.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  HW3_Task2_exportoulala.zip\n",
            "   creating: HW3_Task2_export 2/\n",
            "  inflating: HW3_Task2_export 2/.DS_Store  \n",
            "   creating: __MACOSX/\n",
            "   creating: __MACOSX/HW3_Task2_export 2/\n",
            "  inflating: __MACOSX/HW3_Task2_export 2/._.DS_Store  \n",
            "   creating: HW3_Task2_export 2/agent/\n",
            "  inflating: HW3_Task2_export 2/agent/buffer_src.py  \n",
            "   creating: __MACOSX/HW3_Task2_export 2/agent/\n",
            "  inflating: __MACOSX/HW3_Task2_export 2/agent/._buffer_src.py  \n",
            "  inflating: HW3_Task2_export 2/agent/env.py  \n",
            "  inflating: __MACOSX/HW3_Task2_export 2/agent/._env.py  \n",
            "  inflating: HW3_Task2_export 2/agent/models.py  \n",
            "  inflating: __MACOSX/HW3_Task2_export 2/agent/._models.py  \n",
            "  inflating: HW3_Task2_export 2/agent/loss_src.py  \n",
            "  inflating: __MACOSX/HW3_Task2_export 2/agent/._loss_src.py  \n",
            "  inflating: HW3_Task2_export 2/agent/__init__.py  \n",
            "  inflating: __MACOSX/HW3_Task2_export 2/agent/.___init__.py  \n",
            "   creating: HW3_Task2_export 2/agent/__pycache__/\n",
            "  inflating: HW3_Task2_export 2/agent/__pycache__/buffer_src.cpython-36.pyc  \n",
            "   creating: __MACOSX/HW3_Task2_export 2/agent/__pycache__/\n",
            "  inflating: __MACOSX/HW3_Task2_export 2/agent/__pycache__/._buffer_src.cpython-36.pyc  \n",
            "  inflating: HW3_Task2_export 2/agent/__pycache__/loss_src.cpython-36.pyc  \n",
            "  inflating: __MACOSX/HW3_Task2_export 2/agent/__pycache__/._loss_src.cpython-36.pyc  \n",
            "  inflating: HW3_Task2_export 2/agent/__pycache__/models.cpython-36.pyc  \n",
            "  inflating: __MACOSX/HW3_Task2_export 2/agent/__pycache__/._models.cpython-36.pyc  \n",
            "  inflating: HW3_Task2_export 2/agent/__pycache__/agent.cpython-36.pyc  \n",
            "  inflating: __MACOSX/HW3_Task2_export 2/agent/__pycache__/._agent.cpython-36.pyc  \n",
            "  inflating: HW3_Task2_export 2/agent/__pycache__/state_change.cpython-36.pyc  \n",
            "  inflating: __MACOSX/HW3_Task2_export 2/agent/__pycache__/._state_change.cpython-36.pyc  \n",
            "  inflating: HW3_Task2_export 2/agent/__pycache__/__init__.cpython-36.pyc  \n",
            "  inflating: __MACOSX/HW3_Task2_export 2/agent/__pycache__/.___init__.cpython-36.pyc  \n",
            "  inflating: HW3_Task2_export 2/agent/__pycache__/env.cpython-36.pyc  \n",
            "  inflating: __MACOSX/HW3_Task2_export 2/agent/__pycache__/._env.cpython-36.pyc  \n",
            "  inflating: __MACOSX/HW3_Task2_export 2/agent/.___pycache__  \n",
            "  inflating: HW3_Task2_export 2/agent/state_change.py  \n",
            "  inflating: __MACOSX/HW3_Task2_export 2/agent/._state_change.py  \n",
            "  inflating: HW3_Task2_export 2/agent/loss.py  \n",
            "  inflating: __MACOSX/HW3_Task2_export 2/agent/._loss.py  \n",
            "  inflating: HW3_Task2_export 2/agent/agent.py  \n",
            "  inflating: __MACOSX/HW3_Task2_export 2/agent/._agent.py  \n",
            "   creating: HW3_Task2_export 2/agent/.ipynb_checkpoints/\n",
            "  inflating: __MACOSX/HW3_Task2_export 2/agent/._.ipynb_checkpoints  \n",
            "  inflating: __MACOSX/HW3_Task2_export 2/._agent  \n",
            "  inflating: HW3_Task2_export 2/setup.py  \n",
            "  inflating: __MACOSX/HW3_Task2_export 2/._setup.py  \n",
            "   creating: HW3_Task2_export 2/.ipynb_checkpoints/\n",
            "  inflating: __MACOSX/HW3_Task2_export 2/._.ipynb_checkpoints  \n",
            " extracting: HW3_Task2_export 2/untitled  \n",
            "  inflating: __MACOSX/HW3_Task2_export 2/._untitled  \n",
            "  inflating: __MACOSX/._HW3_Task2_export 2  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjfLwUv1dY6j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        },
        "outputId": "532eeea1-f78d-40b1-f6ae-d5579101cd8a"
      },
      "source": [
        ""
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-55-2646bbe1e858>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    print([0:7:1])\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdZAvDb9w31e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1c6f6768-a926-4336-e29f-04b0cce2017c"
      },
      "source": [
        ""
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kz-1RmXNFtuz",
        "colab_type": "code",
        "outputId": "303c7306-d6a6-446a-b2b5-f68ef93eaece",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        }
      },
      "source": [
        "from HW3_Task2.agent import env as env_builder\n",
        "import numpy as np\n",
        "from HW3_Task2.agent import state_change \n",
        "\n",
        "\n",
        "\n",
        "def act(pos_x,pos_y,state,vitesses,counter):\n",
        "  \n",
        "  def blurred_from_state(state,vitesses):\n",
        "    res = np.zeros([10,50])\n",
        "    pos_x,pos_y = state_change.pos_from_state(state)\n",
        "    for i in range(len(vitesses)):\n",
        "      if i != pos_y:\n",
        "        vit_cur = abs(np.array(vitesses[i]))\n",
        "        min_vit = np.min(vit_cur)\n",
        "        max_vit = np.max(vit_cur)\n",
        "        for vit in range(1,max_vit+1):\n",
        "          res[i,:] += np.roll(state[0,i,:], -vit)\n",
        "        res[i,:] += state[0,i,:]\n",
        "      else:\n",
        "        res[i,:] += state[0,i,:]\n",
        "    return(res)\n",
        "\n",
        "  def pos_y_proche(pos_x,pos_y,blurred):\n",
        "    return None\n",
        "\n",
        "  def can_go_up(pos_x,pos_y,blurred):\n",
        "    if pos_y != 0 :  \n",
        "      if blurred[pos_y-1,pos_x-1] == 0:\n",
        "        return True \n",
        "    return False\n",
        "\n",
        "  def can_doubler(pos_x,pos_y,blurred):\n",
        "    row1 = blurred[pos_y-1,pos_x+1:50]\n",
        "    row2 = np.flip(blurred[pos_y-1,0:pos_x])\n",
        "    apres = np.where(row1 == 0)[0]\n",
        "    avant = np.where(row2 == 0)[0]\n",
        "    if len(avant) > 1:\n",
        "      if avant[0] == avant[1]+1:\n",
        "        return avant[0][0]\n",
        "    return -1\n",
        "\n",
        "  def nearest_go_up(pos_x,pos_y,blurred):\n",
        "    row1 = blurred[pos_y-1,pos_x+1:50]\n",
        "    row2 = np.flip(blurred[pos_y-1,0:pos_x])\n",
        "    apres = np.where(row1 == 0)[0]\n",
        "    avant = np.where(row2 == 0)[0]\n",
        "    if len(apres)==0:\n",
        "      apres = 0 \n",
        "    else:\n",
        "      apres = apres[0]\n",
        "    if len(avant)==0:\n",
        "      avant = 0 \n",
        "    else:\n",
        "      avant = avant[0]\n",
        "    return apres,avant\n",
        "\n",
        "  \n",
        "  \n",
        "  blurred = blurred_from_state(state,vitesses)\n",
        "  action = -1\n",
        "  add = 0\n",
        "  #print(can_go_up(pos_x,pos_y,blurred))\n",
        "  vitesses = [[-2, -1],[-2, -1],[-1, -1],[-3, -1],[-2, -1],[-2, -1],[-3, -2],[-1, -1],[-2, -1],[-2, -2]]\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "  if can_go_up(pos_x,pos_y,blurred):\n",
        "    action =0\n",
        "    to_print = \"go_up\"\n",
        "    apres,avant = nearest_go_up(pos_x,pos_y,blurred)\n",
        "    devant = np.flip(blurred[pos_y,0:pos_x])\n",
        "    espace_dispo_devant = (np.where(devant > 0)[0])\n",
        "  else:\n",
        "    apres,avant = nearest_go_up(pos_x,pos_y,blurred)\n",
        "    devant = np.flip(blurred[pos_y,0:pos_x])\n",
        "    espace_dispo_devant = (np.where(devant > 0)[0])\n",
        "    \n",
        "    if len(espace_dispo_devant) == 0 and action ==-1:\n",
        "      if counter >= 10:\n",
        "          if pos_y == 0:\n",
        "            action = 4\n",
        "            add = 1\n",
        "            to_print = \"Vroum vroum ralenti {}\".format(0)\n",
        "          elif abs(np.mean(vitesses[pos_y-1]) == 1):\n",
        "            action,add = 2\n",
        "            to_print = \"les escargots {}\".format(0)\n",
        "          else:\n",
        "            action = 4\n",
        "            to_print = \"cheat code ralenti {}\".format(0)\n",
        "      else:\n",
        "        action = 2\n",
        "        to_print = \"cheat code {}\".format(0)\n",
        "      \n",
        "    def go_ligne_droite(pos_y,espace_dispo_devant,counter):\n",
        "      add = 0\n",
        "      if len(espace_dispo_devant) > 0:\n",
        "        espace_dispo = espace_dispo_devant[0]\n",
        "        if pos_y == 0:\n",
        "          if espace_dispo > 4:\n",
        "              action = 3\n",
        "              add = 1\n",
        "          if espace_dispo > 3:\n",
        "            action = 2\n",
        "            add = 2\n",
        "          else:\n",
        "            action = 4\n",
        "        else:\n",
        "          if espace_dispo > 4 and (counter < 10 or abs(np.mean(vitesses[pos_y-1])) == 1):\n",
        "              action = 3\n",
        "              add = 1\n",
        "          if espace_dispo > 3 and (counter < 10 or abs(np.mean(vitesses[pos_y-1])) == 1):\n",
        "            action = 2\n",
        "            add = 2\n",
        "          else:\n",
        "            action = 4\n",
        "      else:\n",
        "        action = -1\n",
        "      return action,add\n",
        "\n",
        "    if pos_y == 0 and action ==-1:\n",
        "      action,add = go_ligne_droite(pos_y,espace_dispo_devant,counter)\n",
        "      to_print = \"ligne droite {}\".format(0)\n",
        "    \n",
        "    if action == -1:\n",
        "      to_print = \"ligne droite {}\".format(0)\n",
        "      action,add = go_ligne_droite(pos_y,espace_dispo_devant,counter)\n",
        "    \n",
        "    if ((pos_y > 6 and pos_x <35)) and counter > 10 and abs(np.mean(vitesses[pos_y-1])) > 1 and (action == 2 or action == 3) :\n",
        "      action = 4\n",
        "\n",
        "  #  return(action)\n",
        "  return(action,to_print,espace_dispo_devant,avant,apres,pos_x,pos_y,blurred,counter+add)\n",
        "\n",
        "\n",
        "env= env_builder.construct_task2_env_ij(49,9)\n",
        "state = env.reset()\n",
        "env.render()\n",
        "\n",
        "#print(blurred_from_state(state,vitesses))\n",
        "\n",
        "import torch\n",
        "def mainTest(agent,runs=25,env=None,t1=50,t2=40,cropping=None):\n",
        "    \n",
        "    import sys\n",
        "    import time\n",
        "    from HW3_Task2.agent import state_change\n",
        "\n",
        "    FAST_DOWNWARD_PATH = \"/fast_downward/\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"device is:\",device)\n",
        "\n",
        "\n",
        "    def test(agent, env=None, runs=1000, t_max=100):\n",
        "        rewards = []\n",
        "        vitesses = [[-2, -1],[-2, -1],[-1, -1],[-3, -1],[-2, -1],[-2, -1],[-3, -2],[-1, -1],[-2, -1],[-2, -2]]\n",
        "        for run in range(runs):\n",
        "            state = env.reset()\n",
        "            agent_init = {'fast_downward_path': FAST_DOWNWARD_PATH, 'agent_speed_range': (-3,-1), 'gamma' : 1}\n",
        "            #agent.initialize(**agent_init)\n",
        "            episode_rewards = 0.0\n",
        "            #print(\"EPISODE\")\n",
        "            coutner = 0 \n",
        "            for t in range(t_max):\n",
        "                pos_x,pos_y = state_change.pos_from_state(state)\n",
        "                #action = act(pos_x,pos_y,state,vitesses,coutner)\n",
        "                action,to_print,espace_dispo_devant,avant,apres,pos_x,pos_y,blurred,counter = act(pos_x,pos_y,state,vitesses,coutner)\n",
        "                #print(to_print)\n",
        "                #print(action,to_print,espace_dispo_devant,avant,apres,pos_x,pos_y,coutner)\n",
        "                #blurred[pos_y,pos_x] = 10\n",
        "                #print(blurred[pos_y-1:pos_y+2,pos_x-5:pos_x+5])\n",
        "                #env.render()\n",
        "                next_state, reward, done, info = env.step(action)\n",
        "                \n",
        "\n",
        "\n",
        "                full_state = {\n",
        "                    'state': state, 'action': action, 'reward': reward, 'next_state': next_state, \n",
        "                    'done': done, 'info': info\n",
        "                }\n",
        "                #agent.update(**full_state)\n",
        "                state = next_state\n",
        "                '''if pos_y == 0 and not(done):\n",
        "                  reward = 10\n",
        "                  done = True'''\n",
        "                \n",
        "                episode_rewards += reward\n",
        "\n",
        "                if done:\n",
        "                    if reward != 10:\n",
        "                      #env.render()\n",
        "                      print(\"action {},espace_dispo_devant {},avant {},apres {},pos_x {},pos_y {}\".format(action,espace_dispo_devant,avant,apres,pos_x,pos_y))\n",
        "                    break\n",
        "            rewards.append(episode_rewards)\n",
        "        avg_rewards = sum(rewards)/len(rewards)\n",
        "        print(\"{} run(s) avg rewards : {:.1f}\".format(runs, avg_rewards))\n",
        "        return avg_rewards\n",
        "\n",
        "    def timed_test(agent,task):\n",
        "        start_time = time.time()\n",
        "        rewards = []\n",
        "        for tc in task['testcases']:\n",
        "            print(\"[{}]\".format(tc['id']), end=' ')\n",
        "            avg_rewards = test(agent, tc['env'], tc['runs'], tc['t_max'])\n",
        "            rewards.append(avg_rewards)\n",
        "        point = sum(rewards)/len(rewards)\n",
        "        elapsed_time = time.time() - start_time\n",
        "\n",
        "        print('Point:', point)\n",
        "        return(point)\n",
        "\n",
        "        for t, remarks in [(0.4, 'fast'), (0.6, 'safe'), (0.8, 'dangerous'), (1.0, 'time limit exceeded')]:\n",
        "            if elapsed_time < task['time_limit'] * t:\n",
        "                print(\"Local runtime: {} seconds --- {}\".format(elapsed_time, remarks))\n",
        "                print(\"WARNING: do note that this might not reflect the runtime on the server.\")\n",
        "                break\n",
        "\n",
        "    def get_task(env=env,runs=300,t1=50,t2=40,t_max= 50):\n",
        "        \n",
        "        tcs = [('task_2_tmax{}'.format(t1), t1), ('task_2_tmax{}'.format(t2), t2)]\n",
        "        return {\n",
        "            'time_limit': 600,\n",
        "            'testcases': [{ 'id': tc, 'env':env, 'runs': runs, 't_max': t_max } for tc, t_max in tcs]\n",
        "        }\n",
        "   \n",
        "    print(\"t1,t2,runs,tmax: {},{},{},{}\".format(t1,t2,runs,t1))\n",
        "    task = get_task(env=env,runs=runs,t1=t1,t2=t2,t_max= t1)\n",
        "    # return scores\n",
        "    return(timed_test(agent,task))\n",
        "env = env_builder.construct_task2_env_ij_lanes(49,9,0,0,0,10)\n",
        "mainTest(None,runs=300,env=env,t1=50,t2=40,cropping=None)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "========================================================================================================================================================================================================\n",
            "  F   -   -   -   -   2   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   6   -   -   -   -   -   -   -   -   O   -   4   5   -   1   3   -   -   -   -   -   -   -   -   -   -   -\n",
            " 12   -   -   9   -   -   -   -   -   -   -   -   -   -   -   -   -  13   8   7   -   -   -   -   -   -   -   -   -   -   -   -   -   -  11   -   -   -   -   -   -  10   -   -   -   -   -  14   -   -\n",
            "  -   -   -   -   -  17   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -  16   -   -  15   -   -   -  20   -   -   -   -   -   -   -   -   -   -   -   -  18  19   -   -   -\n",
            "  -   -   -   -   -   -   -   -   -   -   -  21   -   -   -   -   -   -   -   -   -   -   -  23   -   -   -   -  26   -   -   -   -   -  25   -  24   -   -   -   -   -  22   -   -   -   -   -   -   -\n",
            "  -   -   -  29   -   -   -   -  33   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -  28   -   -   -   -   -   -   -  30   -   -   -  31   -   -   -   -   -   -   -  27  32\n",
            " 37   -   -   -   -   -   -   -   -   -   -   -   -  40   -  35   -   -   -   -   -   -   -   -   -  36   -   -   -   -   -   -   -   -   -  41   -   -   -   -   -  38   -  39   -   -   -   -  34   -\n",
            "  -   -   -   -  46   -   -   -   -   -   -   -   -   -   -   -  43   -   -   -   -   -   -   -   -   -  42   -   -   -   -   -  44   -   -   -   -   -   -   -   -   -   -   -  45   -   -   -   -  47\n",
            "  -   -  53   -  51   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -  48  50   -   -   -   -   -   -   -   -   -   -   -   -   -  49  52  54   -   -   -   -   -   -   -\n",
            "  -   -   -  58  56   -   -  57   -   -   -   -  55  60   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -  59   -   -   -   -   -   -   -   -   -   -   -   -\n",
            "  -   -   -   -   -   -   -   -  61   -   -   -   -   -   -   -  63  65   -   -   -   -   -   -   -   -   -   -   -   -  62  67  64   -   -   -   -   -   -   -   -   -   -  66   -   -   -   -  68   <\n",
            "========================================================================================================================================================================================================\n",
            "device is: cuda\n",
            "t1,t2,runs,tmax: 50,40,300,50\n",
            "[task_2_tmax50] action 2,espace_dispo_devant [],avant 0,apres 2,pos_x 0,pos_y 4\n",
            "action 2,espace_dispo_devant [],avant 0,apres 3,pos_x 0,pos_y 4\n",
            "action 2,espace_dispo_devant [],avant 0,apres 1,pos_x 1,pos_y 2\n",
            "action 2,espace_dispo_devant [],avant 0,apres 2,pos_x 1,pos_y 5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xb4_MhhE7-WS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!zip -r HW3_Task2.zip HW3_Task2\n",
        "#!unzip HW3_Task2_2.zip\n",
        "from HW3_Task2.agent import env as env_builder\n",
        "\n",
        "#env= env_builder.construct_task2_env_ij(49,9)\n",
        "#state = env.reset()\n",
        "\n",
        "env.step(0)\n",
        "env.render()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoWOdlwF0GCV",
        "colab_type": "code",
        "outputId": "711a1727-d1b3-4a74-fc75-ea8b136a7e13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        }
      },
      "source": [
        "from HW3_Task2.agent import models \n",
        "import torch\n",
        "import importlib\n",
        "#Best model 0-3 : m_DQN_10_2.pt\n",
        "#Best model 1-4 : m_DQN_12_2_1_4.pt\n",
        "\n",
        "importlib.reload(models)\n",
        "from HW3_Task2.agent import state_change\n",
        "#importlib.reload(env)\n",
        "\n",
        "\n",
        "\n",
        "def act_multi_head(state, epsilon=0.0,heads=None,heads_config=None,init_head=None,pos_head=None):\n",
        "        \n",
        "        pos_x,pos_y = pos_from_state(state)\n",
        "        \n",
        "        if pos_y <= 2 : \n",
        "          head = 0\n",
        "          if init_head[0]:\n",
        "            pos_head[0] = pos_x\n",
        "            pos_head[1] = pos_y + 1\n",
        "            init_head[0] = False\n",
        "\n",
        "          \n",
        "        if pos_y == 3:\n",
        "          head = 1\n",
        "          if init_head[1]:\n",
        "            pos_head[0] = pos_x\n",
        "            pos_head[1] = pos_y + 1\n",
        "            init_head[1] = False \n",
        "\n",
        "        pos_head_x = pos_head[0]\n",
        "        pos_head_y = pos_head[1]\n",
        "        #Random value  \n",
        "        head_largeur = heads_config[head][0]\n",
        "        head_hauteur = heads_config[head][1]\n",
        "        #Recrop state for current_head \n",
        "        state = state_change.state_from_pos(pos_head_x,pos_head_y,state,largeur=head_largeur,hauteur=head_hauteur)\n",
        "        #print(\"head\",head,pos_head_x,pos_head_y)\n",
        "        #print(\"state\",state)\n",
        "        \n",
        "        if not isinstance(state, torch.FloatTensor):\n",
        "            state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "        \n",
        "      \n",
        "        rand_val = np.random.random()\n",
        "        if rand_val < epsilon:\n",
        "            #Return random action with outpout < num_actions        \n",
        "            \n",
        "            output = np.random.randint(0, self.num_actions)\n",
        "            \n",
        "\n",
        "        else:\n",
        "            #Use forward prediction to choose action \n",
        "          \n",
        "            output = torch.argmax(heads[head].forward(state), dim=1)\n",
        "            if head == 1 and output == 0 and pos_y == 1 :\n",
        "              output = 4\n",
        "            if head == 0 and output == 1 and pos_y == 2 :\n",
        "              output = 4\n",
        "\n",
        "\n",
        "        return int(output),init_head,pos_head\n",
        "\n",
        "\n",
        "def mainTest_multihead(models,runs=25,i1=49,j1=9,cropping=[],x_debut=None,x_fin=0,y_fin=0,env=None,t1=50,t2=40,heads=None,heads_config=None):\n",
        "\n",
        "    import sys\n",
        "    import time\n",
        "\n",
        "\n",
        "    FAST_DOWNWARD_PATH = \"/fast_downward/\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"device is:\",device)\n",
        "\n",
        "    def test(agent, env, runs=1000, t_max=100,x_debut=10,t1=50,t2=40):\n",
        "        rewards = []\n",
        "        print(\"Test: runs {}, x_debut {} t_max {}\".format(runs,x_debut,t_max))\n",
        "        \n",
        "        timing = []\n",
        "        for run in range(runs):\n",
        "            init_head = [True,True]\n",
        "            pos_head = [0,0]\n",
        "            state = env.reset()\n",
        "            #if True:\n",
        "            #  pos_x,pos_y = state_change.pos_from_state(state)\n",
        "            #state = state_change.state_from_pos(pos_x,pos_y,state,largeur=cropping[1],hauteur=cropping[0])\n",
        "            agent_init = {'fast_downward_path': FAST_DOWNWARD_PATH, 'agent_speed_range': (-3,-1), 'gamma' : 1}\n",
        "            #agent.initialize(**agent_init)\n",
        "            episode_rewards = 0.0\n",
        "            for t in range(t_max):\n",
        "                #env.render()\n",
        "                action,init_head,pos_head = act_multi_head(state, epsilon=0.0,heads=heads,heads_config=heads_config,init_head=init_head,pos_head=pos_head)   \n",
        "                next_state, reward, done, info = env.step(action)\n",
        "                \n",
        "                #if cropping:\n",
        "                pos_x,pos_y = state_change.pos_from_state(next_state)\n",
        "                '''\n",
        "                if pos_x > x_debut:\n",
        "                  done = True\n",
        "                  reward  = 0\n",
        "                '''\n",
        "                if pos_y == 0:\n",
        "                  done = True\n",
        "                  timing.append(t)\n",
        "                  reward  = 10\n",
        "\n",
        "                full_state = {\n",
        "                    'state': state, 'action': action, 'reward': reward, 'next_state': next_state, \n",
        "                    'done': done, 'info': info\n",
        "                }\n",
        "                #agent.update(**full_state)\n",
        "                state = next_state\n",
        "                #env.render()\n",
        "                episode_rewards += reward\n",
        "                if done:\n",
        "                    #env.render()\n",
        "                    break\n",
        "            rewards.append(episode_rewards)\n",
        "        avg_rewards = sum(rewards)/len(rewards)\n",
        "        print(\"{} run(s) avg rewards : {:.1f}, timing {}\".format(runs, avg_rewards,np.mean(timing)))\n",
        "        return avg_rewards\n",
        "\n",
        "    def timed_test(agent,task,x_debut=x_debut):\n",
        "        start_time = time.time()\n",
        "        rewards = []\n",
        "        for tc in task['testcases']:\n",
        "            #agent = create_agent(tc['id'])\n",
        "            print(\"[{}]\".format(tc['id']), end=' ')\n",
        "            avg_rewards = test(agent, tc['env'], tc['runs'], tc['t_max'],x_debut=x_debut)\n",
        "            rewards.append(avg_rewards)\n",
        "        point = sum(rewards)/len(rewards)\n",
        "        elapsed_time = time.time() - start_time\n",
        "\n",
        "        print('Point:', point)\n",
        "        return(point)\n",
        "\n",
        "        for t, remarks in [(0.4, 'fast'), (0.6, 'safe'), (0.8, 'dangerous'), (1.0, 'time limit exceeded')]:\n",
        "            if elapsed_time < task['time_limit'] * t:\n",
        "                print(\"Local runtime: {} seconds --- {}\".format(elapsed_time, remarks))\n",
        "                print(\"WARNING: do note that this might not reflect the runtime on the server.\")\n",
        "                break\n",
        "\n",
        "    def get_task(runs=300,env=env,t1=50,t2=40):\n",
        "        tcs = [('task_2_tmax50', t1), ('task_2_tmax40', t2)]\n",
        "        return {\n",
        "            'time_limit': 600,\n",
        "            'testcases': [{ 'id': tc, 'env': env, 'runs': runs, 't_max': t_max } for tc, t_max in tcs]\n",
        "        }\n",
        "        \n",
        "\n",
        "    task = get_task(runs=runs,env=env,t1=t1,t2=t2)\n",
        "    return(timed_test(agent,task,x_debut=x_debut))\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# 15,3 \n",
        "model_0_3 = models.get_model(modelpath='HW3_Task2/saves/m_DQN_10_2.pt',device=device)\n",
        "# 18,3 \n",
        "model_1_4 = models.get_model(modelpath='HW3_Task2/saves/m_DQN_12_2_1_4.pt',device=device)\n",
        "\n",
        "heads = [model_0_3,model_1_4]\n",
        "heads_config = [[15,3],[18,3]]\n",
        "\n",
        "x_debut = 22\n",
        "y_debut = 3\n",
        "x_fin = 0 \n",
        "y_fin = 0 \n",
        "lane_d = 0\n",
        "lane_f = 4\n",
        "\n",
        "env = env_builder.construct_task2_env_lanes(x_debut,y_debut,x_fin,y_fin,lane_d,lane_f)\n",
        "scores = mainTest_multihead(models,runs=1000,x_debut=22,x_fin=0,y_fin=0,env=env,t1=22,t2=int(4*22/5),heads=heads,heads_config=heads_config)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model loaded to device is: cuda\n",
            "model loaded to device is: cuda\n",
            "device is: cuda\n",
            "[task_2_tmax50] Test: runs 1000, x_debut 22 t_max 22\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-336-2772ba7b0a21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_builder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstruct_task2_env_lanes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_debut\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_debut\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_fin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_fin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlane_d\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlane_f\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmainTest_multihead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mruns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_debut\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m22\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_fin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_fin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m22\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m22\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheads_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheads_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-336-2772ba7b0a21>\u001b[0m in \u001b[0;36mmainTest_multihead\u001b[0;34m(models, runs, i1, j1, cropping, x_debut, x_fin, y_fin, env, t1, t2, heads, heads_config)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0mtask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mruns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mruns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimed_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_debut\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_debut\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-336-2772ba7b0a21>\u001b[0m in \u001b[0;36mtimed_test\u001b[0;34m(agent, task, x_debut)\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0;31m#agent = create_agent(tc['id'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[{}]\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0mavg_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'env'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'runs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m't_max'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_debut\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_debut\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m             \u001b[0mrewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_rewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-336-2772ba7b0a21>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(agent, env, runs, t_max, x_debut, t1, t2)\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0;31m#env.render()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minit_head\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpos_head\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mact_multi_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheads_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheads_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minit_head\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minit_head\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpos_head\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpos_head\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                 \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m                 \u001b[0;31m#if cropping:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym_grid_driving/envs/grid_driving.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action, state)\u001b[0m\n\u001b[1;32m    608\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym_grid_driving/envs/grid_driving.py\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    601\u001b[0m             \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFINISH_REWARD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym_grid_driving/envs/grid_driving.py\u001b[0m in \u001b[0;36mupdate_state\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    664\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tensor'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 666\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    667\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'vector'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym_grid_driving/envs/grid_driving.py\u001b[0m in \u001b[0;36mtensor_state\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    466\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtensor_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym_grid_driving/envs/grid_driving.py\u001b[0m in \u001b[0;36mas_tensor\u001b[0;34m(self, pytorch)\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpytorch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# [C, H, W]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor_space\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpytorch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym_grid_driving/envs/grid_driving.py\u001b[0m in \u001b[0;36mtensor_space\u001b[0;34m(self, pytorch, channel)\u001b[0m\n\u001b[1;32m    418\u001b[0m         \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0mtensor_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpytorch\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mspaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhigh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mchannel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvector_space\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspeed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrail\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/spaces/box.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, low, high, shape, dtype)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbounded_above\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minf\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhigh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mis_bounded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmanner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"both\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/spaces/space.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, shape, dtype)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp_random\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/spaces/space.py\u001b[0m in \u001b[0;36mseed\u001b[0;34m(self, seed)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;34m\"\"\"Seed the PRNG of this space. \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseeding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/utils/seeding.py\u001b[0m in \u001b[0;36mnp_random\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mrng\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mrng\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_int_list_from_bigint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhash_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.__init__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m_mt19937.pyx\u001b[0m in \u001b[0;36mnumpy.random._mt19937.MT19937.__init__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recreate_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/_ufunc_config.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moldstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseterr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    443\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_Unspecified\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moldcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseterrcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/_ufunc_config.py\u001b[0m in \u001b[0;36mseterr\u001b[0;34m(all, divide, over, under, invalid)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0mpyvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mumath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeterrobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m     \u001b[0mold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgeterr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdivide\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/_ufunc_config.py\u001b[0m in \u001b[0;36mgeterr\u001b[0;34m()\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmaskvalue\u001b[0m \u001b[0;34m>>\u001b[0m \u001b[0mSHIFT_UNDERFLOW\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'under'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_errdict_rev\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m     \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmaskvalue\u001b[0m \u001b[0;34m>>\u001b[0m \u001b[0mSHIFT_INVALID\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m     \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'invalid'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_errdict_rev\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfaeFp3u_6Yh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from HW3_Task2.agent import models \n",
        "from HW3_Task2.agent import agent \n",
        "from HW3_Task2.agent import env as env_builder\n",
        "import importlib \n",
        "importlib.reload(models)\n",
        "importlib.reload(agent)\n",
        "importlib.reload(env_builder)\n",
        "try:\n",
        "  importlib.reload(models)\n",
        "  importlib.reload(agent)\n",
        "  importlib.reload(env)\n",
        "except:pass\n",
        "Config = collections.namedtuple('Config', ('x_debut', 'y_debut', 'max_episode', 'max_epsilon', 'epsilon_decay', 'test_interval','save_interval','batch_size','buffer_limit','methode','gamma_nstep','nstep'))\n",
        "config_l=[]\n",
        "last_y = 2\n",
        "for i in range(0,9):\n",
        "  x_debut = 10+i*5\n",
        "  \n",
        "\n",
        "  for j in range(0,last_y):\n",
        "    time = i + j \n",
        "    \n",
        "    y_debut = j\n",
        "    max_episode = 250*(i+1)+250*(j)\n",
        "\n",
        "    # max_epsilon decrease with time \n",
        "    max_epsilon = 1.0 / (time+1)\n",
        "    epsilon_decay = max_episode // 8\n",
        "    test_interval = min(500,max_episode//2)\n",
        "    save_interval = 2000\n",
        "    batch_size = 32\n",
        "    #buffer_limit = min(10000,2000*(time+1))\n",
        "    buffer_limit = 8000\n",
        "    methode = 'Mixed Monte Carlo + DQN'\n",
        "    gamma_nstep  = 0.5\n",
        "    nstep = 3\n",
        "    config_cur = Config(x_debut,y_debut,max_episode,max_epsilon,epsilon_decay,test_interval,save_interval,batch_size,buffer_limit,methode,gamma_nstep,nstep)\n",
        "    \n",
        "    env = env_builder.construct_task2_env_ij(x_debut,y_debut)\n",
        "    \n",
        "    if i == 0 and j == 0:\n",
        "      model = models.train(agent.ConvDQN, env=env,pretrain=False,model_p= None,savepath='HW3_Task2/saves/m_',config=config_cur)\n",
        "    else:\n",
        "      model = models.train(agent.ConvDQN, env=env,pretrain=True,model_p= model,savepath='HW3_Task2/saves/m_',config=config_cur)\n",
        "\n",
        "\n",
        "  last_y += 1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Rbvg4vCI9nE",
        "colab_type": "code",
        "outputId": "b1401bbe-fe5a-40b1-b8cd-6edeffd6a401",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        }
      },
      "source": [
        "# Dependencies\n",
        "\n",
        "!pip install torch numpy git+https://github.com/cs4246/gym-grid-driving.git\n",
        "!pip install -U -q PyDrive"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/cs4246/gym-grid-driving.git\n",
            "  Cloning https://github.com/cs4246/gym-grid-driving.git to /tmp/pip-req-build-eds5b98s\n",
            "  Running command git clone -q https://github.com/cs4246/gym-grid-driving.git /tmp/pip-req-build-eds5b98s\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.18.3)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (from gym-grid-driving==0.0.1) (0.17.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym->gym-grid-driving==0.0.1) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym->gym-grid-driving==0.0.1) (1.5.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym->gym-grid-driving==0.0.1) (1.12.0)\n",
            "Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym->gym-grid-driving==0.0.1) (1.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym->gym-grid-driving==0.0.1) (0.16.0)\n",
            "Building wheels for collected packages: gym-grid-driving\n",
            "  Building wheel for gym-grid-driving (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym-grid-driving: filename=gym_grid_driving-0.0.1-cp36-none-any.whl size=8623 sha256=a0a81c613c524f249adac2f3beb043d74a6d444d2a311e21e231cb8f8dc27067\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-g_pau3_j/wheels/e1/30/f2/157c0938ab9bfe9c10c29c9fcab8392f587c9d141f215b67ca\n",
            "Successfully built gym-grid-driving\n",
            "Installing collected packages: gym-grid-driving\n",
            "Successfully installed gym-grid-driving-0.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8v5zMQtQ8pvl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgYsRl_buv89",
        "colab_type": "code",
        "outputId": "c8596c68-2fc4-4a50-d443-2e5089062f48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 840
        }
      },
      "source": [
        "!unzip HW3_Task2\\ \\(9\\).zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  HW3_Task2 (9).zip\n",
            "   creating: HW3_Task2/\n",
            "  inflating: HW3_Task2/.DS_Store     \n",
            " extracting: HW3_Task2/untitled      \n",
            "   creating: HW3_Task2/agent/\n",
            "  inflating: HW3_Task2/agent/agent.py  \n",
            "  inflating: HW3_Task2/agent/models.py  \n",
            "   creating: HW3_Task2/agent/__pycache__/\n",
            "  inflating: HW3_Task2/agent/__pycache__/env.cpython-36.pyc  \n",
            "  inflating: HW3_Task2/agent/__pycache__/agent.cpython-36.pyc  \n",
            "  inflating: HW3_Task2/agent/__pycache__/models.cpython-36.pyc  \n",
            "  inflating: HW3_Task2/agent/__pycache__/state_change.cpython-36.pyc  \n",
            "  inflating: HW3_Task2/agent/__pycache__/buffer_src.cpython-36.pyc  \n",
            "  inflating: HW3_Task2/agent/__pycache__/loss_src.cpython-36.pyc  \n",
            "  inflating: HW3_Task2/agent/__pycache__/__init__.cpython-36.pyc  \n",
            "  inflating: HW3_Task2/agent/buffer_src.py  \n",
            "   creating: HW3_Task2/agent/.ipynb_checkpoints/\n",
            "  inflating: HW3_Task2/agent/env.py  \n",
            "  inflating: HW3_Task2/agent/loss_src.py  \n",
            "  inflating: HW3_Task2/agent/state_change.py  \n",
            "  inflating: HW3_Task2/agent/__init__.py  \n",
            "  inflating: HW3_Task2/agent/loss.py  \n",
            "   creating: HW3_Task2/.ipynb_checkpoints/\n",
            "   creating: HW3_Task2/saves/\n",
            "  inflating: HW3_Task2/saves/m_DQN_12_2_0_3_f.pt  \n",
            "  inflating: HW3_Task2/saves/m__history_lastcb.pt  \n",
            "  inflating: HW3_Task2/saves/m_DQN_20_2_0_3_f.pt  \n",
            "  inflating: HW3_Task2/saves/m__cb_2000_20_2.pt  \n",
            "  inflating: HW3_Task2/saves/m_DQN_10_2_2_5_f.pt  \n",
            "  inflating: HW3_Task2/saves/m_DQN_10_2_1_4_f.pt  \n",
            "  inflating: HW3_Task2/saves/m__cb_2000_12_2.pt  \n",
            "  inflating: HW3_Task2/saves/m__cb_2000_25_2.pt  \n",
            "  inflating: HW3_Task2/saves/m_DQN_25_2_0_3_f.pt  \n",
            "  inflating: HW3_Task2/saves/m__cb_15_1.pt  \n",
            "  inflating: HW3_Task2/saves/m_DQN_10_2_0_3_f.pt  \n",
            "  inflating: HW3_Task2/saves/m__cb_12_2.pt  \n",
            "  inflating: HW3_Task2/saves/m__cb_2000_10_2.pt  \n",
            "   creating: HW3_Task2/models_finaux/\n",
            "  inflating: HW3_Task2/models_finaux/m_DQN_10_2_2_5_f.pt  \n",
            "  inflating: HW3_Task2/models_finaux/m_DQN_10_2_5_8_f.pt  \n",
            "  inflating: HW3_Task2/models_finaux/m_DQN_10_2_1_4_f.pt  \n",
            "  inflating: HW3_Task2/models_finaux/m_DQN_10_2_6_9_f.pt  \n",
            "  inflating: HW3_Task2/models_finaux/m_DQN_10_2_4_7_f.pt  \n",
            "  inflating: HW3_Task2/models_finaux/m_DQN_10_2_0_3_f.pt  \n",
            "  inflating: HW3_Task2/models_finaux/m_DQN_10_2_7_10_f.pt  \n",
            "  inflating: HW3_Task2/models_finaux/m_DQN_10_2_3_6_f.pt  \n",
            "  inflating: HW3_Task2/setup.py      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXjaFAhwkQPw",
        "colab_type": "code",
        "outputId": "21048fae-c991-4f36-b270-c3b225ee304d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 647
        }
      },
      "source": [
        "!zip -r HW3_Task_folder.zip HW3_Task2"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: HW3_Task2/ (stored 0%)\n",
            "  adding: HW3_Task2/untitled (stored 0%)\n",
            "  adding: HW3_Task2/saves/ (stored 0%)\n",
            "  adding: HW3_Task2/saves/m_DQN_10_3.pt (deflated 7%)\n",
            "  adding: HW3_Task2/saves/m_DQN_2000_12_4.pt (deflated 6%)\n",
            "  adding: HW3_Task2/saves/m_DQN_12_4.pt (deflated 6%)\n",
            "  adding: HW3_Task2/saves/m_DQN_2000_14_4.pt (deflated 7%)\n",
            "  adding: HW3_Task2/saves/m_DQN_2000_12_2.pt (deflated 6%)\n",
            "  adding: HW3_Task2/saves/m__cb_10_3.pt (deflated 7%)\n",
            "  adding: HW3_Task2/saves/m_DQN_12_2_1_4.pt (deflated 6%)\n",
            "  adding: HW3_Task2/saves/m_DQN_2000_10_2.pt (deflated 7%)\n",
            "  adding: HW3_Task2/saves/m__cb_2000_10_3.pt (deflated 7%)\n",
            "  adding: HW3_Task2/saves/m_DQN_10_2.pt (deflated 7%)\n",
            "  adding: HW3_Task2/saves/m__cb_15_1.pt (deflated 7%)\n",
            "  adding: HW3_Task2/saves/m_DQN_2000_10_3.pt (deflated 6%)\n",
            "  adding: HW3_Task2/saves/m__history_lastcb.pt (deflated 54%)\n",
            "  adding: HW3_Task2/.ipynb_checkpoints/ (stored 0%)\n",
            "  adding: HW3_Task2/agent/ (stored 0%)\n",
            "  adding: HW3_Task2/agent/state_change.py (deflated 62%)\n",
            "  adding: HW3_Task2/agent/__init__.py (deflated 61%)\n",
            "  adding: HW3_Task2/agent/buffer_src.py (deflated 67%)\n",
            "  adding: HW3_Task2/agent/agent.py (deflated 72%)\n",
            "  adding: HW3_Task2/agent/__pycache__/ (stored 0%)\n",
            "  adding: HW3_Task2/agent/__pycache__/__init__.cpython-36.pyc (deflated 41%)\n",
            "  adding: HW3_Task2/agent/__pycache__/models.cpython-36.pyc (deflated 45%)\n",
            "  adding: HW3_Task2/agent/__pycache__/state_change.cpython-36.pyc (deflated 40%)\n",
            "  adding: HW3_Task2/agent/__pycache__/loss_src.cpython-36.pyc (deflated 39%)\n",
            "  adding: HW3_Task2/agent/__pycache__/buffer_src.cpython-36.pyc (deflated 50%)\n",
            "  adding: HW3_Task2/agent/__pycache__/agent.cpython-36.pyc (deflated 51%)\n",
            "  adding: HW3_Task2/agent/__pycache__/env.cpython-36.pyc (deflated 58%)\n",
            "  adding: HW3_Task2/agent/loss.py (deflated 61%)\n",
            "  adding: HW3_Task2/agent/.ipynb_checkpoints/ (stored 0%)\n",
            "  adding: HW3_Task2/agent/env.py (deflated 87%)\n",
            "  adding: HW3_Task2/agent/models.py (deflated 71%)\n",
            "  adding: HW3_Task2/agent/loss_src.py (deflated 62%)\n",
            "  adding: HW3_Task2/setup.py (deflated 27%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjrdeWB-JA1x",
        "colab_type": "code",
        "outputId": "8f37c046-74cf-4585-bc74-9944152e1904",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        }
      },
      "source": [
        "# Authentification / Initialization of workspace \n",
        "\n",
        "import os\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# Create repo folders \n",
        "\n",
        "task_1 = False\n",
        "# If False task_2 workplace will be downloaded\n",
        "\n",
        "if task_1:\n",
        "  number_task = 1\n",
        "else:\n",
        "  number_task = 2\n",
        "\n",
        "root = '/content'\n",
        "local_download = os.path.join(root,'HW3_Task{}'.format(number_task))\n",
        "\n",
        "if not(os.path.exists(local_download)): \n",
        "  os.mkdir(local_download)\n",
        "\n",
        "local_download_agent = os.path.join(local_download,'agent')\n",
        "\n",
        "if not(os.path.exists(local_download_agent)): \n",
        "  os.mkdir(local_download_agent)\n",
        "\n",
        "  # Download files \n",
        "\n",
        "\n",
        "def download_list(file_list,path_name):\n",
        "    \n",
        "  error_l = [];\n",
        "    \n",
        "  for f in file_list:\n",
        "    # 3. Create & download by id.\n",
        "  \n",
        "    print('file found : title: %s, id: %s' % (f['title'], f['id']))\n",
        "    try:\n",
        "      #print('title: %s, id: %s' % (f['title'], f['id']))\n",
        "      fname = os.path.join(path_name, f['title'])\n",
        "      \n",
        "      # Download only .py files\n",
        "      if fname[-3:] == \".py\":\n",
        "        print('downloading to {}'.format(fname))\n",
        "        f_ = drive.CreateFile({'id': f['id']})\n",
        "        f_.GetContentFile(fname)\n",
        "      \n",
        "    except:\n",
        "      print(\"there is an error\")          \n",
        "      error_l.append(fname)\n",
        "\n",
        "\n",
        "if task_1:\n",
        "  # Initial folder\n",
        "  local_download_path = local_download_agent\n",
        "\n",
        "  # Agent files \n",
        "  file_list = drive.ListFile(\n",
        "        {'q': \"'1ktZR8KIIWEi8Cre92SFomSEchWleHtSu' in parents\"}).GetList()\n",
        "\n",
        "  download_list(file_list,local_download_path)\n",
        "\n",
        "\n",
        "  # Initial files\n",
        "  local_download_path = local_download\n",
        "\n",
        "  file_list = drive.ListFile(\n",
        "        {'q': \"'1x4sJIKHA6NZ5y78AnOJeCZP8abI4gLLO' in parents\"}).GetList()\n",
        "\n",
        "  download_list(file_list,local_download_path)\n",
        "\n",
        "else:\n",
        "\n",
        "  # Initial folder\n",
        "  local_download_path = local_download_agent\n",
        "\n",
        "  # Agent files \n",
        "  file_list = drive.ListFile(\n",
        "        {'q': \"'1PUrHkG6ki4JsiXtKOtvc2vAuR6IJMnCa' in parents\"}).GetList()\n",
        "\n",
        "  download_list(file_list,local_download_path)\n",
        "\n",
        "\n",
        "  # Initial files\n",
        "  local_download_path = local_download\n",
        "\n",
        "  file_list = drive.ListFile(\n",
        "        {'q': \"'17wsroYnRmoTt-OIzDJRrUNvVFmohBVFR' in parents\"}).GetList()\n",
        "\n",
        "  download_list(file_list,local_download_path)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "file found : title: state_change.py, id: 1NUFRvldgt6dRkjycttkz11dmY56cRIEz\n",
            "downloading to /content/HW3_Task2/agent/state_change.py\n",
            "file found : title: models.py, id: 1BW6dbcbWvmC7JbqE78269K1IXIPwtlEi\n",
            "downloading to /content/HW3_Task2/agent/models.py\n",
            "file found : title: __pycache__, id: 1OfJrkmLCYqUS8vVjJnxgeZCvQEnQSQ4J\n",
            "file found : title: env.py, id: 1bYr6NdzVOXe77n-gUac3VdGA8tnpS8HJ\n",
            "downloading to /content/HW3_Task2/agent/env.py\n",
            "file found : title: __init__.py, id: 1WO1O3RwqCpk7XdN07UUZbF6YTaj5mGGg\n",
            "downloading to /content/HW3_Task2/agent/__init__.py\n",
            "file found : title: model_last_20_05_actionR_cb_.textClipping, id: 1aESmsKWtCbxwgAAqKV9wP1eE4xDOCBFp\n",
            "file found : title: model_last_20_05_actionR_cb_8_5000.pt, id: 1FdLXoWHZQz8l5hS76ygQJXe2HWa-wLJI\n",
            "file found : title: model.pt, id: 1_hr_fysoSC6AXix1Qjqq0bBYk0KiSIcq\n",
            "file found : title: .ipynb_checkpoints, id: 14rimDklDDBO4_OqcqfbiybhzNMxQQFSm\n",
            "file found : title: ZZZ.zip, id: 1CVWs9pxeQ6FieIRMszIIfNK1my32N3Wl\n",
            "file found : title: ZZZ, id: 10kHEkPFSjAINnDqi2s5KF7PxQN9bImk3\n",
            "file found : title: try2 2, id: 1Ka1yYoo2aGsxPITkqKfBYB4AAHln0aMZ\n",
            "file found : title: try2.zip, id: 1o5hCQ_s6UkmcbRNTkR8crcNcftp1IhZ7\n",
            "file found : title: try2 2.zip, id: 1R8lmVqVxgvfuQ150WK4ZckNmfjvWmvW4\n",
            "file found : title: try2, id: 1T1A0dN0enM4yy8DmA6A7HJOFyGQS0IlX\n",
            "file found : title: A0212190W_Task2.zip, id: 1XJ9tRmPvxju3IjD76WkmINyUlYTIJvVm\n",
            "file found : title: agent, id: 1PUrHkG6ki4JsiXtKOtvc2vAuR6IJMnCa\n",
            "file found : title: setup.py, id: 1GrxqesqYqNJit3lNQABi9n3Wisa5zuir\n",
            "downloading to /content/HW3_Task2/setup.py\n",
            "file found : title: MANIFEST.in, id: 16_Iy5dYGYMUP2T9hl-OZdODIhV7dbgCf\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D--g2rvTwbSs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python HW3_Task2/rl/actor_critic.py --train --path=HW3_Task2/rl/ac_cb_6_19_13_33_0.pt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbI0Tym5iGWW",
        "colab_type": "code",
        "outputId": "1cb40226-a7f7-4e97-c69e-42037b387ee2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# TASK 2\n",
        "\n",
        "import sys\n",
        "import time\n",
        "\n",
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "date_save = time.strftime(\"%d %H %M\")\n",
        "model_path = os.path.join('HW3_Task2','agent', 'model_{}_{}.pt'.format(date_save,2))\n",
        "save_path = os.path.join('HW3_Task2','agent', 'model_{}_{}.pt'.format(date_save,3))\n",
        "\n",
        "duree = 0 \n",
        "nb_bad_init = 0 \n",
        "# Try for 10 diferent inits\n",
        "\n",
        "for i in range(0,1):\n",
        "  debut = time.time()\n",
        "  # Try an init \n",
        "  #!python3 HW3_Task2/agent/models.py --test --path=HW3_Task2/agent/model_last1_20_05_cb_8_1000.pt --savepath=HW3_Task2/agent/model_last1_20_05\n",
        "  #!python3 HW3_Task2/agent/models.py --pretrain --path=HW3_Task2/agent/model_last_20_05_cb_8.pt --savepath=HW3_Task2/agent/model_last_20_05\n",
        "  !python3 HW3_Task2/agent/models.py --train --path=HW3_Task2/agent/model_last_20_05_cb_8.pt --savepath=HW3_Task2/agent/model_last_21_05\n",
        "  #!python3 HW3_Task2/agent/testenv.py\n",
        "  duree = (time.time()-debut)\n",
        "  # If good init exit loop\n",
        "  print(duree)\n",
        "  if duree > 10000:\n",
        "    break\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n",
            "pretrain mode\n",
            "model loaded from HW3_Task2/agent/model_last_20_05_cb_8.pt\n",
            "ConvDQN(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(4, 32, kernel_size=(2, 2), stride=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): Conv2d(32, 64, kernel_size=(2, 2), stride=(1, 1))\n",
            "    (3): ReLU()\n",
            "  )\n",
            "  (layers): Sequential(\n",
            "    (0): Linear(in_features=24576, out_features=256, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=256, out_features=5, bias=True)\n",
            "  )\n",
            ")\n",
            "range change : x_min 49 x_max 49\n",
            "environment change : i_1 49 j_1 9\n",
            "[Episode 100]\t rewards globals : 0.891 \tavg rewards : 0.891,\tavg loss: : 0.329282,\tbuffer size : 2353,\tepsilon : 14.0%, \t r <=40 0.0, \t r > 40 8.91089108910891\n",
            "[Episode 200]\t rewards globals : 1.940 \tavg rewards : 2.970,\tavg loss: : 0.324356,\tbuffer size : 4000,\tepsilon : 5.8%, \t r <=40 0.0, \t r > 40 29.7029702970297\n",
            "[Episode 300]\t rewards globals : 2.558 \tavg rewards : 3.762,\tavg loss: : 0.329656,\tbuffer size : 4000,\tepsilon : 2.8%, \t r <=40 0.0, \t r > 40 37.62376237623762\n",
            "[Episode 400]\t rewards globals : 3.242 \tavg rewards : 5.347,\tavg loss: : 0.324906,\tbuffer size : 4000,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 53.46534653465347\n",
            "environment change : i_1 49 j_1 9\n",
            "Test mode, runs: 100, i: 49, j: 9, final: True\n",
            "[task_2_tmax50] 100 run(s) avg rewards : 6.9\n",
            "[task_2_tmax40] 100 run(s) avg rewards : 0.0\n",
            "Point: 3.45\n",
            "Local runtime: 88.78776741027832 seconds --- fast\n",
            "WARNING: do note that this might not reflect the runtime on the server.\n",
            "[Episode 500]\t rewards globals : 3.832 \tavg rewards : 6.139,\tavg loss: : 0.324121,\tbuffer size : 4000,\tepsilon : 36.2%, \t r <=40 0.0, \t r > 40 61.386138613861384\n",
            "[Episode 600]\t rewards globals : 3.378 \tavg rewards : 1.089,\tavg loss: : 0.324359,\tbuffer size : 4000,\tepsilon : 14.0%, \t r <=40 0.0, \t r > 40 10.891089108910892\n",
            "[Episode 700]\t rewards globals : 3.338 \tavg rewards : 3.069,\tavg loss: : 0.317103,\tbuffer size : 4000,\tepsilon : 5.8%, \t r <=40 0.0, \t r > 40 30.693069306930692\n",
            "[Episode 800]\t rewards globals : 3.446 \tavg rewards : 4.257,\tavg loss: : 0.312053,\tbuffer size : 4000,\tepsilon : 2.8%, \t r <=40 0.0, \t r > 40 42.57425742574257\n",
            "[Episode 900]\t rewards globals : 3.818 \tavg rewards : 6.733,\tavg loss: : 0.306261,\tbuffer size : 4000,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 67.32673267326733\n",
            "environment change : i_1 49 j_1 9\n",
            "Test mode, runs: 100, i: 49, j: 9, final: True\n",
            "[task_2_tmax50] 100 run(s) avg rewards : 7.7\n",
            "[task_2_tmax40] 100 run(s) avg rewards : 0.0\n",
            "Point: 3.85\n",
            "Local runtime: 87.98091053962708 seconds --- fast\n",
            "WARNING: do note that this might not reflect the runtime on the server.\n",
            "[Episode 1000]\t rewards globals : 4.146 \tavg rewards : 7.129,\tavg loss: : 0.300492,\tbuffer size : 4000,\tepsilon : 36.2%, \t r <=40 0.0, \t r > 40 71.28712871287128\n",
            "[Episode 1100]\t rewards globals : 3.878 \tavg rewards : 1.188,\tavg loss: : 0.295383,\tbuffer size : 4000,\tepsilon : 14.0%, \t r <=40 0.0, \t r > 40 11.881188118811881\n",
            "[Episode 1200]\t rewards globals : 3.797 \tavg rewards : 2.871,\tavg loss: : 0.294049,\tbuffer size : 4000,\tepsilon : 5.8%, \t r <=40 0.0, \t r > 40 28.71287128712871\n",
            "[Episode 1300]\t rewards globals : 3.928 \tavg rewards : 5.446,\tavg loss: : 0.294026,\tbuffer size : 4000,\tepsilon : 2.8%, \t r <=40 0.0, \t r > 40 54.45544554455446\n",
            "[Episode 1400]\t rewards globals : 4.083 \tavg rewards : 6.040,\tavg loss: : 0.293010,\tbuffer size : 4000,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 60.396039603960396\n",
            "environment change : i_1 49 j_1 9\n",
            "Test mode, runs: 100, i: 49, j: 9, final: True\n",
            "[task_2_tmax50] 100 run(s) avg rewards : 6.5\n",
            "[task_2_tmax40] 100 run(s) avg rewards : 0.2\n",
            "Point: 3.35\n",
            "Local runtime: 85.66874504089355 seconds --- fast\n",
            "WARNING: do note that this might not reflect the runtime on the server.\n",
            "[Episode 1500]\t rewards globals : 4.184 \tavg rewards : 5.644,\tavg loss: : 0.293054,\tbuffer size : 4000,\tepsilon : 36.2%, \t r <=40 0.0, \t r > 40 56.43564356435643\n",
            "[Episode 1600]\t rewards globals : 4.072 \tavg rewards : 2.376,\tavg loss: : 0.291943,\tbuffer size : 4000,\tepsilon : 14.0%, \t r <=40 0.0, \t r > 40 23.762376237623762\n",
            "[Episode 1700]\t rewards globals : 4.039 \tavg rewards : 3.465,\tavg loss: : 0.289170,\tbuffer size : 4000,\tepsilon : 5.8%, \t r <=40 0.0, \t r > 40 34.65346534653465\n",
            "[Episode 1800]\t rewards globals : 4.098 \tavg rewards : 5.149,\tavg loss: : 0.288775,\tbuffer size : 4000,\tepsilon : 2.8%, \t r <=40 0.0, \t r > 40 51.48514851485149\n",
            "[Episode 1900]\t rewards globals : 4.203 \tavg rewards : 6.139,\tavg loss: : 0.285435,\tbuffer size : 4000,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 61.386138613861384\n",
            "environment change : i_1 49 j_1 9\n",
            "Test mode, runs: 100, i: 49, j: 9, final: True\n",
            "[task_2_tmax50] 100 run(s) avg rewards : 7.2\n",
            "[task_2_tmax40] 100 run(s) avg rewards : 0.2\n",
            "Point: 3.7\n",
            "Local runtime: 87.72738099098206 seconds --- fast\n",
            "WARNING: do note that this might not reflect the runtime on the server.\n",
            "[Episode 2000]\t rewards globals : 4.293 \tavg rewards : 6.040,\tavg loss: : 0.282827,\tbuffer size : 4000,\tepsilon : 36.2%, \t r <=40 0.0, \t r > 40 60.396039603960396\n",
            "[Episode 2100]\t rewards globals : 4.136 \tavg rewards : 0.990,\tavg loss: : 0.281210,\tbuffer size : 4000,\tepsilon : 14.0%, \t r <=40 0.0, \t r > 40 9.900990099009901\n",
            "[Episode 2200]\t rewards globals : 4.094 \tavg rewards : 3.267,\tavg loss: : 0.280792,\tbuffer size : 4000,\tepsilon : 5.8%, \t r <=40 0.0, \t r > 40 32.67326732673268\n",
            "[Episode 2300]\t rewards globals : 4.155 \tavg rewards : 5.545,\tavg loss: : 0.280727,\tbuffer size : 4000,\tepsilon : 2.8%, \t r <=40 0.0, \t r > 40 55.44554455445545\n",
            "[Episode 2400]\t rewards globals : 4.257 \tavg rewards : 6.634,\tavg loss: : 0.281182,\tbuffer size : 4000,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 66.33663366336634\n",
            "environment change : i_1 49 j_1 9\n",
            "Test mode, runs: 100, i: 49, j: 9, final: True\n",
            "[task_2_tmax50] 100 run(s) avg rewards : 6.8\n",
            "[task_2_tmax40] 100 run(s) avg rewards : 0.4\n",
            "Point: 3.6\n",
            "Local runtime: 84.96619939804077 seconds --- fast\n",
            "WARNING: do note that this might not reflect the runtime on the server.\n",
            "[Episode 2500]\t rewards globals : 4.354 \tavg rewards : 6.634,\tavg loss: : 0.280225,\tbuffer size : 4000,\tepsilon : 36.2%, \t r <=40 0.0, \t r > 40 66.33663366336634\n",
            "[Episode 2600]\t rewards globals : 4.252 \tavg rewards : 1.683,\tavg loss: : 0.278948,\tbuffer size : 4000,\tepsilon : 14.0%, \t r <=40 0.0, \t r > 40 16.831683168316832\n",
            "[Episode 2700]\t rewards globals : 4.239 \tavg rewards : 3.861,\tavg loss: : 0.278128,\tbuffer size : 4000,\tepsilon : 5.8%, \t r <=40 0.0, \t r > 40 38.613861386138616\n",
            "[Episode 2800]\t rewards globals : 4.302 \tavg rewards : 6.040,\tavg loss: : 0.277898,\tbuffer size : 4000,\tepsilon : 2.8%, \t r <=40 0.0, \t r > 40 60.396039603960396\n",
            "[Episode 2900]\t rewards globals : 4.354 \tavg rewards : 5.743,\tavg loss: : 0.278218,\tbuffer size : 4000,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 57.42574257425742\n",
            "environment change : i_1 49 j_1 9\n",
            "Test mode, runs: 100, i: 49, j: 9, final: True\n",
            "[task_2_tmax50] 100 run(s) avg rewards : 7.2\n",
            "[task_2_tmax40] 100 run(s) avg rewards : 1.1\n",
            "Point: 4.15\n",
            "Local runtime: 83.96934413909912 seconds --- fast\n",
            "WARNING: do note that this might not reflect the runtime on the server.\n",
            "[Episode 3000]\t rewards globals : 4.409 \tavg rewards : 5.941,\tavg loss: : 0.278409,\tbuffer size : 4000,\tepsilon : 36.2%, \t r <=40 0.0, \t r > 40 59.4059405940594\n",
            "[Episode 3100]\t rewards globals : 4.318 \tavg rewards : 1.584,\tavg loss: : 0.277450,\tbuffer size : 4000,\tepsilon : 14.0%, \t r <=40 0.0, \t r > 40 15.841584158415841\n",
            "[Episode 3200]\t rewards globals : 4.283 \tavg rewards : 3.168,\tavg loss: : 0.276213,\tbuffer size : 4000,\tepsilon : 5.8%, \t r <=40 0.0, \t r > 40 31.683168316831683\n",
            "[Episode 3300]\t rewards globals : 4.268 \tavg rewards : 3.762,\tavg loss: : 0.275639,\tbuffer size : 4000,\tepsilon : 2.8%, \t r <=40 0.0, \t r > 40 37.62376237623762\n",
            "[Episode 3400]\t rewards globals : 4.296 \tavg rewards : 5.149,\tavg loss: : 0.275082,\tbuffer size : 4000,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 51.48514851485149\n",
            "environment change : i_1 49 j_1 9\n",
            "Test mode, runs: 100, i: 49, j: 9, final: True\n",
            "[task_2_tmax50] 100 run(s) avg rewards : 6.6\n",
            "[task_2_tmax40] 100 run(s) avg rewards : 1.5\n",
            "Point: 4.05\n",
            "Local runtime: 82.06571173667908 seconds --- fast\n",
            "WARNING: do note that this might not reflect the runtime on the server.\n",
            "[Episode 3500]\t rewards globals : 4.347 \tavg rewards : 6.040,\tavg loss: : 0.273648,\tbuffer size : 4000,\tepsilon : 36.2%, \t r <=40 0.0, \t r > 40 60.396039603960396\n",
            "[Episode 3600]\t rewards globals : 4.274 \tavg rewards : 1.683,\tavg loss: : 0.272580,\tbuffer size : 4000,\tepsilon : 14.0%, \t r <=40 0.0, \t r > 40 16.831683168316832\n",
            "[Episode 3700]\t rewards globals : 4.229 \tavg rewards : 2.574,\tavg loss: : 0.271710,\tbuffer size : 4000,\tepsilon : 5.8%, \t r <=40 0.0, \t r > 40 25.742574257425744\n",
            "[Episode 3800]\t rewards globals : 4.262 \tavg rewards : 5.446,\tavg loss: : 0.272302,\tbuffer size : 4000,\tepsilon : 2.8%, \t r <=40 0.0, \t r > 40 54.45544554455446\n",
            "[Episode 3900]\t rewards globals : 4.301 \tavg rewards : 5.842,\tavg loss: : 0.273048,\tbuffer size : 4000,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 58.415841584158414\n",
            "environment change : i_1 49 j_1 9\n",
            "Test mode, runs: 100, i: 49, j: 9, final: True\n",
            "[task_2_tmax50] 100 run(s) avg rewards : 6.7\n",
            "[task_2_tmax40] 100 run(s) avg rewards : 0.2\n",
            "Point: 3.45\n",
            "Local runtime: 83.58305191993713 seconds --- fast\n",
            "WARNING: do note that this might not reflect the runtime on the server.\n",
            "[Episode 4000]\t rewards globals : 4.341 \tavg rewards : 5.842,\tavg loss: : 0.273182,\tbuffer size : 4000,\tepsilon : 36.2%, \t r <=40 0.0, \t r > 40 58.415841584158414\n",
            "[Episode 4100]\t rewards globals : 4.277 \tavg rewards : 1.683,\tavg loss: : 0.272366,\tbuffer size : 4000,\tepsilon : 14.0%, \t r <=40 0.0, \t r > 40 16.831683168316832\n",
            "[Episode 4200]\t rewards globals : 4.268 \tavg rewards : 3.960,\tavg loss: : 0.272865,\tbuffer size : 4000,\tepsilon : 5.8%, \t r <=40 0.0, \t r > 40 39.603960396039604\n",
            "[Episode 4300]\t rewards globals : 4.262 \tavg rewards : 4.059,\tavg loss: : 0.274690,\tbuffer size : 4000,\tepsilon : 2.8%, \t r <=40 0.0, \t r > 40 40.5940594059406\n",
            "[Episode 4400]\t rewards globals : 4.279 \tavg rewards : 5.050,\tavg loss: : 0.275230,\tbuffer size : 4000,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 50.495049504950494\n",
            "environment change : i_1 49 j_1 9\n",
            "Test mode, runs: 100, i: 49, j: 9, final: True\n",
            "[task_2_tmax50] 100 run(s) avg rewards : 4.9\n",
            "[task_2_tmax40] 100 run(s) avg rewards : 0.9\n",
            "Point: 2.9000000000000004\n",
            "Local runtime: 84.36832237243652 seconds --- fast\n",
            "WARNING: do note that this might not reflect the runtime on the server.\n",
            "[Episode 4500]\t rewards globals : 4.281 \tavg rewards : 4.455,\tavg loss: : 0.275755,\tbuffer size : 4000,\tepsilon : 36.2%, \t r <=40 0.0, \t r > 40 44.554455445544555\n",
            "[Episode 4600]\t rewards globals : 4.203 \tavg rewards : 0.693,\tavg loss: : 0.275692,\tbuffer size : 4000,\tepsilon : 14.0%, \t r <=40 0.0, \t r > 40 6.9306930693069315\n",
            "[Episode 4700]\t rewards globals : 4.186 \tavg rewards : 3.366,\tavg loss: : 0.275810,\tbuffer size : 4000,\tepsilon : 5.8%, \t r <=40 0.0, \t r > 40 33.663366336633665\n",
            "[Episode 4800]\t rewards globals : 4.210 \tavg rewards : 5.248,\tavg loss: : 0.275936,\tbuffer size : 4000,\tepsilon : 2.8%, \t r <=40 0.0, \t r > 40 52.475247524752476\n",
            "[Episode 4900]\t rewards globals : 4.242 \tavg rewards : 5.743,\tavg loss: : 0.275784,\tbuffer size : 4000,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 57.42574257425742\n",
            "environment change : i_1 49 j_1 9\n",
            "Test mode, runs: 100, i: 49, j: 9, final: True\n",
            "[task_2_tmax50] 100 run(s) avg rewards : 6.2\n",
            "[task_2_tmax40] 100 run(s) avg rewards : 0.1\n",
            "Point: 3.15\n",
            "Local runtime: 86.25348472595215 seconds --- fast\n",
            "WARNING: do note that this might not reflect the runtime on the server.\n",
            "[Episode 5000]\t rewards globals : 4.287 \tavg rewards : 6.535,\tavg loss: : 0.276578,\tbuffer size : 4000,\tepsilon : 36.2%, \t r <=40 0.0, \t r > 40 65.34653465346535\n",
            "[Episode 5100]\t rewards globals : 4.233 \tavg rewards : 1.485,\tavg loss: : 0.275974,\tbuffer size : 4000,\tepsilon : 14.0%, \t r <=40 0.0, \t r > 40 14.85148514851485\n",
            "[Episode 5200]\t rewards globals : 4.207 \tavg rewards : 2.871,\tavg loss: : 0.276054,\tbuffer size : 4000,\tepsilon : 5.8%, \t r <=40 0.0, \t r > 40 28.71287128712871\n",
            "[Episode 5300]\t rewards globals : 4.218 \tavg rewards : 4.752,\tavg loss: : 0.276114,\tbuffer size : 4000,\tepsilon : 2.8%, \t r <=40 0.0, \t r > 40 47.524752475247524\n",
            "[Episode 5400]\t rewards globals : 4.253 \tavg rewards : 6.139,\tavg loss: : 0.275596,\tbuffer size : 4000,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 61.386138613861384\n",
            "environment change : i_1 49 j_1 9\n",
            "Test mode, runs: 100, i: 49, j: 9, final: True\n",
            "[task_2_tmax50] 100 run(s) avg rewards : 6.0\n",
            "[task_2_tmax40] 100 run(s) avg rewards : 0.6\n",
            "Point: 3.3\n",
            "Local runtime: 83.65603947639465 seconds --- fast\n",
            "WARNING: do note that this might not reflect the runtime on the server.\n",
            "[Episode 5500]\t rewards globals : 4.272 \tavg rewards : 5.248,\tavg loss: : 0.275582,\tbuffer size : 4000,\tepsilon : 36.2%, \t r <=40 0.0, \t r > 40 52.475247524752476\n",
            "[Episode 5600]\t rewards globals : 4.212 \tavg rewards : 0.891,\tavg loss: : 0.275493,\tbuffer size : 4000,\tepsilon : 14.0%, \t r <=40 0.0, \t r > 40 8.91089108910891\n",
            "[Episode 5700]\t rewards globals : 4.194 \tavg rewards : 3.168,\tavg loss: : 0.274765,\tbuffer size : 4000,\tepsilon : 5.8%, \t r <=40 0.0, \t r > 40 31.683168316831683\n",
            "[Episode 5800]\t rewards globals : 4.203 \tavg rewards : 4.653,\tavg loss: : 0.274827,\tbuffer size : 4000,\tepsilon : 2.8%, \t r <=40 0.0, \t r > 40 46.53465346534654\n",
            "[Episode 5900]\t rewards globals : 4.220 \tavg rewards : 5.149,\tavg loss: : 0.275149,\tbuffer size : 4000,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 51.48514851485149\n",
            "environment change : i_1 49 j_1 9\n",
            "Test mode, runs: 100, i: 49, j: 9, final: True\n",
            "[task_2_tmax50] 100 run(s) avg rewards : 6.0\n",
            "[task_2_tmax40] 100 run(s) avg rewards : 0.4\n",
            "Point: 3.2\n",
            "Local runtime: 84.13158869743347 seconds --- fast\n",
            "WARNING: do note that this might not reflect the runtime on the server.\n",
            "[Episode 6000]\t rewards globals : 4.243 \tavg rewards : 5.545,\tavg loss: : 0.275479,\tbuffer size : 4000,\tepsilon : 36.2%, \t r <=40 0.0, \t r > 40 55.44554455445545\n",
            "[Episode 6100]\t rewards globals : 4.191 \tavg rewards : 1.089,\tavg loss: : 0.275238,\tbuffer size : 4000,\tepsilon : 14.0%, \t r <=40 0.0, \t r > 40 10.891089108910892\n",
            "[Episode 6200]\t rewards globals : 4.183 \tavg rewards : 3.663,\tavg loss: : 0.275315,\tbuffer size : 4000,\tepsilon : 5.8%, \t r <=40 0.0, \t r > 40 36.633663366336634\n",
            "[Episode 6300]\t rewards globals : 4.196 \tavg rewards : 5.050,\tavg loss: : 0.275490,\tbuffer size : 4000,\tepsilon : 2.8%, \t r <=40 0.0, \t r > 40 50.495049504950494\n",
            "[Episode 6400]\t rewards globals : 4.234 \tavg rewards : 6.535,\tavg loss: : 0.275496,\tbuffer size : 4000,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 65.34653465346535\n",
            "environment change : i_1 49 j_1 9\n",
            "Test mode, runs: 100, i: 49, j: 9, final: True\n",
            "[task_2_tmax50] 100 run(s) avg rewards : 6.6\n",
            "[task_2_tmax40] 100 run(s) avg rewards : 0.6\n",
            "Point: 3.5999999999999996\n",
            "Local runtime: 82.97839570045471 seconds --- fast\n",
            "WARNING: do note that this might not reflect the runtime on the server.\n",
            "[Episode 6500]\t rewards globals : 4.270 \tavg rewards : 6.535,\tavg loss: : 0.275266,\tbuffer size : 4000,\tepsilon : 36.2%, \t r <=40 0.0, \t r > 40 65.34653465346535\n",
            "[Episode 6600]\t rewards globals : 4.222 \tavg rewards : 1.089,\tavg loss: : 0.274939,\tbuffer size : 4000,\tepsilon : 14.0%, \t r <=40 0.0, \t r > 40 10.891089108910892\n",
            "[Episode 6700]\t rewards globals : 4.202 \tavg rewards : 2.871,\tavg loss: : 0.274680,\tbuffer size : 4000,\tepsilon : 5.8%, \t r <=40 0.0, \t r > 40 28.71287128712871\n",
            "[Episode 6800]\t rewards globals : 4.216 \tavg rewards : 5.149,\tavg loss: : 0.274371,\tbuffer size : 4000,\tepsilon : 2.8%, \t r <=40 0.0, \t r > 40 51.48514851485149\n",
            "[Episode 6900]\t rewards globals : 4.233 \tavg rewards : 5.347,\tavg loss: : 0.274006,\tbuffer size : 4000,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 53.46534653465347\n",
            "environment change : i_1 49 j_1 9\n",
            "Test mode, runs: 100, i: 49, j: 9, final: True\n",
            "[task_2_tmax50] 100 run(s) avg rewards : 6.7\n",
            "[task_2_tmax40] 100 run(s) avg rewards : 0.1\n",
            "Point: 3.4\n",
            "Local runtime: 82.28992581367493 seconds --- fast\n",
            "WARNING: do note that this might not reflect the runtime on the server.\n",
            "[Episode 7000]\t rewards globals : 4.275 \tavg rewards : 7.228,\tavg loss: : 0.273623,\tbuffer size : 4000,\tepsilon : 36.2%, \t r <=40 0.0, \t r > 40 72.27722772277228\n",
            "[Episode 7100]\t rewards globals : 4.239 \tavg rewards : 1.683,\tavg loss: : 0.272916,\tbuffer size : 4000,\tepsilon : 14.0%, \t r <=40 0.0, \t r > 40 16.831683168316832\n",
            "[Episode 7200]\t rewards globals : 4.237 \tavg rewards : 4.059,\tavg loss: : 0.272411,\tbuffer size : 4000,\tepsilon : 5.8%, \t r <=40 0.0, \t r > 40 40.5940594059406\n",
            "[Episode 7300]\t rewards globals : 4.239 \tavg rewards : 4.455,\tavg loss: : 0.272108,\tbuffer size : 4000,\tepsilon : 2.8%, \t r <=40 0.0, \t r > 40 44.554455445544555\n",
            "[Episode 7400]\t rewards globals : 4.263 \tavg rewards : 6.040,\tavg loss: : 0.271580,\tbuffer size : 4000,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 60.396039603960396\n",
            "environment change : i_1 49 j_1 9\n",
            "Test mode, runs: 100, i: 49, j: 9, final: True\n",
            "[task_2_tmax50] 100 run(s) avg rewards : 7.4\n",
            "[task_2_tmax40] 100 run(s) avg rewards : 0.0\n",
            "Point: 3.7\n",
            "Local runtime: 82.24107336997986 seconds --- fast\n",
            "WARNING: do note that this might not reflect the runtime on the server.\n",
            "[Episode 7500]\t rewards globals : 4.274 \tavg rewards : 5.050,\tavg loss: : 0.270960,\tbuffer size : 4000,\tepsilon : 36.2%, \t r <=40 0.0, \t r > 40 50.495049504950494\n",
            "[Episode 7600]\t rewards globals : 4.240 \tavg rewards : 1.683,\tavg loss: : 0.270529,\tbuffer size : 4000,\tepsilon : 14.0%, \t r <=40 0.0, \t r > 40 16.831683168316832\n",
            "[Episode 7700]\t rewards globals : 4.229 \tavg rewards : 3.465,\tavg loss: : 0.270893,\tbuffer size : 4000,\tepsilon : 5.8%, \t r <=40 0.0, \t r > 40 34.65346534653465\n",
            "[Episode 7800]\t rewards globals : 4.233 \tavg rewards : 4.554,\tavg loss: : 0.270835,\tbuffer size : 4000,\tepsilon : 2.8%, \t r <=40 0.0, \t r > 40 45.54455445544555\n",
            "[Episode 7900]\t rewards globals : 4.245 \tavg rewards : 5.248,\tavg loss: : 0.270766,\tbuffer size : 4000,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 52.475247524752476\n",
            "environment change : i_1 49 j_1 9\n",
            "Test mode, runs: 100, i: 49, j: 9, final: True\n",
            "[task_2_tmax50] 100 run(s) avg rewards : 6.7\n",
            "[task_2_tmax40] 100 run(s) avg rewards : 0.2\n",
            "Point: 3.45\n",
            "Local runtime: 80.37525463104248 seconds --- fast\n",
            "WARNING: do note that this might not reflect the runtime on the server.\n",
            "[Episode 8000]\t rewards globals : 4.261 \tavg rewards : 5.446,\tavg loss: : 0.270511,\tbuffer size : 4000,\tepsilon : 36.2%, \t r <=40 0.0, \t r > 40 54.45544554455446\n",
            "[Episode 8100]\t rewards globals : 4.220 \tavg rewards : 0.990,\tavg loss: : 0.270137,\tbuffer size : 4000,\tepsilon : 14.0%, \t r <=40 0.0, \t r > 40 9.900990099009901\n",
            "[Episode 8200]\t rewards globals : 4.206 \tavg rewards : 2.970,\tavg loss: : 0.270219,\tbuffer size : 4000,\tepsilon : 5.8%, \t r <=40 0.0, \t r > 40 29.7029702970297\n",
            "[Episode 8300]\t rewards globals : 4.221 \tavg rewards : 5.545,\tavg loss: : 0.270200,\tbuffer size : 4000,\tepsilon : 2.8%, \t r <=40 0.0, \t r > 40 55.44554455445545\n",
            "[Episode 8400]\t rewards globals : 4.234 \tavg rewards : 5.347,\tavg loss: : 0.270303,\tbuffer size : 4000,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 53.46534653465347\n",
            "environment change : i_1 49 j_1 9\n",
            "Test mode, runs: 100, i: 49, j: 9, final: True\n",
            "[task_2_tmax50] 100 run(s) avg rewards : 6.0\n",
            "[task_2_tmax40] 100 run(s) avg rewards : 0.3\n",
            "Point: 3.15\n",
            "Local runtime: 83.57546806335449 seconds --- fast\n",
            "WARNING: do note that this might not reflect the runtime on the server.\n",
            "[Episode 8500]\t rewards globals : 4.249 \tavg rewards : 5.446,\tavg loss: : 0.270337,\tbuffer size : 4000,\tepsilon : 36.2%, \t r <=40 0.0, \t r > 40 54.45544554455446\n",
            "[Episode 8600]\t rewards globals : 4.212 \tavg rewards : 1.089,\tavg loss: : 0.270068,\tbuffer size : 4000,\tepsilon : 14.0%, \t r <=40 0.0, \t r > 40 10.891089108910892\n",
            "[Episode 8700]\t rewards globals : 4.201 \tavg rewards : 3.267,\tavg loss: : 0.269936,\tbuffer size : 4000,\tepsilon : 5.8%, \t r <=40 0.0, \t r > 40 32.67326732673268\n",
            "[Episode 8800]\t rewards globals : 4.218 \tavg rewards : 5.644,\tavg loss: : 0.270031,\tbuffer size : 4000,\tepsilon : 2.8%, \t r <=40 0.0, \t r > 40 56.43564356435643\n",
            "[Episode 8900]\t rewards globals : 4.241 \tavg rewards : 6.337,\tavg loss: : 0.270154,\tbuffer size : 4000,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 63.366336633663366\n",
            "environment change : i_1 49 j_1 9\n",
            "Test mode, runs: 100, i: 49, j: 9, final: True\n",
            "[task_2_tmax50] 100 run(s) avg rewards : 6.3\n",
            "[task_2_tmax40] 100 run(s) avg rewards : 2.7\n",
            "Point: 4.5\n",
            "Local runtime: 76.41131448745728 seconds --- fast\n",
            "WARNING: do note that this might not reflect the runtime on the server.\n",
            "[Episode 9000]\t rewards globals : 4.252 \tavg rewards : 5.248,\tavg loss: : 0.270530,\tbuffer size : 4000,\tepsilon : 36.2%, \t r <=40 0.0, \t r > 40 52.475247524752476\n",
            "[Episode 9100]\t rewards globals : 4.225 \tavg rewards : 1.782,\tavg loss: : 0.270595,\tbuffer size : 4000,\tepsilon : 14.0%, \t r <=40 0.0, \t r > 40 17.82178217821782\n",
            "[Episode 9200]\t rewards globals : 4.209 \tavg rewards : 2.772,\tavg loss: : 0.270626,\tbuffer size : 4000,\tepsilon : 5.8%, \t r <=40 0.0, \t r > 40 27.722772277227726\n",
            "[Episode 9300]\t rewards globals : 4.222 \tavg rewards : 5.347,\tavg loss: : 0.270841,\tbuffer size : 4000,\tepsilon : 2.8%, \t r <=40 0.0, \t r > 40 53.46534653465347\n",
            "[Episode 9400]\t rewards globals : 4.239 \tavg rewards : 5.743,\tavg loss: : 0.271145,\tbuffer size : 4000,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 57.42574257425742\n",
            "environment change : i_1 49 j_1 9\n",
            "Test mode, runs: 100, i: 49, j: 9, final: True\n",
            "[task_2_tmax50] 100 run(s) avg rewards : 7.1\n",
            "[task_2_tmax40] 100 run(s) avg rewards : 0.8\n",
            "Point: 3.9499999999999997\n",
            "Local runtime: 79.76133918762207 seconds --- fast\n",
            "WARNING: do note that this might not reflect the runtime on the server.\n",
            "[Episode 9500]\t rewards globals : 4.260 \tavg rewards : 6.238,\tavg loss: : 0.271149,\tbuffer size : 4000,\tepsilon : 36.2%, \t r <=40 0.0, \t r > 40 62.37623762376238\n",
            "[Episode 9600]\t rewards globals : 4.231 \tavg rewards : 1.485,\tavg loss: : 0.270835,\tbuffer size : 4000,\tepsilon : 14.0%, \t r <=40 0.0, \t r > 40 14.85148514851485\n",
            "[Episode 9700]\t rewards globals : 4.228 \tavg rewards : 3.960,\tavg loss: : 0.270812,\tbuffer size : 4000,\tepsilon : 5.8%, \t r <=40 0.0, \t r > 40 39.603960396039604\n",
            "[Episode 9800]\t rewards globals : 4.240 \tavg rewards : 5.347,\tavg loss: : 0.271096,\tbuffer size : 4000,\tepsilon : 2.8%, \t r <=40 0.0, \t r > 40 53.46534653465347\n",
            "[Episode 9900]\t rewards globals : 4.262 \tavg rewards : 6.436,\tavg loss: : 0.271356,\tbuffer size : 4000,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 64.35643564356435\n",
            "7144.447502613068\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kM9mre8d4VL-",
        "colab_type": "code",
        "outputId": "89801504-863c-47d2-9e74-173e6eb3df5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# TASK 2\n",
        "\n",
        "import sys\n",
        "import time\n",
        "\n",
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "date_save = time.strftime(\"%d %H %M\")\n",
        "model_path = os.path.join('HW3_Task2','agent', 'model_{}_{}.pt'.format(date_save,2))\n",
        "save_path = os.path.join('HW3_Task2','agent', 'model_{}_{}.pt'.format(date_save,3))\n",
        "\n",
        "duree = 0 \n",
        "nb_bad_init = 0 \n",
        "# Try for 10 diferent inits\n",
        "\n",
        "for i in range(0,1):\n",
        "  debut = time.time()\n",
        "  # Try an init \n",
        "  #!python3 HW3_Task2/agent/models.py --test --path=HW3_Task2/agent/model_last1_20_05_cb_8_1000.pt --savepath=HW3_Task2/agent/model_last1_20_05\n",
        "  #!python3 HW3_Task2/agent/models.py --pretrain --path=HW3_Task2/agent/model_last_20_05_cb_8.pt --savepath=HW3_Task2/agent/model_last_20_05\n",
        "  !python3 HW3_Task2/agent/models.py --pretrain --path=HW3_Task2/agent/model_last_21_05_cb_final_8.pt --savepath=HW3_Task2/agent/model_snif_21_05\n",
        "  #!python3 HW3_Task2/agent/testenv.py\n",
        "  duree = (time.time()-debut)\n",
        "  # If good init exit loop\n",
        "  print(duree)\n",
        "  if duree > 10000:\n",
        "    break\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n",
            "pretrain mode\n",
            "model loaded from HW3_Task2/agent/model_last_21_05_cb_final_8.pt\n",
            "ConvDQN(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(4, 32, kernel_size=(2, 2), stride=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): Conv2d(32, 64, kernel_size=(2, 2), stride=(1, 1))\n",
            "    (3): ReLU()\n",
            "  )\n",
            "  (layers): Sequential(\n",
            "    (0): Linear(in_features=24576, out_features=256, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=256, out_features=5, bias=True)\n",
            "  )\n",
            ")\n",
            "range change : x_min 49 x_max 49\n",
            "environment change : i_1 49 j_1 9\n",
            "[Episode 50]\t rewards globals : 0.598 \tavg rewards : 0.598,\tavg loss: : 0.256007,\tfirst time zero 0.7058823529411765,\tbuffer size : 1187,\tepsilon : 28.3%, \t r <=40 0.0, \t r > 40 5.88235294117647\n",
            "[Episode 100]\t rewards globals : 0.599 \tavg rewards : 0.588,\tavg loss: : 0.109032,\tfirst time zero 0.3564356435643564,\tbuffer size : 2129,\tepsilon : 22.3%, \t r <=40 0.0, \t r > 40 5.88235294117647\n",
            "[Episode 150]\t rewards globals : 0.960 \tavg rewards : 1.657,\tavg loss: : 0.113348,\tfirst time zero 0.6821192052980133,\tbuffer size : 3461,\tepsilon : 17.6%, \t r <=40 0.0, \t r > 40 11.76470588235294\n",
            "[Episode 200]\t rewards globals : 1.179 \tavg rewards : 1.804,\tavg loss: : 0.102202,\tfirst time zero 0.8557213930348259,\tbuffer size : 4790,\tepsilon : 13.9%, \t r <=40 0.0, \t r > 40 13.725490196078432\n",
            "[Episode 250]\t rewards globals : 1.394 \tavg rewards : 2.412,\tavg loss: : 0.116100,\tfirst time zero 0.9840637450199203,\tbuffer size : 6261,\tepsilon : 11.0%, \t r <=40 0.0, \t r > 40 19.607843137254903\n",
            "[Episode 300]\t rewards globals : 1.641 \tavg rewards : 2.824,\tavg loss: : 0.114048,\tfirst time zero 1.4551495016611296,\tbuffer size : 7838,\tepsilon : 8.8%, \t r <=40 0.0, \t r > 40 21.568627450980394\n",
            "[Episode 350]\t rewards globals : 1.947 \tavg rewards : 3.912,\tavg loss: : 0.126491,\tfirst time zero 2.034188034188034,\tbuffer size : 9432,\tepsilon : 7.1%, \t r <=40 0.0, \t r > 40 27.450980392156865\n",
            "[Episode 400]\t rewards globals : 2.349 \tavg rewards : 5.284,\tavg loss: : 0.127224,\tfirst time zero 2.4713216957605986,\tbuffer size : 10000,\tepsilon : 5.7%, \t r <=40 0.0, \t r > 40 37.254901960784316\n",
            "[Episode 450]\t rewards globals : 2.579 \tavg rewards : 4.529,\tavg loss: : 0.137102,\tfirst time zero 2.8758314855875833,\tbuffer size : 10000,\tepsilon : 4.7%, \t r <=40 0.0, \t r > 40 29.411764705882355\n",
            "Test mode, runs: 100, i: 49, j: 9, final: True\n",
            "[task_2_tmax50] 100 run(s) avg rewards : 5.9\n",
            "[task_2_tmax40] 100 run(s) avg rewards : 3.2\n",
            "Point: 4.550000000000001\n",
            "Local runtime: 74.69968676567078 seconds --- fast\n",
            "WARNING: do note that this might not reflect the runtime on the server.\n",
            "[Episode 500]\t rewards globals : 2.750 \tavg rewards : 4.402,\tavg loss: : 0.138225,\tfirst time zero 2.9081836327345307,\tbuffer size : 10000,\tepsilon : 3.9%, \t r <=40 0.0, \t r > 40 39.21568627450981\n",
            "[Episode 550]\t rewards globals : 3.134 \tavg rewards : 7.059,\tavg loss: : 0.147388,\tfirst time zero 3.2123411978221417,\tbuffer size : 10000,\tepsilon : 3.2%, \t r <=40 0.0, \t r > 40 58.82352941176471\n",
            "[Episode 600]\t rewards globals : 3.359 \tavg rewards : 5.912,\tavg loss: : 0.147040,\tfirst time zero 3.6622296173044924,\tbuffer size : 10000,\tepsilon : 2.7%, \t r <=40 0.0, \t r > 40 45.09803921568628\n",
            "[Episode 650]\t rewards globals : 3.594 \tavg rewards : 6.294,\tavg loss: : 0.154384,\tfirst time zero 4.339477726574501,\tbuffer size : 10000,\tepsilon : 2.4%, \t r <=40 0.0, \t r > 40 39.21568627450981\n",
            "[Episode 700]\t rewards globals : 3.821 \tavg rewards : 6.647,\tavg loss: : 0.154144,\tfirst time zero 4.807417974322396,\tbuffer size : 10000,\tepsilon : 2.1%, \t r <=40 0.0, \t r > 40 39.21568627450981\n",
            "[Episode 750]\t rewards globals : 3.889 \tavg rewards : 4.951,\tavg loss: : 0.161009,\tfirst time zero 4.958721704394141,\tbuffer size : 10000,\tepsilon : 1.8%, \t r <=40 0.0, \t r > 40 33.33333333333333\n",
            "[Episode 800]\t rewards globals : 3.928 \tavg rewards : 4.608,\tavg loss: : 0.161204,\tfirst time zero 5.228464419475656,\tbuffer size : 10000,\tepsilon : 1.6%, \t r <=40 0.0, \t r > 40 25.49019607843137\n",
            "[Episode 850]\t rewards globals : 4.073 \tavg rewards : 6.480,\tavg loss: : 0.166485,\tfirst time zero 5.643948296122209,\tbuffer size : 10000,\tepsilon : 1.5%, \t r <=40 0.0, \t r > 40 37.254901960784316\n",
            "[Episode 900]\t rewards globals : 4.227 \tavg rewards : 6.902,\tavg loss: : 0.166099,\tfirst time zero 6.064372918978912,\tbuffer size : 10000,\tepsilon : 1.4%, \t r <=40 0.0, \t r > 40 47.05882352941176\n",
            "[Episode 950]\t rewards globals : 4.341 \tavg rewards : 6.265,\tavg loss: : 0.170307,\tfirst time zero 6.3249211356466875,\tbuffer size : 10000,\tepsilon : 1.3%, \t r <=40 0.0, \t r > 40 39.21568627450981\n",
            "Test mode, runs: 100, i: 49, j: 9, final: True\n",
            "[task_2_tmax50] 100 run(s) avg rewards : 7.3\n",
            "[task_2_tmax40] 100 run(s) avg rewards : 0.1\n",
            "Point: 3.6999999999999997\n",
            "Local runtime: 79.1206738948822 seconds --- fast\n",
            "WARNING: do note that this might not reflect the runtime on the server.\n",
            "[Episode 1000]\t rewards globals : 4.477 \tavg rewards : 7.118,\tavg loss: : 0.169548,\tfirst time zero 6.772227772227772,\tbuffer size : 10000,\tepsilon : 1.2%, \t r <=40 0.0, \t r > 40 45.09803921568628\n",
            "[Episode 1050]\t rewards globals : 4.529 \tavg rewards : 5.706,\tavg loss: : 0.174438,\tfirst time zero 6.87535680304472,\tbuffer size : 10000,\tepsilon : 1.2%, \t r <=40 0.0, \t r > 40 29.411764705882355\n",
            "[Episode 1100]\t rewards globals : 4.612 \tavg rewards : 6.422,\tavg loss: : 0.174220,\tfirst time zero 7.136239782016348,\tbuffer size : 10000,\tepsilon : 1.1%, \t r <=40 0.0, \t r > 40 33.33333333333333\n",
            "[Episode 1150]\t rewards globals : 4.716 \tavg rewards : 7.098,\tavg loss: : 0.177923,\tfirst time zero 7.420503909643788,\tbuffer size : 10000,\tepsilon : 1.1%, \t r <=40 0.0, \t r > 40 43.13725490196079\n",
            "[Episode 1200]\t rewards globals : 4.726 \tavg rewards : 5.069,\tavg loss: : 0.177564,\tfirst time zero 7.502081598667777,\tbuffer size : 10000,\tepsilon : 1.1%, \t r <=40 0.0, \t r > 40 27.450980392156865\n",
            "[Episode 1250]\t rewards globals : 4.736 \tavg rewards : 5.118,\tavg loss: : 0.181282,\tfirst time zero 7.394084732214228,\tbuffer size : 10000,\tepsilon : 1.1%, \t r <=40 0.0, \t r > 40 35.294117647058826\n",
            "[Episode 1300]\t rewards globals : 4.786 \tavg rewards : 6.098,\tavg loss: : 0.181036,\tfirst time zero 7.283627978478094,\tbuffer size : 10000,\tepsilon : 1.1%, \t r <=40 0.0, \t r > 40 47.05882352941176\n",
            "[Episode 1350]\t rewards globals : 4.837 \tavg rewards : 6.304,\tavg loss: : 0.184070,\tfirst time zero 7.467061435973353,\tbuffer size : 10000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 37.254901960784316\n",
            "[Episode 1400]\t rewards globals : 4.889 \tavg rewards : 6.382,\tavg loss: : 0.183338,\tfirst time zero 7.650963597430406,\tbuffer size : 10000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 33.33333333333333\n",
            "[Episode 1450]\t rewards globals : 4.910 \tavg rewards : 5.588,\tavg loss: : 0.186032,\tfirst time zero 7.835975189524466,\tbuffer size : 10000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 25.49019607843137\n",
            "Test mode, runs: 100, i: 49, j: 9, final: True\n",
            "[task_2_tmax50] 100 run(s) avg rewards : 6.5\n",
            "[task_2_tmax40] 100 run(s) avg rewards : 0.4\n",
            "Point: 3.45\n",
            "Local runtime: 81.2177209854126 seconds --- fast\n",
            "WARNING: do note that this might not reflect the runtime on the server.\n",
            "[Episode 1500]\t rewards globals : 4.973 \tavg rewards : 6.686,\tavg loss: : 0.184894,\tfirst time zero 8.135243171219187,\tbuffer size : 10000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 29.411764705882355\n",
            "[Episode 1550]\t rewards globals : 5.028 \tavg rewards : 6.784,\tavg loss: : 0.186866,\tfirst time zero 8.274016763378466,\tbuffer size : 10000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 37.254901960784316\n",
            "[Episode 1600]\t rewards globals : 5.068 \tavg rewards : 6.373,\tavg loss: : 0.185897,\tfirst time zero 8.443472829481575,\tbuffer size : 10000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 37.254901960784316\n",
            "[Episode 1650]\t rewards globals : 5.119 \tavg rewards : 6.627,\tavg loss: : 0.187189,\tfirst time zero 8.603876438522107,\tbuffer size : 10000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 35.294117647058826\n",
            "[Episode 1700]\t rewards globals : 5.191 \tavg rewards : 7.647,\tavg loss: : 0.185976,\tfirst time zero 8.753674309229865,\tbuffer size : 10000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 41.17647058823529\n",
            "[Episode 1750]\t rewards globals : 5.206 \tavg rewards : 5.804,\tavg loss: : 0.187058,\tfirst time zero 8.918332381496288,\tbuffer size : 10000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 31.372549019607842\n",
            "[Episode 1800]\t rewards globals : 5.243 \tavg rewards : 6.412,\tavg loss: : 0.185985,\tfirst time zero 8.99833425874514,\tbuffer size : 10000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 31.372549019607842\n",
            "[Episode 1850]\t rewards globals : 5.307 \tavg rewards : 7.471,\tavg loss: : 0.187595,\tfirst time zero 9.180983252296055,\tbuffer size : 10000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 41.17647058823529\n",
            "[Episode 1900]\t rewards globals : 5.344 \tavg rewards : 6.755,\tavg loss: : 0.186942,\tfirst time zero 9.19726459758022,\tbuffer size : 10000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 50.98039215686274\n",
            "[Episode 1950]\t rewards globals : 5.397 \tavg rewards : 7.294,\tavg loss: : 0.189402,\tfirst time zero 9.302921578677601,\tbuffer size : 10000,\tepsilon : 1.0%, \t r <=40 0.0, \t r > 40 35.294117647058826\n",
            "2273.953361272812\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEhO5nj6eZz8",
        "colab_type": "code",
        "outputId": "599ff076-81de-4613-8555-a6cce530cec2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "# Test for TASK 2:\n",
        "!python3 HW3_Task2/agent/__init__.py\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[task_2_tmax50] 300 run(s) avg rewards : 0.0\n",
            "[task_2_tmax40] 300 run(s) avg rewards : 0.0\n",
            "Point: 0.0\n",
            "Local runtime: 291.5432770252228 seconds --- safe\n",
            "WARNING: do note that this might not reflect the runtime on the server.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RIkGEjdJw83",
        "colab_type": "code",
        "outputId": "5a1e1651-31fd-4784-91fa-0d672a113f88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        }
      },
      "source": [
        "# TASK 1\n",
        "\n",
        "import time\n",
        "duree = 0 \n",
        "nb_bad_init = 0 \n",
        "# Try for 10 diferent inits\n",
        "\n",
        "for i in range(0,10):\n",
        "  debut = time.time()\n",
        "  # Try an init \n",
        "  !python3 HW3_Task1/agent/dqn.py --train\n",
        "  duree = (time.time()-debut)\n",
        "  # If good init exit loop\n",
        "  if duree > 100:\n",
        "    break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "python3: can't open file 'HW3_Task1/agent/dqn.py': [Errno 2] No such file or directory\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-80-c525fe8926e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mdebut\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;31m# Try an init\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m   \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'python3 HW3_Task1/agent/dqn.py --train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m   \u001b[0mduree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdebut\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0;31m# If good init exit loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'also_return_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_system_commands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_system_compat\u001b[0;34m(shell, cmd, also_return_output)\u001b[0m\n\u001b[1;32m    436\u001b[0m   \u001b[0;31m# stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m   result = _run_command(\n\u001b[0;32m--> 438\u001b[0;31m       shell.var_expand(cmd, depth=2), clear_streamed_output=False)\n\u001b[0m\u001b[1;32m    439\u001b[0m   \u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_exit_code'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_INTERRUPTED_SIGNALS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_run_command\u001b[0;34m(cmd, clear_streamed_output)\u001b[0m\n\u001b[1;32m    193\u001b[0m       \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild_pty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0m_monitor_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_pty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_stdin_widget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0mepoll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_display_stdin_widget\u001b[0;34m(delay_millis)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m   \u001b[0mhide_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'cell_remove_stdin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m   \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocking_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mhide_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_header\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    169\u001b[0m   \u001b[0;31m# unique.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m   \u001b[0mrequest_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msend_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     if (reply.get('type') == 'colab_reply' and\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ru96YQP2MC35",
        "colab_type": "code",
        "outputId": "6e2c6c2e-91e2-49c1-8f20-1a86007be6a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 755
        }
      },
      "source": [
        "from HW3_Task2.agent.env import construct_task2_env_ij\n",
        "import numpy as np\n",
        "\n",
        "def pos_from_state(state):\n",
        "  for i in range(0,10):\n",
        "    column = (np.where(state[1,i,:] == 1.))\n",
        "    if len(column[0]) > 0:\n",
        "      column = column[0][0]\n",
        "      row = i \n",
        "      return(column,row)\n",
        "\n",
        "def state_from_pos(pos_x,pos_y,state,largeur=15,hauteur=4):\n",
        "\n",
        "  #Crop state \n",
        "  #y_max = pos_y + (hauteur+1)//2-1\n",
        "  #y_min = pos_y - (hauteur)//2\n",
        "  y_max = pos_y + 1\n",
        "  y_min = pos_y - hauteur +1  \n",
        "\n",
        "  x_max = pos_x + largeur //3 \n",
        "  x_min = pos_x - 2*(largeur//3)\n",
        "  \n",
        "  #print(\"xmin,xmax,ymin,ymax,hauteur\",x_min,x_max,y_min,y_max,hauteur)\n",
        "\n",
        "  if y_max > 9:\n",
        "    y_max = 9 \n",
        "    y_min = 9-hauteur+1\n",
        "\n",
        "  elif y_min < 0:\n",
        "    \n",
        "    y_max = hauteur-1\n",
        "    y_min = 0 \n",
        "    \n",
        "\n",
        "  if x_min < 0:\n",
        "    x_min = 0 \n",
        "    x_max = largeur \n",
        "  elif x_max > 49:\n",
        "    res = np.zeros([4,hauteur,largeur+1])\n",
        "    delta = (largeur//3)\n",
        "    xx_min = 49-x_min \n",
        "    largeur_delta = largeur - xx_min\n",
        "    #print(\"crop 1:\",xx_min,xx_min+largeur_delta)\n",
        "    print(\"yMin ymax 1:\",y_min,y_max)\n",
        "    res[:,:,xx_min+1:xx_min+1+largeur_delta] = state[:,y_min:(y_max),0:largeur_delta]\n",
        "    res[:,:,0:xx_min+1] = state[:,y_min:(y_max),x_min:50]\n",
        "    #print(\"res.shape\",res.shape)\n",
        "    return(res)\n",
        "  #print(\"res state.shape\",state[:,y_min:(y_max+1),x_min:(x_max+1)].shape)\n",
        "  #print(\"Y min de tes mortsn\",y_min,y_max)\n",
        "  return(state[:,y_min:(y_max),x_min:(x_max+1)])\n",
        "\n",
        "\n",
        "env = construct_task2_env_ij(20,7)\n",
        "\n",
        "state = env.reset()\n",
        "env.render()\n",
        "#state,_,_,_ = env.step(0)\n",
        "#state,_,_,_ = env.step(3)\n",
        "pos_x,pos_y = pos_from_state(state)\n",
        "env.render()\n",
        "print(pos_x,pos_y)\n",
        "next_state = state_from_pos(pos_x,pos_y,state,largeur=15,hauteur=3)\n",
        "print(next_state)\n",
        "print(next_state.shape)\n",
        "import numpy as np\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "========================================================================================================================================================================================================\n",
            "  F   -   -   -   -   5   -   -   -   -   2   -   -   -   -   -   -   -   -   4   -   -   -   -   -   1   -   -   3   O   -   -   -   -   -   6   -   -   -   -   -   -   -   -   -   -   -   -   -   -\n",
            "  -   -   -   -   -   -   -   -   -   -   -   -   9   -   -   -   -   -   -   -   8   -  12   -   -   -   -   -   -   -  11   -   -   -   -   -   -   7  13   -   -   -   -   -   -  14   -  10   -   -\n",
            "  -   -   -  20   -   -  15  16   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -  19   -   -  18   -   -   -   -   -   -   -   -   -   -   -   -   -   -  17   -   -   -   -   -   -\n",
            "  -   -  26   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -  24   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -  22   -  23   -   -   -   -  21  25\n",
            " 33   -  31   -   -   -   -   -  30   -   -   -   -   -  29   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -  27  28   -   -   -   -   -   -   -   -   -   -   -   -  32   -   -\n",
            "  -   -  41   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -  40  38   -  37   -   -   -   -   -   -   -  35   -   -   -   -  36   -   -   -  39   -  34   -   -   -   -   -   -\n",
            "  -   -   -   -  43   -   -   -   -   -  45   -   -   -   -   -   -   -   -   -   -  46  42  47   -   -   -   -   -  44   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -\n",
            "  -   -   -   -   -   -  51   -   -  52   -   -   -   -   -   -   -   -   -   -   <   -   -   -   -   -   -   -  54  50   -   -   -  53   -  49   -   -   -   -  48   -   -   -   -   -   -   -   -   -\n",
            "  -   -   -   -  56  59   -   -   -   -   -   -  55  57   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -  60   -   -   -   -   -   -   -   -  58   -\n",
            " 63  61  68   -   -   -   -   -   -   -   -   -   -   -   -  66   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -  67   -   -   -   -   -  62   -  65  64   -   -   -   -   -   -   -\n",
            "========================================================================================================================================================================================================\n",
            "========================================================================================================================================================================================================\n",
            "  F   -   -   -   -   5   -   -   -   -   2   -   -   -   -   -   -   -   -   4   -   -   -   -   -   1   -   -   3   O   -   -   -   -   -   6   -   -   -   -   -   -   -   -   -   -   -   -   -   -\n",
            "  -   -   -   -   -   -   -   -   -   -   -   -   9   -   -   -   -   -   -   -   8   -  12   -   -   -   -   -   -   -  11   -   -   -   -   -   -   7  13   -   -   -   -   -   -  14   -  10   -   -\n",
            "  -   -   -  20   -   -  15  16   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -  19   -   -  18   -   -   -   -   -   -   -   -   -   -   -   -   -   -  17   -   -   -   -   -   -\n",
            "  -   -  26   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -  24   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -  22   -  23   -   -   -   -  21  25\n",
            " 33   -  31   -   -   -   -   -  30   -   -   -   -   -  29   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -  27  28   -   -   -   -   -   -   -   -   -   -   -   -  32   -   -\n",
            "  -   -  41   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -  40  38   -  37   -   -   -   -   -   -   -  35   -   -   -   -  36   -   -   -  39   -  34   -   -   -   -   -   -\n",
            "  -   -   -   -  43   -   -   -   -   -  45   -   -   -   -   -   -   -   -   -   -  46  42  47   -   -   -   -   -  44   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -\n",
            "  -   -   -   -   -   -  51   -   -  52   -   -   -   -   -   -   -   -   -   -   <   -   -   -   -   -   -   -  54  50   -   -   -  53   -  49   -   -   -   -  48   -   -   -   -   -   -   -   -   -\n",
            "  -   -   -   -  56  59   -   -   -   -   -   -  55  57   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -  60   -   -   -   -   -   -   -   -  58   -\n",
            " 63  61  68   -   -   -   -   -   -   -   -   -   -   -   -  66   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -  67   -   -   -   -   -  62   -  65  64   -   -   -   -   -   -   -\n",
            "========================================================================================================================================================================================================\n",
            "20 7\n",
            "[[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n",
            "(4, 3, 16)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxdIlwbW_2RK",
        "colab_type": "code",
        "outputId": "aa1b46ba-bfb9-460d-f267-1933719d3f9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        }
      },
      "source": [
        "#env = construct_task2_env_ij(49,9)\n",
        "#env.reset()\n",
        "env.step(2)\n",
        "env.render()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "========================================================================================================================================================================================================\n",
            " 2F   -   -   -   -   -   -   1   -   -   -   O   5   ~   -   -   -   -   -   -   -   -   -   -   -   -   -   6   ~   -   -   3   ~   -   -   -   <   -   -   -   4   -   -   -   -   -   -   -   -   -\n",
            "  -   -   -   -   -   -  14   -   -   -   -   9   ~   -   -  13   ~  12   -   -   -   7   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -  11   ~   -  10   8\n",
            "  -   -   -   -   -   -   -   -   -  16   -  18  17   -   -   -  15  19   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -  20   -   -   -   -   -   -   -   -   -   -   -   -   -\n",
            "  -  26   -   -   -  25   ~   -   -   -   -   -   -   -   -   -   -   -  21   -   -   -   -   -   -   -   -   -  24   ~   -   -   -   -   -   -   -  22   -   -  23   ~   ~   -   -   -   -   -   -   -\n",
            "  -   -   -   -   -  27   ~   -   -  31   ~   -   -   -   -  28   -   -   -  33   -  32   ~   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -  29   -  30   ~   -   -   -   -   -   -\n",
            "  -   -  40   ~   -  38  34   ~   -   -   -   -   -   -   -  41   -   -   -   -   -   -   -   -   -   -   -  36   -  39   ~   -   -   -   -   -   -  35   ~   -   -   -   -   -   -  37   ~   -   -   -\n",
            "  ~   -   -   -   -   -   -   -   -   -   -   -   -   -  47   ~   ~   -   -   -   -   -   -   -   -   -   -  42   ~   -   -   -  45   ~   ~   -   -   -   -   -  44   ~   -   -   -   -  46   ~  43   ~\n",
            " 50   -   -  49   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -  54   -   -   -   -   -   -   -  52   -   -   -   -   -   -   -   -   -   -   -   -   -   -  48  53   -   -  51   -   -\n",
            " 58   ~   -   -   -   -   -   -   -   -   -   -   -   -   -  59   -   -   -  55   -   -  57   -  56   ~   -   -  60   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -\n",
            " 62   ~   -  68  66   ~   -   -   -   -   -   -   -   -   -   -   -   -  65   ~   -   -   -  61   ~   -  67   ~   -   -   -   -   -   -   -   -   -   -   -   -   -   -   -  64  63   ~   -   -   -   -\n",
            "========================================================================================================================================================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bjWQhZdswyC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from HW3_Task2.agent.loss_src import monteCarloUpdate\n",
        "import collections\n",
        "from HW3_Task2.agent.buffer_src import ReplayBuffer\n",
        "Transition = collections.namedtuple('Transition', ('state', 'action', 'reward', 'next_state', 'done'))\n",
        "\n",
        "memory = ReplayBuffer(buffer_limit=10)\n",
        "print(memory.current_number)\n",
        "memory.push(Transition(1,1,0,0,0))\n",
        "memory.push(Transition(1,1,2,0,0))\n",
        "memory.push(Transition(1,1,3,0,0))\n",
        "\n",
        "monteCarloUpdate((memory,3,0.8))\n",
        "print(memory)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mac08SvWkxM6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sr0jIYNEmV_w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install zip\n",
        "!zip -r HW3_Task2.zip HW3_Task2\n",
        "from google.colab import files\n",
        "files.download('HW3_Task2.zip')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjesHYzy3DnR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}